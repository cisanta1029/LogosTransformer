{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab9423c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef4a6ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c86c2",
   "metadata": {},
   "source": [
    "### GPT Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b30119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL VARIABLES\n",
    "batch_size = 8                  # Number of sequences processed in parallel during one training step.\n",
    "block_size = 32                 # context window length\n",
    "n_embd = 384                    # Embedding dimension — the size of each token’s vector.\n",
    "n_head = 8                      # Number of attention heads. Each head learns a different relational pattern among tokens. Each head operates on n_embd / n_head = 48 features here.\n",
    "n_layer = 1                     # Number of Transformer blocks/layers stacked.\n",
    "dropout = 0.2                   # Randomly zeroes out 20% of connections during training to prevent overfitting.\n",
    "\n",
    "# TRAINING CONTROL VARIABLES\n",
    "max_iters = 2000                # How many training steps (batches) to run. One iteration = one optimizer update.\n",
    "learning_rate = 3e-4            # Step size in optimization (how far parameters move per update).\n",
    "eval_iters = 100                # How many mini-batches to average when estimating training/validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55b385ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'æ', '—', '’', '“', '”', '\\ufeff']\n",
      "﻿\n",
      "\n",
      "=== PLATO ===\n",
      "\n",
      "How you, O Athenians, have been affected by my accusers, I cannot tell;\n",
      "but I know that they almost made me forget who I was—so persuasively\n",
      "did they speak; and yet they have hardly \n"
     ]
    }
   ],
   "source": [
    "# read in our corpus into text string\n",
    "chars = \"\"\n",
    "with open('data\\philosophers.txt','r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "\n",
    "vocab_size = len(chars)\n",
    "print(chars)\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "edd67a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder and decoder for strings to integers and vice versa\n",
    "\n",
    "#chars = sorted(set(text))\n",
    "string_to_int = {ch:i for i,ch in enumerate(chars)}\n",
    "int_to_string = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10516a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory map for using small snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = \"data\\\\philosophers.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c71d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "# use first 80% of corpus for training; remaining 20% for validation\n",
    "n = int(0.8 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split) # Elliot's version - uses a different method to get data from train/test\n",
    "    #data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    #print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x,y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba69d6a5",
   "metadata": {},
   "source": [
    "#### Classes of the GPT Model\n",
    "* Head\n",
    "* MultiHeadAttention\n",
    "* FeedForward\n",
    "* Block\n",
    "* GPTLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f50005e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "        # computer attention scores (\"affinities\")\n",
    "\n",
    "        # head_size = subproblems\n",
    "        # scaling controls so no one individual head dominates / \"hear all voices evenly\"\n",
    "        weights = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B,T, hs) @ (B, hs, T) -> (B,T,T)\n",
    "\n",
    "        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        weights = F.softmax(weights, dim=-1) # (B, T, T)\n",
    "        weights = self.dropout(weights)\n",
    "\n",
    "        # perform the weighted aggregation of the values\n",
    "        out = weights @ v # (B, T, T) @ (B,T, hs) -> (B ,T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])     # heads running in parallel - from HEAD class\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)                        # projection\n",
    "        self.dropout = nn.Dropout(dropout)                                          # turn off some neurons\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)                         # concatenate along feature dimension (B,T,F) -> (B,T, [h1, h1, h1, h1, h2,h2,h2,h2,h3,h3,h3,h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4* n_embd, n_embd),\n",
    "            nn.Dropout(dropout) # used to prevent overfitting. look up later.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation\"\"\"\n",
    "    # Blocks have two main sub-components: the self attention layer, and the feed forward network\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head                        # number of features that each head will capture in multi-head attention\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)     # self-attention layer from MultiHeadAttention class\n",
    "        self.ffwd = FeedForward(n_embd)                     # feed-forward network\n",
    "        self.ln1 = nn.LayerNorm(n_embd)                     # post-norm\n",
    "        self.ln2 = nn.LayerNorm(n_embd)                     # post-norm\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)                  # self-attention layer from MultiHeadAttention class\n",
    "        x = self.ln1(x+y)               # add+norm\n",
    "        y = self.ffwd(x)                # feed-forward network\n",
    "        x = self.ln2(x+y)               # add+norm\n",
    "        return x\n",
    "\n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super().__init__()\n",
    "        self.token_embeddings_table = nn.Embedding(vocabulary_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)]) # decoder layers\n",
    "        self.ln_f = nn.LayerNorm(n_embd) #normalization\n",
    "        self.lm_head = nn.Linear(n_embd, vocabulary_size) # linear transformation\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # forward transforms tokens into embeddings, passing them through attention blocks, and projecting them back to vocabulary scores\n",
    "    def forward(self, index, targets=None):                                         # index is the X tensor passed from model(X,Y); targets is the Y tensor\n",
    "\n",
    "        B, T = index.shape                                                          # index is a tensor with B batches, and T time-steps (or sequence lengths)\n",
    "\n",
    "        tok_emb = self.token_embeddings_table(index)                                # token embedding vectors for the index tensor. should be shape of (B, T, number of embedding features for each token - aka n_embd aka C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))     # positional embedding vector for the index tensor. shape is (T, C aka n_embd)\n",
    "        x = tok_emb + pos_emb                                                       # token and positional embeddings combined to form tensor of shape (B,T,C aka n_embd)\n",
    "        x = self.blocks(x)                                                          # transformer block(s) from the Block class\n",
    "        x = self.ln_f(x)                                                            # normalization\n",
    "        logits = self.lm_head(x)                                                    # linear transformation\n",
    "\n",
    "        if targets is None:\n",
    "            return logits, None\n",
    "        \n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B*T, C)\n",
    "        targets = targets.view(B*T)\n",
    "        loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss #return logits.view(B, T, C), loss\n",
    "    \n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond) #logits, _ = self.forward(index)          # (B,T,C)\n",
    "            #print(f'index {index}')\n",
    "            #print(logits)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]                # take last time step -> (B,C)\n",
    "            #print('last')\n",
    "            #print(logits)\n",
    "            probs = F.softmax(logits, dim=-1)        # turn into probabilities\n",
    "            #print('probs')\n",
    "            #print(probs)\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # sample next token -> (B,1)\n",
    "            #print(index_next)\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f0e955c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size).to(device)\n",
    "\n",
    "#m = model.to(device)\n",
    "\n",
    "#next(m.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25037c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()                            # decorator that disables gradient tracking - unneccesary during evaluation\n",
    "def estimate_loss() -> dict:                # returns a dictionary that contains the average loss for training and validation splits\n",
    "    out = {}                                # instantiate dictionary that will be returned by the function\n",
    "    model.eval()                            # switch model to evaluation mode; disables dropout randomness, among other things unnecessary when not training\n",
    "    for split in ['train','val']:           # done to return a loss for training and validation\n",
    "        losses = torch.zeros(eval_iters)    # eval_iters = number of mini-batches of data to be used during loss estimation.\n",
    "        for k in range(eval_iters):\n",
    "            X,Y = get_batch(split)          # get your X and Y values\n",
    "            logits, loss = model(X,Y)       # model(X,Y) calls the forward() method\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()          # average loss among the n number of batches saved.\n",
    "    model.train()                           # switch model back to train mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebdad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "step: 0, train loss: 4.453, val loss 4.453\n",
      "1\n",
      "step: 1, train loss: 4.420, val loss 4.424\n",
      "2\n",
      "step: 2, train loss: 4.388, val loss 4.389\n",
      "3\n",
      "step: 3, train loss: 4.357, val loss 4.361\n",
      "4\n",
      "step: 4, train loss: 4.325, val loss 4.330\n",
      "5\n",
      "step: 5, train loss: 4.296, val loss 4.295\n",
      "6\n",
      "step: 6, train loss: 4.263, val loss 4.264\n",
      "7\n",
      "step: 7, train loss: 4.223, val loss 4.226\n",
      "8\n",
      "step: 8, train loss: 4.196, val loss 4.200\n",
      "9\n",
      "step: 9, train loss: 4.158, val loss 4.157\n",
      "10\n",
      "step: 10, train loss: 4.124, val loss 4.118\n",
      "11\n",
      "step: 11, train loss: 4.081, val loss 4.085\n",
      "12\n",
      "step: 12, train loss: 4.039, val loss 4.042\n",
      "13\n",
      "step: 13, train loss: 3.999, val loss 4.000\n",
      "14\n",
      "step: 14, train loss: 3.964, val loss 3.955\n",
      "15\n",
      "step: 15, train loss: 3.916, val loss 3.918\n",
      "16\n",
      "step: 16, train loss: 3.873, val loss 3.859\n",
      "17\n",
      "step: 17, train loss: 3.815, val loss 3.824\n",
      "18\n",
      "step: 18, train loss: 3.775, val loss 3.773\n",
      "19\n",
      "step: 19, train loss: 3.724, val loss 3.731\n",
      "20\n",
      "step: 20, train loss: 3.685, val loss 3.687\n",
      "21\n",
      "step: 21, train loss: 3.633, val loss 3.629\n",
      "22\n",
      "step: 22, train loss: 3.586, val loss 3.583\n",
      "23\n",
      "step: 23, train loss: 3.557, val loss 3.542\n",
      "24\n",
      "step: 24, train loss: 3.502, val loss 3.496\n",
      "25\n",
      "step: 25, train loss: 3.455, val loss 3.461\n",
      "26\n",
      "step: 26, train loss: 3.432, val loss 3.408\n",
      "27\n",
      "step: 27, train loss: 3.393, val loss 3.380\n",
      "28\n",
      "step: 28, train loss: 3.354, val loss 3.359\n",
      "29\n",
      "step: 29, train loss: 3.324, val loss 3.322\n",
      "30\n",
      "step: 30, train loss: 3.290, val loss 3.286\n",
      "31\n",
      "step: 31, train loss: 3.285, val loss 3.275\n",
      "32\n",
      "step: 32, train loss: 3.254, val loss 3.245\n",
      "33\n",
      "step: 33, train loss: 3.217, val loss 3.234\n",
      "34\n",
      "step: 34, train loss: 3.214, val loss 3.208\n",
      "35\n",
      "step: 35, train loss: 3.189, val loss 3.187\n",
      "36\n",
      "step: 36, train loss: 3.174, val loss 3.162\n",
      "37\n",
      "step: 37, train loss: 3.162, val loss 3.141\n",
      "38\n",
      "step: 38, train loss: 3.133, val loss 3.135\n",
      "39\n",
      "step: 39, train loss: 3.135, val loss 3.140\n",
      "40\n",
      "step: 40, train loss: 3.110, val loss 3.110\n",
      "41\n",
      "step: 41, train loss: 3.101, val loss 3.110\n",
      "42\n",
      "step: 42, train loss: 3.100, val loss 3.093\n",
      "43\n",
      "step: 43, train loss: 3.062, val loss 3.089\n",
      "44\n",
      "step: 44, train loss: 3.073, val loss 3.064\n",
      "45\n",
      "step: 45, train loss: 3.053, val loss 3.054\n",
      "46\n",
      "step: 46, train loss: 3.046, val loss 3.045\n",
      "47\n",
      "step: 47, train loss: 3.041, val loss 3.038\n",
      "48\n",
      "step: 48, train loss: 3.017, val loss 3.029\n",
      "49\n",
      "step: 49, train loss: 3.008, val loss 3.035\n",
      "50\n",
      "step: 50, train loss: 3.029, val loss 3.020\n",
      "51\n",
      "step: 51, train loss: 2.984, val loss 3.011\n",
      "52\n",
      "step: 52, train loss: 2.989, val loss 2.984\n",
      "53\n",
      "step: 53, train loss: 2.983, val loss 2.969\n",
      "54\n",
      "step: 54, train loss: 2.993, val loss 2.971\n",
      "55\n",
      "step: 55, train loss: 2.968, val loss 2.978\n",
      "56\n",
      "step: 56, train loss: 2.956, val loss 2.950\n",
      "57\n",
      "step: 57, train loss: 2.954, val loss 2.930\n",
      "58\n",
      "step: 58, train loss: 2.947, val loss 2.942\n",
      "59\n",
      "step: 59, train loss: 2.932, val loss 2.929\n",
      "60\n",
      "step: 60, train loss: 2.919, val loss 2.922\n",
      "61\n",
      "step: 61, train loss: 2.909, val loss 2.911\n",
      "62\n",
      "step: 62, train loss: 2.901, val loss 2.917\n",
      "63\n",
      "step: 63, train loss: 2.906, val loss 2.902\n",
      "64\n",
      "step: 64, train loss: 2.896, val loss 2.899\n",
      "65\n",
      "step: 65, train loss: 2.910, val loss 2.872\n",
      "66\n",
      "step: 66, train loss: 2.889, val loss 2.879\n",
      "67\n",
      "step: 67, train loss: 2.880, val loss 2.865\n",
      "68\n",
      "step: 68, train loss: 2.860, val loss 2.856\n",
      "69\n",
      "step: 69, train loss: 2.855, val loss 2.868\n",
      "70\n",
      "step: 70, train loss: 2.875, val loss 2.859\n",
      "71\n",
      "step: 71, train loss: 2.839, val loss 2.870\n",
      "72\n",
      "step: 72, train loss: 2.851, val loss 2.843\n",
      "73\n",
      "step: 73, train loss: 2.834, val loss 2.829\n",
      "74\n",
      "step: 74, train loss: 2.815, val loss 2.809\n",
      "75\n",
      "step: 75, train loss: 2.806, val loss 2.814\n",
      "76\n",
      "step: 76, train loss: 2.801, val loss 2.816\n",
      "77\n",
      "step: 77, train loss: 2.796, val loss 2.797\n",
      "78\n",
      "step: 78, train loss: 2.833, val loss 2.812\n",
      "79\n",
      "step: 79, train loss: 2.783, val loss 2.825\n",
      "80\n",
      "step: 80, train loss: 2.781, val loss 2.822\n",
      "81\n",
      "step: 81, train loss: 2.785, val loss 2.794\n",
      "82\n",
      "step: 82, train loss: 2.773, val loss 2.782\n",
      "83\n",
      "step: 83, train loss: 2.758, val loss 2.755\n",
      "84\n",
      "step: 84, train loss: 2.758, val loss 2.756\n",
      "85\n",
      "step: 85, train loss: 2.765, val loss 2.752\n",
      "86\n",
      "step: 86, train loss: 2.761, val loss 2.766\n",
      "87\n",
      "step: 87, train loss: 2.774, val loss 2.735\n",
      "88\n",
      "step: 88, train loss: 2.757, val loss 2.737\n",
      "89\n",
      "step: 89, train loss: 2.742, val loss 2.728\n",
      "90\n",
      "step: 90, train loss: 2.735, val loss 2.744\n",
      "91\n",
      "step: 91, train loss: 2.738, val loss 2.758\n",
      "92\n",
      "step: 92, train loss: 2.703, val loss 2.727\n",
      "93\n",
      "step: 93, train loss: 2.722, val loss 2.722\n",
      "94\n",
      "step: 94, train loss: 2.718, val loss 2.703\n",
      "95\n",
      "step: 95, train loss: 2.721, val loss 2.702\n",
      "96\n",
      "step: 96, train loss: 2.693, val loss 2.712\n",
      "97\n",
      "step: 97, train loss: 2.708, val loss 2.728\n",
      "98\n",
      "step: 98, train loss: 2.682, val loss 2.711\n",
      "99\n",
      "step: 99, train loss: 2.691, val loss 2.706\n",
      "100\n",
      "step: 100, train loss: 2.717, val loss 2.692\n",
      "101\n",
      "step: 101, train loss: 2.674, val loss 2.667\n",
      "102\n",
      "step: 102, train loss: 2.681, val loss 2.683\n",
      "103\n",
      "step: 103, train loss: 2.665, val loss 2.665\n",
      "104\n",
      "step: 104, train loss: 2.694, val loss 2.681\n",
      "105\n",
      "step: 105, train loss: 2.664, val loss 2.658\n",
      "106\n",
      "step: 106, train loss: 2.681, val loss 2.676\n",
      "107\n",
      "step: 107, train loss: 2.675, val loss 2.660\n",
      "108\n",
      "step: 108, train loss: 2.660, val loss 2.669\n",
      "109\n",
      "step: 109, train loss: 2.681, val loss 2.675\n",
      "110\n",
      "step: 110, train loss: 2.671, val loss 2.673\n",
      "111\n",
      "step: 111, train loss: 2.643, val loss 2.668\n",
      "112\n",
      "step: 112, train loss: 2.651, val loss 2.644\n",
      "113\n",
      "step: 113, train loss: 2.650, val loss 2.687\n",
      "114\n",
      "step: 114, train loss: 2.662, val loss 2.651\n",
      "115\n",
      "step: 115, train loss: 2.644, val loss 2.624\n",
      "116\n",
      "step: 116, train loss: 2.638, val loss 2.642\n",
      "117\n",
      "step: 117, train loss: 2.625, val loss 2.629\n",
      "118\n",
      "step: 118, train loss: 2.656, val loss 2.616\n",
      "119\n",
      "step: 119, train loss: 2.635, val loss 2.629\n",
      "120\n",
      "step: 120, train loss: 2.635, val loss 2.624\n",
      "121\n",
      "step: 121, train loss: 2.639, val loss 2.620\n",
      "122\n",
      "step: 122, train loss: 2.636, val loss 2.617\n",
      "123\n",
      "step: 123, train loss: 2.615, val loss 2.645\n",
      "124\n",
      "step: 124, train loss: 2.635, val loss 2.637\n",
      "125\n",
      "step: 125, train loss: 2.615, val loss 2.638\n",
      "126\n",
      "step: 126, train loss: 2.639, val loss 2.625\n",
      "127\n",
      "step: 127, train loss: 2.635, val loss 2.611\n",
      "128\n",
      "step: 128, train loss: 2.617, val loss 2.634\n",
      "129\n",
      "step: 129, train loss: 2.628, val loss 2.622\n",
      "130\n",
      "step: 130, train loss: 2.629, val loss 2.622\n",
      "131\n",
      "step: 131, train loss: 2.589, val loss 2.602\n",
      "132\n",
      "step: 132, train loss: 2.611, val loss 2.621\n",
      "133\n",
      "step: 133, train loss: 2.584, val loss 2.647\n",
      "134\n",
      "step: 134, train loss: 2.602, val loss 2.607\n",
      "135\n",
      "step: 135, train loss: 2.616, val loss 2.573\n",
      "136\n",
      "step: 136, train loss: 2.623, val loss 2.623\n",
      "137\n",
      "step: 137, train loss: 2.580, val loss 2.584\n",
      "138\n",
      "step: 138, train loss: 2.610, val loss 2.587\n",
      "139\n",
      "step: 139, train loss: 2.581, val loss 2.572\n",
      "140\n",
      "step: 140, train loss: 2.601, val loss 2.599\n",
      "141\n",
      "step: 141, train loss: 2.586, val loss 2.605\n",
      "142\n",
      "step: 142, train loss: 2.595, val loss 2.602\n",
      "143\n",
      "step: 143, train loss: 2.591, val loss 2.576\n",
      "144\n",
      "step: 144, train loss: 2.583, val loss 2.591\n",
      "145\n",
      "step: 145, train loss: 2.572, val loss 2.614\n",
      "146\n",
      "step: 146, train loss: 2.619, val loss 2.581\n",
      "147\n",
      "step: 147, train loss: 2.593, val loss 2.572\n",
      "148\n",
      "step: 148, train loss: 2.590, val loss 2.576\n",
      "149\n",
      "step: 149, train loss: 2.584, val loss 2.561\n",
      "150\n",
      "step: 150, train loss: 2.565, val loss 2.582\n",
      "151\n",
      "step: 151, train loss: 2.556, val loss 2.553\n",
      "152\n",
      "step: 152, train loss: 2.585, val loss 2.577\n",
      "153\n",
      "step: 153, train loss: 2.583, val loss 2.553\n",
      "154\n",
      "step: 154, train loss: 2.566, val loss 2.575\n",
      "155\n",
      "step: 155, train loss: 2.615, val loss 2.575\n",
      "156\n",
      "step: 156, train loss: 2.569, val loss 2.586\n",
      "157\n",
      "step: 157, train loss: 2.581, val loss 2.549\n",
      "158\n",
      "step: 158, train loss: 2.566, val loss 2.588\n",
      "159\n",
      "step: 159, train loss: 2.548, val loss 2.571\n",
      "160\n",
      "step: 160, train loss: 2.556, val loss 2.557\n",
      "161\n",
      "step: 161, train loss: 2.551, val loss 2.558\n",
      "162\n",
      "step: 162, train loss: 2.554, val loss 2.533\n",
      "163\n",
      "step: 163, train loss: 2.586, val loss 2.558\n",
      "164\n",
      "step: 164, train loss: 2.562, val loss 2.542\n",
      "165\n",
      "step: 165, train loss: 2.537, val loss 2.553\n",
      "166\n",
      "step: 166, train loss: 2.539, val loss 2.568\n",
      "167\n",
      "step: 167, train loss: 2.550, val loss 2.576\n",
      "168\n",
      "step: 168, train loss: 2.552, val loss 2.533\n",
      "169\n",
      "step: 169, train loss: 2.551, val loss 2.580\n",
      "170\n",
      "step: 170, train loss: 2.547, val loss 2.549\n",
      "171\n",
      "step: 171, train loss: 2.536, val loss 2.561\n",
      "172\n",
      "step: 172, train loss: 2.558, val loss 2.535\n",
      "173\n",
      "step: 173, train loss: 2.579, val loss 2.565\n",
      "174\n",
      "step: 174, train loss: 2.564, val loss 2.553\n",
      "175\n",
      "step: 175, train loss: 2.542, val loss 2.564\n",
      "176\n",
      "step: 176, train loss: 2.541, val loss 2.563\n",
      "177\n",
      "step: 177, train loss: 2.518, val loss 2.544\n",
      "178\n",
      "step: 178, train loss: 2.537, val loss 2.571\n",
      "179\n",
      "step: 179, train loss: 2.555, val loss 2.546\n",
      "180\n",
      "step: 180, train loss: 2.535, val loss 2.540\n",
      "181\n",
      "step: 181, train loss: 2.557, val loss 2.531\n",
      "182\n",
      "step: 182, train loss: 2.536, val loss 2.552\n",
      "183\n",
      "step: 183, train loss: 2.574, val loss 2.555\n",
      "184\n",
      "step: 184, train loss: 2.544, val loss 2.536\n",
      "185\n",
      "step: 185, train loss: 2.561, val loss 2.542\n",
      "186\n",
      "step: 186, train loss: 2.545, val loss 2.531\n",
      "187\n",
      "step: 187, train loss: 2.517, val loss 2.538\n",
      "188\n",
      "step: 188, train loss: 2.516, val loss 2.533\n",
      "189\n",
      "step: 189, train loss: 2.530, val loss 2.557\n",
      "190\n",
      "step: 190, train loss: 2.542, val loss 2.544\n",
      "191\n",
      "step: 191, train loss: 2.531, val loss 2.520\n",
      "192\n",
      "step: 192, train loss: 2.514, val loss 2.520\n",
      "193\n",
      "step: 193, train loss: 2.533, val loss 2.567\n",
      "194\n",
      "step: 194, train loss: 2.524, val loss 2.543\n",
      "195\n",
      "step: 195, train loss: 2.515, val loss 2.515\n",
      "196\n",
      "step: 196, train loss: 2.514, val loss 2.537\n",
      "197\n",
      "step: 197, train loss: 2.520, val loss 2.558\n",
      "198\n",
      "step: 198, train loss: 2.538, val loss 2.556\n",
      "199\n",
      "step: 199, train loss: 2.528, val loss 2.531\n",
      "200\n",
      "step: 200, train loss: 2.517, val loss 2.499\n",
      "201\n",
      "step: 201, train loss: 2.537, val loss 2.537\n",
      "202\n",
      "step: 202, train loss: 2.526, val loss 2.534\n",
      "203\n",
      "step: 203, train loss: 2.504, val loss 2.525\n",
      "204\n",
      "step: 204, train loss: 2.521, val loss 2.524\n",
      "205\n",
      "step: 205, train loss: 2.512, val loss 2.543\n",
      "206\n",
      "step: 206, train loss: 2.505, val loss 2.533\n",
      "207\n",
      "step: 207, train loss: 2.520, val loss 2.505\n",
      "208\n",
      "step: 208, train loss: 2.587, val loss 2.523\n",
      "209\n",
      "step: 209, train loss: 2.520, val loss 2.526\n",
      "210\n",
      "step: 210, train loss: 2.517, val loss 2.519\n",
      "211\n",
      "step: 211, train loss: 2.511, val loss 2.507\n",
      "212\n",
      "step: 212, train loss: 2.507, val loss 2.528\n",
      "213\n",
      "step: 213, train loss: 2.512, val loss 2.545\n",
      "214\n",
      "step: 214, train loss: 2.504, val loss 2.528\n",
      "215\n",
      "step: 215, train loss: 2.539, val loss 2.520\n",
      "216\n",
      "step: 216, train loss: 2.570, val loss 2.518\n",
      "217\n",
      "step: 217, train loss: 2.534, val loss 2.541\n",
      "218\n",
      "step: 218, train loss: 2.553, val loss 2.508\n",
      "219\n",
      "step: 219, train loss: 2.508, val loss 2.484\n",
      "220\n",
      "step: 220, train loss: 2.559, val loss 2.512\n",
      "221\n",
      "step: 221, train loss: 2.502, val loss 2.491\n",
      "222\n",
      "step: 222, train loss: 2.508, val loss 2.496\n",
      "223\n",
      "step: 223, train loss: 2.507, val loss 2.510\n",
      "224\n",
      "step: 224, train loss: 2.527, val loss 2.510\n",
      "225\n",
      "step: 225, train loss: 2.495, val loss 2.524\n",
      "226\n",
      "step: 226, train loss: 2.500, val loss 2.525\n",
      "227\n",
      "step: 227, train loss: 2.549, val loss 2.496\n",
      "228\n",
      "step: 228, train loss: 2.499, val loss 2.481\n",
      "229\n",
      "step: 229, train loss: 2.527, val loss 2.492\n",
      "230\n",
      "step: 230, train loss: 2.530, val loss 2.478\n",
      "231\n",
      "step: 231, train loss: 2.553, val loss 2.492\n",
      "232\n",
      "step: 232, train loss: 2.518, val loss 2.502\n",
      "233\n",
      "step: 233, train loss: 2.492, val loss 2.506\n",
      "234\n",
      "step: 234, train loss: 2.492, val loss 2.506\n",
      "235\n",
      "step: 235, train loss: 2.479, val loss 2.489\n",
      "236\n",
      "step: 236, train loss: 2.499, val loss 2.485\n",
      "237\n",
      "step: 237, train loss: 2.477, val loss 2.523\n",
      "238\n",
      "step: 238, train loss: 2.505, val loss 2.473\n",
      "239\n",
      "step: 239, train loss: 2.499, val loss 2.492\n",
      "240\n",
      "step: 240, train loss: 2.521, val loss 2.494\n",
      "241\n",
      "step: 241, train loss: 2.509, val loss 2.502\n",
      "242\n",
      "step: 242, train loss: 2.485, val loss 2.480\n",
      "243\n",
      "step: 243, train loss: 2.504, val loss 2.483\n",
      "244\n",
      "step: 244, train loss: 2.505, val loss 2.498\n",
      "245\n",
      "step: 245, train loss: 2.466, val loss 2.482\n",
      "246\n",
      "step: 246, train loss: 2.492, val loss 2.509\n",
      "247\n",
      "step: 247, train loss: 2.471, val loss 2.488\n",
      "248\n",
      "step: 248, train loss: 2.497, val loss 2.486\n",
      "249\n",
      "step: 249, train loss: 2.519, val loss 2.494\n",
      "250\n",
      "step: 250, train loss: 2.529, val loss 2.505\n",
      "251\n",
      "step: 251, train loss: 2.474, val loss 2.515\n",
      "252\n",
      "step: 252, train loss: 2.490, val loss 2.471\n",
      "253\n",
      "step: 253, train loss: 2.486, val loss 2.495\n",
      "254\n",
      "step: 254, train loss: 2.493, val loss 2.485\n",
      "255\n",
      "step: 255, train loss: 2.491, val loss 2.499\n",
      "256\n",
      "step: 256, train loss: 2.507, val loss 2.470\n",
      "257\n",
      "step: 257, train loss: 2.460, val loss 2.481\n",
      "258\n",
      "step: 258, train loss: 2.463, val loss 2.487\n",
      "259\n",
      "step: 259, train loss: 2.493, val loss 2.494\n",
      "260\n",
      "step: 260, train loss: 2.487, val loss 2.480\n",
      "261\n",
      "step: 261, train loss: 2.464, val loss 2.492\n",
      "262\n",
      "step: 262, train loss: 2.497, val loss 2.497\n",
      "263\n",
      "step: 263, train loss: 2.522, val loss 2.504\n",
      "264\n",
      "step: 264, train loss: 2.482, val loss 2.488\n",
      "265\n",
      "step: 265, train loss: 2.481, val loss 2.487\n",
      "266\n",
      "step: 266, train loss: 2.480, val loss 2.462\n",
      "267\n",
      "step: 267, train loss: 2.521, val loss 2.484\n",
      "268\n",
      "step: 268, train loss: 2.502, val loss 2.488\n",
      "269\n",
      "step: 269, train loss: 2.477, val loss 2.494\n",
      "270\n",
      "step: 270, train loss: 2.489, val loss 2.479\n",
      "271\n",
      "step: 271, train loss: 2.507, val loss 2.492\n",
      "272\n",
      "step: 272, train loss: 2.485, val loss 2.455\n",
      "273\n",
      "step: 273, train loss: 2.499, val loss 2.482\n",
      "274\n",
      "step: 274, train loss: 2.458, val loss 2.498\n",
      "275\n",
      "step: 275, train loss: 2.464, val loss 2.484\n",
      "276\n",
      "step: 276, train loss: 2.485, val loss 2.450\n",
      "277\n",
      "step: 277, train loss: 2.498, val loss 2.500\n",
      "278\n",
      "step: 278, train loss: 2.506, val loss 2.495\n",
      "279\n",
      "step: 279, train loss: 2.491, val loss 2.486\n",
      "280\n",
      "step: 280, train loss: 2.481, val loss 2.477\n",
      "281\n",
      "step: 281, train loss: 2.498, val loss 2.473\n",
      "282\n",
      "step: 282, train loss: 2.479, val loss 2.509\n",
      "283\n",
      "step: 283, train loss: 2.484, val loss 2.456\n",
      "284\n",
      "step: 284, train loss: 2.499, val loss 2.486\n",
      "285\n",
      "step: 285, train loss: 2.483, val loss 2.461\n",
      "286\n",
      "step: 286, train loss: 2.474, val loss 2.498\n",
      "287\n",
      "step: 287, train loss: 2.483, val loss 2.468\n",
      "288\n",
      "step: 288, train loss: 2.474, val loss 2.481\n",
      "289\n",
      "step: 289, train loss: 2.484, val loss 2.475\n",
      "290\n",
      "step: 290, train loss: 2.475, val loss 2.483\n",
      "291\n",
      "step: 291, train loss: 2.473, val loss 2.492\n",
      "292\n",
      "step: 292, train loss: 2.468, val loss 2.447\n",
      "293\n",
      "step: 293, train loss: 2.466, val loss 2.452\n",
      "294\n",
      "step: 294, train loss: 2.485, val loss 2.462\n",
      "295\n",
      "step: 295, train loss: 2.453, val loss 2.480\n",
      "296\n",
      "step: 296, train loss: 2.462, val loss 2.469\n",
      "297\n",
      "step: 297, train loss: 2.473, val loss 2.470\n",
      "298\n",
      "step: 298, train loss: 2.465, val loss 2.497\n",
      "299\n",
      "step: 299, train loss: 2.465, val loss 2.475\n",
      "300\n",
      "step: 300, train loss: 2.455, val loss 2.478\n",
      "301\n",
      "step: 301, train loss: 2.493, val loss 2.468\n",
      "302\n",
      "step: 302, train loss: 2.471, val loss 2.491\n",
      "303\n",
      "step: 303, train loss: 2.462, val loss 2.463\n",
      "304\n",
      "step: 304, train loss: 2.464, val loss 2.467\n",
      "305\n",
      "step: 305, train loss: 2.499, val loss 2.511\n",
      "306\n",
      "step: 306, train loss: 2.467, val loss 2.480\n",
      "307\n",
      "step: 307, train loss: 2.458, val loss 2.459\n",
      "308\n",
      "step: 308, train loss: 2.455, val loss 2.452\n",
      "309\n",
      "step: 309, train loss: 2.455, val loss 2.446\n",
      "310\n",
      "step: 310, train loss: 2.479, val loss 2.457\n",
      "311\n",
      "step: 311, train loss: 2.441, val loss 2.475\n",
      "312\n",
      "step: 312, train loss: 2.482, val loss 2.471\n",
      "313\n",
      "step: 313, train loss: 2.473, val loss 2.481\n",
      "314\n",
      "step: 314, train loss: 2.443, val loss 2.467\n",
      "315\n",
      "step: 315, train loss: 2.486, val loss 2.480\n",
      "316\n",
      "step: 316, train loss: 2.459, val loss 2.442\n",
      "317\n",
      "step: 317, train loss: 2.471, val loss 2.470\n",
      "318\n",
      "step: 318, train loss: 2.468, val loss 2.466\n",
      "319\n",
      "step: 319, train loss: 2.456, val loss 2.469\n",
      "320\n",
      "step: 320, train loss: 2.469, val loss 2.471\n",
      "321\n",
      "step: 321, train loss: 2.480, val loss 2.444\n",
      "322\n",
      "step: 322, train loss: 2.448, val loss 2.483\n",
      "323\n",
      "step: 323, train loss: 2.504, val loss 2.457\n",
      "324\n",
      "step: 324, train loss: 2.466, val loss 2.472\n",
      "325\n",
      "step: 325, train loss: 2.472, val loss 2.444\n",
      "326\n",
      "step: 326, train loss: 2.496, val loss 2.481\n",
      "327\n",
      "step: 327, train loss: 2.471, val loss 2.426\n",
      "328\n",
      "step: 328, train loss: 2.465, val loss 2.478\n",
      "329\n",
      "step: 329, train loss: 2.470, val loss 2.436\n",
      "330\n",
      "step: 330, train loss: 2.445, val loss 2.441\n",
      "331\n",
      "step: 331, train loss: 2.454, val loss 2.452\n",
      "332\n",
      "step: 332, train loss: 2.516, val loss 2.458\n",
      "333\n",
      "step: 333, train loss: 2.450, val loss 2.454\n",
      "334\n",
      "step: 334, train loss: 2.456, val loss 2.496\n",
      "335\n",
      "step: 335, train loss: 2.468, val loss 2.477\n",
      "336\n",
      "step: 336, train loss: 2.446, val loss 2.446\n",
      "337\n",
      "step: 337, train loss: 2.460, val loss 2.456\n",
      "338\n",
      "step: 338, train loss: 2.471, val loss 2.480\n",
      "339\n",
      "step: 339, train loss: 2.453, val loss 2.462\n",
      "340\n",
      "step: 340, train loss: 2.477, val loss 2.471\n",
      "341\n",
      "step: 341, train loss: 2.458, val loss 2.442\n",
      "342\n",
      "step: 342, train loss: 2.450, val loss 2.465\n",
      "343\n",
      "step: 343, train loss: 2.462, val loss 2.466\n",
      "344\n",
      "step: 344, train loss: 2.457, val loss 2.447\n",
      "345\n",
      "step: 345, train loss: 2.449, val loss 2.450\n",
      "346\n",
      "step: 346, train loss: 2.440, val loss 2.470\n",
      "347\n",
      "step: 347, train loss: 2.490, val loss 2.476\n",
      "348\n",
      "step: 348, train loss: 2.471, val loss 2.470\n",
      "349\n",
      "step: 349, train loss: 2.445, val loss 2.468\n",
      "350\n",
      "step: 350, train loss: 2.423, val loss 2.445\n",
      "351\n",
      "step: 351, train loss: 2.464, val loss 2.467\n",
      "352\n",
      "step: 352, train loss: 2.494, val loss 2.433\n",
      "353\n",
      "step: 353, train loss: 2.453, val loss 2.435\n",
      "354\n",
      "step: 354, train loss: 2.432, val loss 2.481\n",
      "355\n",
      "step: 355, train loss: 2.448, val loss 2.484\n",
      "356\n",
      "step: 356, train loss: 2.465, val loss 2.447\n",
      "357\n",
      "step: 357, train loss: 2.436, val loss 2.438\n",
      "358\n",
      "step: 358, train loss: 2.441, val loss 2.437\n",
      "359\n",
      "step: 359, train loss: 2.439, val loss 2.450\n",
      "360\n",
      "step: 360, train loss: 2.485, val loss 2.454\n",
      "361\n",
      "step: 361, train loss: 2.466, val loss 2.451\n",
      "362\n",
      "step: 362, train loss: 2.471, val loss 2.448\n",
      "363\n",
      "step: 363, train loss: 2.484, val loss 2.456\n",
      "364\n",
      "step: 364, train loss: 2.457, val loss 2.459\n",
      "365\n",
      "step: 365, train loss: 2.421, val loss 2.494\n",
      "366\n",
      "step: 366, train loss: 2.456, val loss 2.479\n",
      "367\n",
      "step: 367, train loss: 2.426, val loss 2.435\n",
      "368\n",
      "step: 368, train loss: 2.460, val loss 2.425\n",
      "369\n",
      "step: 369, train loss: 2.455, val loss 2.450\n",
      "370\n",
      "step: 370, train loss: 2.455, val loss 2.428\n",
      "371\n",
      "step: 371, train loss: 2.475, val loss 2.464\n",
      "372\n",
      "step: 372, train loss: 2.422, val loss 2.425\n",
      "373\n",
      "step: 373, train loss: 2.441, val loss 2.444\n",
      "374\n",
      "step: 374, train loss: 2.485, val loss 2.489\n",
      "375\n",
      "step: 375, train loss: 2.471, val loss 2.454\n",
      "376\n",
      "step: 376, train loss: 2.445, val loss 2.433\n",
      "377\n",
      "step: 377, train loss: 2.440, val loss 2.470\n",
      "378\n",
      "step: 378, train loss: 2.448, val loss 2.434\n",
      "379\n",
      "step: 379, train loss: 2.450, val loss 2.431\n",
      "380\n",
      "step: 380, train loss: 2.452, val loss 2.431\n",
      "381\n",
      "step: 381, train loss: 2.457, val loss 2.469\n",
      "382\n",
      "step: 382, train loss: 2.453, val loss 2.464\n",
      "383\n",
      "step: 383, train loss: 2.461, val loss 2.410\n",
      "384\n",
      "step: 384, train loss: 2.443, val loss 2.443\n",
      "385\n",
      "step: 385, train loss: 2.458, val loss 2.426\n",
      "386\n",
      "step: 386, train loss: 2.443, val loss 2.440\n",
      "387\n",
      "step: 387, train loss: 2.441, val loss 2.449\n",
      "388\n",
      "step: 388, train loss: 2.453, val loss 2.449\n",
      "389\n",
      "step: 389, train loss: 2.463, val loss 2.434\n",
      "390\n",
      "step: 390, train loss: 2.450, val loss 2.432\n",
      "391\n",
      "step: 391, train loss: 2.424, val loss 2.454\n",
      "392\n",
      "step: 392, train loss: 2.453, val loss 2.474\n",
      "393\n",
      "step: 393, train loss: 2.431, val loss 2.432\n",
      "394\n",
      "step: 394, train loss: 2.451, val loss 2.426\n",
      "395\n",
      "step: 395, train loss: 2.426, val loss 2.435\n",
      "396\n",
      "step: 396, train loss: 2.435, val loss 2.452\n",
      "397\n",
      "step: 397, train loss: 2.444, val loss 2.451\n",
      "398\n",
      "step: 398, train loss: 2.453, val loss 2.460\n",
      "399\n",
      "step: 399, train loss: 2.448, val loss 2.433\n",
      "400\n",
      "step: 400, train loss: 2.427, val loss 2.435\n",
      "401\n",
      "step: 401, train loss: 2.477, val loss 2.456\n",
      "402\n",
      "step: 402, train loss: 2.463, val loss 2.443\n",
      "403\n",
      "step: 403, train loss: 2.423, val loss 2.454\n",
      "404\n",
      "step: 404, train loss: 2.457, val loss 2.443\n",
      "405\n",
      "step: 405, train loss: 2.430, val loss 2.446\n",
      "406\n",
      "step: 406, train loss: 2.455, val loss 2.418\n",
      "407\n",
      "step: 407, train loss: 2.447, val loss 2.464\n",
      "408\n",
      "step: 408, train loss: 2.423, val loss 2.458\n",
      "409\n",
      "step: 409, train loss: 2.428, val loss 2.432\n",
      "410\n",
      "step: 410, train loss: 2.461, val loss 2.436\n",
      "411\n",
      "step: 411, train loss: 2.428, val loss 2.446\n",
      "412\n",
      "step: 412, train loss: 2.433, val loss 2.462\n",
      "413\n",
      "step: 413, train loss: 2.457, val loss 2.454\n",
      "414\n",
      "step: 414, train loss: 2.459, val loss 2.432\n",
      "415\n",
      "step: 415, train loss: 2.440, val loss 2.406\n",
      "416\n",
      "step: 416, train loss: 2.407, val loss 2.439\n",
      "417\n",
      "step: 417, train loss: 2.438, val loss 2.478\n",
      "418\n",
      "step: 418, train loss: 2.428, val loss 2.418\n",
      "419\n",
      "step: 419, train loss: 2.447, val loss 2.442\n",
      "420\n",
      "step: 420, train loss: 2.473, val loss 2.479\n",
      "421\n",
      "step: 421, train loss: 2.456, val loss 2.431\n",
      "422\n",
      "step: 422, train loss: 2.450, val loss 2.489\n",
      "423\n",
      "step: 423, train loss: 2.430, val loss 2.455\n",
      "424\n",
      "step: 424, train loss: 2.424, val loss 2.435\n",
      "425\n",
      "step: 425, train loss: 2.427, val loss 2.438\n",
      "426\n",
      "step: 426, train loss: 2.418, val loss 2.422\n",
      "427\n",
      "step: 427, train loss: 2.435, val loss 2.447\n",
      "428\n",
      "step: 428, train loss: 2.435, val loss 2.453\n",
      "429\n",
      "step: 429, train loss: 2.423, val loss 2.440\n",
      "430\n",
      "step: 430, train loss: 2.455, val loss 2.431\n",
      "431\n",
      "step: 431, train loss: 2.470, val loss 2.428\n",
      "432\n",
      "step: 432, train loss: 2.419, val loss 2.414\n",
      "433\n",
      "step: 433, train loss: 2.420, val loss 2.466\n",
      "434\n",
      "step: 434, train loss: 2.414, val loss 2.433\n",
      "435\n",
      "step: 435, train loss: 2.435, val loss 2.491\n",
      "436\n",
      "step: 436, train loss: 2.457, val loss 2.443\n",
      "437\n",
      "step: 437, train loss: 2.426, val loss 2.428\n",
      "438\n",
      "step: 438, train loss: 2.413, val loss 2.423\n",
      "439\n",
      "step: 439, train loss: 2.411, val loss 2.418\n",
      "440\n",
      "step: 440, train loss: 2.438, val loss 2.445\n",
      "441\n",
      "step: 441, train loss: 2.467, val loss 2.465\n",
      "442\n",
      "step: 442, train loss: 2.450, val loss 2.460\n",
      "443\n",
      "step: 443, train loss: 2.431, val loss 2.448\n",
      "444\n",
      "step: 444, train loss: 2.449, val loss 2.431\n",
      "445\n",
      "step: 445, train loss: 2.437, val loss 2.448\n",
      "446\n",
      "step: 446, train loss: 2.442, val loss 2.435\n",
      "447\n",
      "step: 447, train loss: 2.419, val loss 2.419\n",
      "448\n",
      "step: 448, train loss: 2.450, val loss 2.445\n",
      "449\n",
      "step: 449, train loss: 2.432, val loss 2.432\n",
      "450\n",
      "step: 450, train loss: 2.425, val loss 2.449\n",
      "451\n",
      "step: 451, train loss: 2.433, val loss 2.472\n",
      "452\n",
      "step: 452, train loss: 2.446, val loss 2.438\n",
      "453\n",
      "step: 453, train loss: 2.436, val loss 2.433\n",
      "454\n",
      "step: 454, train loss: 2.425, val loss 2.423\n",
      "455\n",
      "step: 455, train loss: 2.424, val loss 2.429\n",
      "456\n",
      "step: 456, train loss: 2.433, val loss 2.445\n",
      "457\n",
      "step: 457, train loss: 2.432, val loss 2.448\n",
      "458\n",
      "step: 458, train loss: 2.437, val loss 2.432\n",
      "459\n",
      "step: 459, train loss: 2.431, val loss 2.415\n",
      "460\n",
      "step: 460, train loss: 2.414, val loss 2.434\n",
      "461\n",
      "step: 461, train loss: 2.408, val loss 2.421\n",
      "462\n",
      "step: 462, train loss: 2.432, val loss 2.404\n",
      "463\n",
      "step: 463, train loss: 2.441, val loss 2.467\n",
      "464\n",
      "step: 464, train loss: 2.452, val loss 2.434\n",
      "465\n",
      "step: 465, train loss: 2.440, val loss 2.420\n",
      "466\n",
      "step: 466, train loss: 2.429, val loss 2.436\n",
      "467\n",
      "step: 467, train loss: 2.420, val loss 2.420\n",
      "468\n",
      "step: 468, train loss: 2.441, val loss 2.424\n",
      "469\n",
      "step: 469, train loss: 2.461, val loss 2.412\n",
      "470\n",
      "step: 470, train loss: 2.432, val loss 2.416\n",
      "471\n",
      "step: 471, train loss: 2.433, val loss 2.444\n",
      "472\n",
      "step: 472, train loss: 2.447, val loss 2.433\n",
      "473\n",
      "step: 473, train loss: 2.437, val loss 2.441\n",
      "474\n",
      "step: 474, train loss: 2.443, val loss 2.444\n",
      "475\n",
      "step: 475, train loss: 2.415, val loss 2.442\n",
      "476\n",
      "step: 476, train loss: 2.459, val loss 2.415\n",
      "477\n",
      "step: 477, train loss: 2.430, val loss 2.425\n",
      "478\n",
      "step: 478, train loss: 2.443, val loss 2.419\n",
      "479\n",
      "step: 479, train loss: 2.442, val loss 2.443\n",
      "480\n",
      "step: 480, train loss: 2.422, val loss 2.432\n",
      "481\n",
      "step: 481, train loss: 2.420, val loss 2.413\n",
      "482\n",
      "step: 482, train loss: 2.431, val loss 2.459\n",
      "483\n",
      "step: 483, train loss: 2.429, val loss 2.453\n",
      "484\n",
      "step: 484, train loss: 2.428, val loss 2.435\n",
      "485\n",
      "step: 485, train loss: 2.449, val loss 2.410\n",
      "486\n",
      "step: 486, train loss: 2.446, val loss 2.413\n",
      "487\n",
      "step: 487, train loss: 2.437, val loss 2.434\n",
      "488\n",
      "step: 488, train loss: 2.423, val loss 2.423\n",
      "489\n",
      "step: 489, train loss: 2.433, val loss 2.448\n",
      "490\n",
      "step: 490, train loss: 2.454, val loss 2.425\n",
      "491\n",
      "step: 491, train loss: 2.441, val loss 2.425\n",
      "492\n",
      "step: 492, train loss: 2.423, val loss 2.440\n",
      "493\n",
      "step: 493, train loss: 2.403, val loss 2.422\n",
      "494\n",
      "step: 494, train loss: 2.414, val loss 2.424\n",
      "495\n",
      "step: 495, train loss: 2.426, val loss 2.422\n",
      "496\n",
      "step: 496, train loss: 2.427, val loss 2.449\n",
      "497\n",
      "step: 497, train loss: 2.442, val loss 2.431\n",
      "498\n",
      "step: 498, train loss: 2.424, val loss 2.474\n",
      "499\n",
      "step: 499, train loss: 2.438, val loss 2.433\n",
      "500\n",
      "step: 500, train loss: 2.428, val loss 2.439\n",
      "501\n",
      "step: 501, train loss: 2.403, val loss 2.417\n",
      "502\n",
      "step: 502, train loss: 2.410, val loss 2.423\n",
      "503\n",
      "step: 503, train loss: 2.416, val loss 2.449\n",
      "504\n",
      "step: 504, train loss: 2.443, val loss 2.448\n",
      "505\n",
      "step: 505, train loss: 2.438, val loss 2.419\n",
      "506\n",
      "step: 506, train loss: 2.440, val loss 2.404\n",
      "507\n",
      "step: 507, train loss: 2.405, val loss 2.460\n",
      "508\n",
      "step: 508, train loss: 2.432, val loss 2.431\n",
      "509\n",
      "step: 509, train loss: 2.426, val loss 2.421\n",
      "510\n",
      "step: 510, train loss: 2.416, val loss 2.413\n",
      "511\n",
      "step: 511, train loss: 2.432, val loss 2.449\n",
      "512\n",
      "step: 512, train loss: 2.434, val loss 2.429\n",
      "513\n",
      "step: 513, train loss: 2.425, val loss 2.437\n",
      "514\n",
      "step: 514, train loss: 2.418, val loss 2.423\n",
      "515\n",
      "step: 515, train loss: 2.422, val loss 2.444\n",
      "516\n",
      "step: 516, train loss: 2.426, val loss 2.439\n",
      "517\n",
      "step: 517, train loss: 2.434, val loss 2.410\n",
      "518\n",
      "step: 518, train loss: 2.431, val loss 2.426\n",
      "519\n",
      "step: 519, train loss: 2.398, val loss 2.432\n",
      "520\n",
      "step: 520, train loss: 2.436, val loss 2.433\n",
      "521\n",
      "step: 521, train loss: 2.403, val loss 2.422\n",
      "522\n",
      "step: 522, train loss: 2.424, val loss 2.442\n",
      "523\n",
      "step: 523, train loss: 2.408, val loss 2.446\n",
      "524\n",
      "step: 524, train loss: 2.444, val loss 2.423\n",
      "525\n",
      "step: 525, train loss: 2.438, val loss 2.431\n",
      "526\n",
      "step: 526, train loss: 2.423, val loss 2.409\n",
      "527\n",
      "step: 527, train loss: 2.415, val loss 2.429\n",
      "528\n",
      "step: 528, train loss: 2.414, val loss 2.404\n",
      "529\n",
      "step: 529, train loss: 2.420, val loss 2.453\n",
      "530\n",
      "step: 530, train loss: 2.430, val loss 2.414\n",
      "531\n",
      "step: 531, train loss: 2.425, val loss 2.424\n",
      "532\n",
      "step: 532, train loss: 2.419, val loss 2.425\n",
      "533\n",
      "step: 533, train loss: 2.399, val loss 2.390\n",
      "534\n",
      "step: 534, train loss: 2.423, val loss 2.413\n",
      "535\n",
      "step: 535, train loss: 2.422, val loss 2.421\n",
      "536\n",
      "step: 536, train loss: 2.418, val loss 2.408\n",
      "537\n",
      "step: 537, train loss: 2.422, val loss 2.408\n",
      "538\n",
      "step: 538, train loss: 2.437, val loss 2.433\n",
      "539\n",
      "step: 539, train loss: 2.432, val loss 2.424\n",
      "540\n",
      "step: 540, train loss: 2.423, val loss 2.421\n",
      "541\n",
      "step: 541, train loss: 2.431, val loss 2.437\n",
      "542\n",
      "step: 542, train loss: 2.422, val loss 2.408\n",
      "543\n",
      "step: 543, train loss: 2.403, val loss 2.424\n",
      "544\n",
      "step: 544, train loss: 2.418, val loss 2.431\n",
      "545\n",
      "step: 545, train loss: 2.424, val loss 2.420\n",
      "546\n",
      "step: 546, train loss: 2.439, val loss 2.420\n",
      "547\n",
      "step: 547, train loss: 2.426, val loss 2.423\n",
      "548\n",
      "step: 548, train loss: 2.433, val loss 2.425\n",
      "549\n",
      "step: 549, train loss: 2.414, val loss 2.386\n",
      "550\n",
      "step: 550, train loss: 2.416, val loss 2.425\n",
      "551\n",
      "step: 551, train loss: 2.391, val loss 2.418\n",
      "552\n",
      "step: 552, train loss: 2.386, val loss 2.399\n",
      "553\n",
      "step: 553, train loss: 2.424, val loss 2.416\n",
      "554\n",
      "step: 554, train loss: 2.407, val loss 2.412\n",
      "555\n",
      "step: 555, train loss: 2.418, val loss 2.405\n",
      "556\n",
      "step: 556, train loss: 2.419, val loss 2.414\n",
      "557\n",
      "step: 557, train loss: 2.426, val loss 2.422\n",
      "558\n",
      "step: 558, train loss: 2.422, val loss 2.448\n",
      "559\n",
      "step: 559, train loss: 2.420, val loss 2.435\n",
      "560\n",
      "step: 560, train loss: 2.411, val loss 2.446\n",
      "561\n",
      "step: 561, train loss: 2.435, val loss 2.407\n",
      "562\n",
      "step: 562, train loss: 2.411, val loss 2.434\n",
      "563\n",
      "step: 563, train loss: 2.402, val loss 2.423\n",
      "564\n",
      "step: 564, train loss: 2.403, val loss 2.420\n",
      "565\n",
      "step: 565, train loss: 2.416, val loss 2.424\n",
      "566\n",
      "step: 566, train loss: 2.406, val loss 2.418\n",
      "567\n",
      "step: 567, train loss: 2.417, val loss 2.440\n",
      "568\n",
      "step: 568, train loss: 2.429, val loss 2.400\n",
      "569\n",
      "step: 569, train loss: 2.464, val loss 2.415\n",
      "570\n",
      "step: 570, train loss: 2.420, val loss 2.428\n",
      "571\n",
      "step: 571, train loss: 2.412, val loss 2.445\n",
      "572\n",
      "step: 572, train loss: 2.457, val loss 2.432\n",
      "573\n",
      "step: 573, train loss: 2.441, val loss 2.448\n",
      "574\n",
      "step: 574, train loss: 2.436, val loss 2.438\n",
      "575\n",
      "step: 575, train loss: 2.409, val loss 2.430\n",
      "576\n",
      "step: 576, train loss: 2.417, val loss 2.437\n",
      "577\n",
      "step: 577, train loss: 2.419, val loss 2.411\n",
      "578\n",
      "step: 578, train loss: 2.419, val loss 2.432\n",
      "579\n",
      "step: 579, train loss: 2.441, val loss 2.436\n",
      "580\n",
      "step: 580, train loss: 2.416, val loss 2.421\n",
      "581\n",
      "step: 581, train loss: 2.443, val loss 2.399\n",
      "582\n",
      "step: 582, train loss: 2.412, val loss 2.416\n",
      "583\n",
      "step: 583, train loss: 2.415, val loss 2.426\n",
      "584\n",
      "step: 584, train loss: 2.413, val loss 2.422\n",
      "585\n",
      "step: 585, train loss: 2.396, val loss 2.412\n",
      "586\n",
      "step: 586, train loss: 2.424, val loss 2.417\n",
      "587\n",
      "step: 587, train loss: 2.440, val loss 2.441\n",
      "588\n",
      "step: 588, train loss: 2.432, val loss 2.416\n",
      "589\n",
      "step: 589, train loss: 2.422, val loss 2.386\n",
      "590\n",
      "step: 590, train loss: 2.454, val loss 2.417\n",
      "591\n",
      "step: 591, train loss: 2.436, val loss 2.417\n",
      "592\n",
      "step: 592, train loss: 2.420, val loss 2.396\n",
      "593\n",
      "step: 593, train loss: 2.437, val loss 2.439\n",
      "594\n",
      "step: 594, train loss: 2.421, val loss 2.417\n",
      "595\n",
      "step: 595, train loss: 2.412, val loss 2.411\n",
      "596\n",
      "step: 596, train loss: 2.414, val loss 2.433\n",
      "597\n",
      "step: 597, train loss: 2.414, val loss 2.459\n",
      "598\n",
      "step: 598, train loss: 2.407, val loss 2.421\n",
      "599\n",
      "step: 599, train loss: 2.398, val loss 2.460\n",
      "600\n",
      "step: 600, train loss: 2.443, val loss 2.421\n",
      "601\n",
      "step: 601, train loss: 2.411, val loss 2.437\n",
      "602\n",
      "step: 602, train loss: 2.437, val loss 2.425\n",
      "603\n",
      "step: 603, train loss: 2.418, val loss 2.417\n",
      "604\n",
      "step: 604, train loss: 2.427, val loss 2.432\n",
      "605\n",
      "step: 605, train loss: 2.416, val loss 2.401\n",
      "606\n",
      "step: 606, train loss: 2.427, val loss 2.429\n",
      "607\n",
      "step: 607, train loss: 2.420, val loss 2.434\n",
      "608\n",
      "step: 608, train loss: 2.407, val loss 2.440\n",
      "609\n",
      "step: 609, train loss: 2.415, val loss 2.405\n",
      "610\n",
      "step: 610, train loss: 2.425, val loss 2.413\n",
      "611\n",
      "step: 611, train loss: 2.431, val loss 2.420\n",
      "612\n",
      "step: 612, train loss: 2.409, val loss 2.422\n",
      "613\n",
      "step: 613, train loss: 2.390, val loss 2.435\n",
      "614\n",
      "step: 614, train loss: 2.406, val loss 2.415\n",
      "615\n",
      "step: 615, train loss: 2.397, val loss 2.399\n",
      "616\n",
      "step: 616, train loss: 2.412, val loss 2.409\n",
      "617\n",
      "step: 617, train loss: 2.424, val loss 2.410\n",
      "618\n",
      "step: 618, train loss: 2.430, val loss 2.410\n",
      "619\n",
      "step: 619, train loss: 2.386, val loss 2.406\n",
      "620\n",
      "step: 620, train loss: 2.395, val loss 2.398\n",
      "621\n",
      "step: 621, train loss: 2.423, val loss 2.389\n",
      "622\n",
      "step: 622, train loss: 2.421, val loss 2.405\n",
      "623\n",
      "step: 623, train loss: 2.396, val loss 2.402\n",
      "624\n",
      "step: 624, train loss: 2.423, val loss 2.414\n",
      "625\n",
      "step: 625, train loss: 2.433, val loss 2.428\n",
      "626\n",
      "step: 626, train loss: 2.413, val loss 2.409\n",
      "627\n",
      "step: 627, train loss: 2.403, val loss 2.391\n",
      "628\n",
      "step: 628, train loss: 2.414, val loss 2.397\n",
      "629\n",
      "step: 629, train loss: 2.399, val loss 2.407\n",
      "630\n",
      "step: 630, train loss: 2.416, val loss 2.435\n",
      "631\n",
      "step: 631, train loss: 2.409, val loss 2.410\n",
      "632\n",
      "step: 632, train loss: 2.413, val loss 2.392\n",
      "633\n",
      "step: 633, train loss: 2.400, val loss 2.399\n",
      "634\n",
      "step: 634, train loss: 2.411, val loss 2.402\n",
      "635\n",
      "step: 635, train loss: 2.406, val loss 2.433\n",
      "636\n",
      "step: 636, train loss: 2.407, val loss 2.414\n",
      "637\n",
      "step: 637, train loss: 2.421, val loss 2.414\n",
      "638\n",
      "step: 638, train loss: 2.421, val loss 2.396\n",
      "639\n",
      "step: 639, train loss: 2.403, val loss 2.385\n",
      "640\n",
      "step: 640, train loss: 2.424, val loss 2.401\n",
      "641\n",
      "step: 641, train loss: 2.428, val loss 2.386\n",
      "642\n",
      "step: 642, train loss: 2.408, val loss 2.411\n",
      "643\n",
      "step: 643, train loss: 2.378, val loss 2.390\n",
      "644\n",
      "step: 644, train loss: 2.412, val loss 2.412\n",
      "645\n",
      "step: 645, train loss: 2.409, val loss 2.426\n",
      "646\n",
      "step: 646, train loss: 2.415, val loss 2.436\n",
      "647\n",
      "step: 647, train loss: 2.408, val loss 2.409\n",
      "648\n",
      "step: 648, train loss: 2.426, val loss 2.415\n",
      "649\n",
      "step: 649, train loss: 2.426, val loss 2.408\n",
      "650\n",
      "step: 650, train loss: 2.423, val loss 2.408\n",
      "651\n",
      "step: 651, train loss: 2.406, val loss 2.392\n",
      "652\n",
      "step: 652, train loss: 2.414, val loss 2.404\n",
      "653\n",
      "step: 653, train loss: 2.410, val loss 2.384\n",
      "654\n",
      "step: 654, train loss: 2.421, val loss 2.426\n",
      "655\n",
      "step: 655, train loss: 2.412, val loss 2.408\n",
      "656\n",
      "step: 656, train loss: 2.414, val loss 2.396\n",
      "657\n",
      "step: 657, train loss: 2.406, val loss 2.428\n",
      "658\n",
      "step: 658, train loss: 2.381, val loss 2.434\n",
      "659\n",
      "step: 659, train loss: 2.420, val loss 2.383\n",
      "660\n",
      "step: 660, train loss: 2.391, val loss 2.400\n",
      "661\n",
      "step: 661, train loss: 2.400, val loss 2.405\n",
      "662\n",
      "step: 662, train loss: 2.395, val loss 2.419\n",
      "663\n",
      "step: 663, train loss: 2.407, val loss 2.431\n",
      "664\n",
      "step: 664, train loss: 2.441, val loss 2.418\n",
      "665\n",
      "step: 665, train loss: 2.396, val loss 2.419\n",
      "666\n",
      "step: 666, train loss: 2.443, val loss 2.414\n",
      "667\n",
      "step: 667, train loss: 2.397, val loss 2.398\n",
      "668\n",
      "step: 668, train loss: 2.418, val loss 2.417\n",
      "669\n",
      "step: 669, train loss: 2.404, val loss 2.437\n",
      "670\n",
      "step: 670, train loss: 2.406, val loss 2.446\n",
      "671\n",
      "step: 671, train loss: 2.434, val loss 2.401\n",
      "672\n",
      "step: 672, train loss: 2.416, val loss 2.401\n",
      "673\n",
      "step: 673, train loss: 2.407, val loss 2.394\n",
      "674\n",
      "step: 674, train loss: 2.432, val loss 2.391\n",
      "675\n",
      "step: 675, train loss: 2.415, val loss 2.429\n",
      "676\n",
      "step: 676, train loss: 2.414, val loss 2.389\n",
      "677\n",
      "step: 677, train loss: 2.389, val loss 2.427\n",
      "678\n",
      "step: 678, train loss: 2.403, val loss 2.414\n",
      "679\n",
      "step: 679, train loss: 2.422, val loss 2.400\n",
      "680\n",
      "step: 680, train loss: 2.408, val loss 2.421\n",
      "681\n",
      "step: 681, train loss: 2.398, val loss 2.428\n",
      "682\n",
      "step: 682, train loss: 2.382, val loss 2.397\n",
      "683\n",
      "step: 683, train loss: 2.429, val loss 2.420\n",
      "684\n",
      "step: 684, train loss: 2.410, val loss 2.405\n",
      "685\n",
      "step: 685, train loss: 2.416, val loss 2.419\n",
      "686\n",
      "step: 686, train loss: 2.422, val loss 2.403\n",
      "687\n",
      "step: 687, train loss: 2.411, val loss 2.427\n",
      "688\n",
      "step: 688, train loss: 2.430, val loss 2.420\n",
      "689\n",
      "step: 689, train loss: 2.377, val loss 2.417\n",
      "690\n",
      "step: 690, train loss: 2.420, val loss 2.405\n",
      "691\n",
      "step: 691, train loss: 2.405, val loss 2.408\n",
      "692\n",
      "step: 692, train loss: 2.388, val loss 2.410\n",
      "693\n",
      "step: 693, train loss: 2.395, val loss 2.413\n",
      "694\n",
      "step: 694, train loss: 2.407, val loss 2.428\n",
      "695\n",
      "step: 695, train loss: 2.371, val loss 2.417\n",
      "696\n",
      "step: 696, train loss: 2.413, val loss 2.395\n",
      "697\n",
      "step: 697, train loss: 2.413, val loss 2.400\n",
      "698\n",
      "step: 698, train loss: 2.402, val loss 2.434\n",
      "699\n",
      "step: 699, train loss: 2.428, val loss 2.405\n",
      "700\n",
      "step: 700, train loss: 2.417, val loss 2.414\n",
      "701\n",
      "step: 701, train loss: 2.425, val loss 2.410\n",
      "702\n",
      "step: 702, train loss: 2.392, val loss 2.396\n",
      "703\n",
      "step: 703, train loss: 2.374, val loss 2.405\n",
      "704\n",
      "step: 704, train loss: 2.390, val loss 2.403\n",
      "705\n",
      "step: 705, train loss: 2.385, val loss 2.397\n",
      "706\n",
      "step: 706, train loss: 2.433, val loss 2.423\n",
      "707\n",
      "step: 707, train loss: 2.432, val loss 2.406\n",
      "708\n",
      "step: 708, train loss: 2.401, val loss 2.401\n",
      "709\n",
      "step: 709, train loss: 2.412, val loss 2.402\n",
      "710\n",
      "step: 710, train loss: 2.429, val loss 2.396\n",
      "711\n",
      "step: 711, train loss: 2.415, val loss 2.401\n",
      "712\n",
      "step: 712, train loss: 2.418, val loss 2.407\n",
      "713\n",
      "step: 713, train loss: 2.396, val loss 2.442\n",
      "714\n",
      "step: 714, train loss: 2.423, val loss 2.397\n",
      "715\n",
      "step: 715, train loss: 2.405, val loss 2.429\n",
      "716\n",
      "step: 716, train loss: 2.413, val loss 2.400\n",
      "717\n",
      "step: 717, train loss: 2.403, val loss 2.414\n",
      "718\n",
      "step: 718, train loss: 2.455, val loss 2.427\n",
      "719\n",
      "step: 719, train loss: 2.419, val loss 2.428\n",
      "720\n",
      "step: 720, train loss: 2.414, val loss 2.408\n",
      "721\n",
      "step: 721, train loss: 2.406, val loss 2.414\n",
      "722\n",
      "step: 722, train loss: 2.394, val loss 2.410\n",
      "723\n",
      "step: 723, train loss: 2.429, val loss 2.407\n",
      "724\n",
      "step: 724, train loss: 2.416, val loss 2.400\n",
      "725\n",
      "step: 725, train loss: 2.411, val loss 2.408\n",
      "726\n",
      "step: 726, train loss: 2.400, val loss 2.405\n",
      "727\n",
      "step: 727, train loss: 2.398, val loss 2.403\n",
      "728\n",
      "step: 728, train loss: 2.386, val loss 2.413\n",
      "729\n",
      "step: 729, train loss: 2.406, val loss 2.408\n",
      "730\n",
      "step: 730, train loss: 2.391, val loss 2.390\n",
      "731\n",
      "step: 731, train loss: 2.406, val loss 2.383\n",
      "732\n",
      "step: 732, train loss: 2.396, val loss 2.412\n",
      "733\n",
      "step: 733, train loss: 2.401, val loss 2.431\n",
      "734\n",
      "step: 734, train loss: 2.426, val loss 2.414\n",
      "735\n",
      "step: 735, train loss: 2.396, val loss 2.398\n",
      "736\n",
      "step: 736, train loss: 2.406, val loss 2.404\n",
      "737\n",
      "step: 737, train loss: 2.403, val loss 2.419\n",
      "738\n",
      "step: 738, train loss: 2.412, val loss 2.401\n",
      "739\n",
      "step: 739, train loss: 2.415, val loss 2.404\n",
      "740\n",
      "step: 740, train loss: 2.395, val loss 2.393\n",
      "741\n",
      "step: 741, train loss: 2.404, val loss 2.399\n",
      "742\n",
      "step: 742, train loss: 2.399, val loss 2.390\n",
      "743\n",
      "step: 743, train loss: 2.412, val loss 2.404\n",
      "744\n",
      "step: 744, train loss: 2.392, val loss 2.437\n",
      "745\n",
      "step: 745, train loss: 2.378, val loss 2.425\n",
      "746\n",
      "step: 746, train loss: 2.380, val loss 2.404\n",
      "747\n",
      "step: 747, train loss: 2.418, val loss 2.415\n",
      "748\n",
      "step: 748, train loss: 2.441, val loss 2.404\n",
      "749\n",
      "step: 749, train loss: 2.408, val loss 2.390\n",
      "750\n",
      "step: 750, train loss: 2.420, val loss 2.420\n",
      "751\n",
      "step: 751, train loss: 2.395, val loss 2.394\n",
      "752\n",
      "step: 752, train loss: 2.402, val loss 2.409\n",
      "753\n",
      "step: 753, train loss: 2.402, val loss 2.403\n",
      "754\n",
      "step: 754, train loss: 2.423, val loss 2.408\n",
      "755\n",
      "step: 755, train loss: 2.416, val loss 2.418\n",
      "756\n",
      "step: 756, train loss: 2.375, val loss 2.378\n",
      "757\n",
      "step: 757, train loss: 2.426, val loss 2.408\n",
      "758\n",
      "step: 758, train loss: 2.384, val loss 2.433\n",
      "759\n",
      "step: 759, train loss: 2.389, val loss 2.417\n",
      "760\n",
      "step: 760, train loss: 2.381, val loss 2.396\n",
      "761\n",
      "step: 761, train loss: 2.410, val loss 2.410\n",
      "762\n",
      "step: 762, train loss: 2.389, val loss 2.425\n",
      "763\n",
      "step: 763, train loss: 2.391, val loss 2.410\n",
      "764\n",
      "step: 764, train loss: 2.401, val loss 2.435\n",
      "765\n",
      "step: 765, train loss: 2.388, val loss 2.403\n",
      "766\n",
      "step: 766, train loss: 2.385, val loss 2.413\n",
      "767\n",
      "step: 767, train loss: 2.402, val loss 2.411\n",
      "768\n",
      "step: 768, train loss: 2.399, val loss 2.395\n",
      "769\n",
      "step: 769, train loss: 2.390, val loss 2.389\n",
      "770\n",
      "step: 770, train loss: 2.398, val loss 2.425\n",
      "771\n",
      "step: 771, train loss: 2.383, val loss 2.381\n",
      "772\n",
      "step: 772, train loss: 2.394, val loss 2.374\n",
      "773\n",
      "step: 773, train loss: 2.379, val loss 2.401\n",
      "774\n",
      "step: 774, train loss: 2.401, val loss 2.396\n",
      "775\n",
      "step: 775, train loss: 2.441, val loss 2.409\n",
      "776\n",
      "step: 776, train loss: 2.394, val loss 2.405\n",
      "777\n",
      "step: 777, train loss: 2.405, val loss 2.382\n",
      "778\n",
      "step: 778, train loss: 2.432, val loss 2.389\n",
      "779\n",
      "step: 779, train loss: 2.410, val loss 2.399\n",
      "780\n",
      "step: 780, train loss: 2.387, val loss 2.416\n",
      "781\n",
      "step: 781, train loss: 2.392, val loss 2.390\n",
      "782\n",
      "step: 782, train loss: 2.428, val loss 2.407\n",
      "783\n",
      "step: 783, train loss: 2.394, val loss 2.386\n",
      "784\n",
      "step: 784, train loss: 2.417, val loss 2.410\n",
      "785\n",
      "step: 785, train loss: 2.388, val loss 2.396\n",
      "786\n",
      "step: 786, train loss: 2.422, val loss 2.386\n",
      "787\n",
      "step: 787, train loss: 2.391, val loss 2.403\n",
      "788\n",
      "step: 788, train loss: 2.412, val loss 2.395\n",
      "789\n",
      "step: 789, train loss: 2.395, val loss 2.427\n",
      "790\n",
      "step: 790, train loss: 2.414, val loss 2.419\n",
      "791\n",
      "step: 791, train loss: 2.385, val loss 2.409\n",
      "792\n",
      "step: 792, train loss: 2.396, val loss 2.426\n",
      "793\n",
      "step: 793, train loss: 2.416, val loss 2.454\n",
      "794\n",
      "step: 794, train loss: 2.424, val loss 2.475\n",
      "795\n",
      "step: 795, train loss: 2.389, val loss 2.420\n",
      "796\n",
      "step: 796, train loss: 2.428, val loss 2.403\n",
      "797\n",
      "step: 797, train loss: 2.414, val loss 2.406\n",
      "798\n",
      "step: 798, train loss: 2.393, val loss 2.390\n",
      "799\n",
      "step: 799, train loss: 2.412, val loss 2.408\n",
      "800\n",
      "step: 800, train loss: 2.388, val loss 2.427\n",
      "801\n",
      "step: 801, train loss: 2.421, val loss 2.411\n",
      "802\n",
      "step: 802, train loss: 2.384, val loss 2.400\n",
      "803\n",
      "step: 803, train loss: 2.420, val loss 2.424\n",
      "804\n",
      "step: 804, train loss: 2.441, val loss 2.392\n",
      "805\n",
      "step: 805, train loss: 2.460, val loss 2.402\n",
      "806\n",
      "step: 806, train loss: 2.395, val loss 2.379\n",
      "807\n",
      "step: 807, train loss: 2.410, val loss 2.396\n",
      "808\n",
      "step: 808, train loss: 2.432, val loss 2.391\n",
      "809\n",
      "step: 809, train loss: 2.411, val loss 2.397\n",
      "810\n",
      "step: 810, train loss: 2.395, val loss 2.400\n",
      "811\n",
      "step: 811, train loss: 2.390, val loss 2.401\n",
      "812\n",
      "step: 812, train loss: 2.418, val loss 2.388\n",
      "813\n",
      "step: 813, train loss: 2.377, val loss 2.406\n",
      "814\n",
      "step: 814, train loss: 2.390, val loss 2.380\n",
      "815\n",
      "step: 815, train loss: 2.397, val loss 2.386\n",
      "816\n",
      "step: 816, train loss: 2.403, val loss 2.389\n",
      "817\n",
      "step: 817, train loss: 2.393, val loss 2.396\n",
      "818\n",
      "step: 818, train loss: 2.387, val loss 2.413\n",
      "819\n",
      "step: 819, train loss: 2.409, val loss 2.384\n",
      "820\n",
      "step: 820, train loss: 2.386, val loss 2.422\n",
      "821\n",
      "step: 821, train loss: 2.392, val loss 2.378\n",
      "822\n",
      "step: 822, train loss: 2.399, val loss 2.392\n",
      "823\n",
      "step: 823, train loss: 2.410, val loss 2.398\n",
      "824\n",
      "step: 824, train loss: 2.400, val loss 2.366\n",
      "825\n",
      "step: 825, train loss: 2.422, val loss 2.394\n",
      "826\n",
      "step: 826, train loss: 2.380, val loss 2.399\n",
      "827\n",
      "step: 827, train loss: 2.395, val loss 2.402\n",
      "828\n",
      "step: 828, train loss: 2.411, val loss 2.379\n",
      "829\n",
      "step: 829, train loss: 2.367, val loss 2.389\n",
      "830\n",
      "step: 830, train loss: 2.397, val loss 2.380\n",
      "831\n",
      "step: 831, train loss: 2.403, val loss 2.412\n",
      "832\n",
      "step: 832, train loss: 2.399, val loss 2.396\n",
      "833\n",
      "step: 833, train loss: 2.391, val loss 2.408\n",
      "834\n",
      "step: 834, train loss: 2.429, val loss 2.384\n",
      "835\n",
      "step: 835, train loss: 2.378, val loss 2.408\n",
      "836\n",
      "step: 836, train loss: 2.396, val loss 2.387\n",
      "837\n",
      "step: 837, train loss: 2.391, val loss 2.394\n",
      "838\n",
      "step: 838, train loss: 2.369, val loss 2.391\n",
      "839\n",
      "step: 839, train loss: 2.367, val loss 2.386\n",
      "840\n",
      "step: 840, train loss: 2.391, val loss 2.384\n",
      "841\n",
      "step: 841, train loss: 2.416, val loss 2.391\n",
      "842\n",
      "step: 842, train loss: 2.409, val loss 2.389\n",
      "843\n",
      "step: 843, train loss: 2.385, val loss 2.390\n",
      "844\n",
      "step: 844, train loss: 2.435, val loss 2.395\n",
      "845\n",
      "step: 845, train loss: 2.380, val loss 2.368\n",
      "846\n",
      "step: 846, train loss: 2.393, val loss 2.394\n",
      "847\n",
      "step: 847, train loss: 2.409, val loss 2.417\n",
      "848\n",
      "step: 848, train loss: 2.397, val loss 2.391\n",
      "849\n",
      "step: 849, train loss: 2.389, val loss 2.392\n",
      "850\n",
      "step: 850, train loss: 2.377, val loss 2.448\n",
      "851\n",
      "step: 851, train loss: 2.382, val loss 2.388\n",
      "852\n",
      "step: 852, train loss: 2.390, val loss 2.400\n",
      "853\n",
      "step: 853, train loss: 2.421, val loss 2.401\n",
      "854\n",
      "step: 854, train loss: 2.411, val loss 2.393\n",
      "855\n",
      "step: 855, train loss: 2.392, val loss 2.380\n",
      "856\n",
      "step: 856, train loss: 2.412, val loss 2.405\n",
      "857\n",
      "step: 857, train loss: 2.408, val loss 2.423\n",
      "858\n",
      "step: 858, train loss: 2.400, val loss 2.413\n",
      "859\n",
      "step: 859, train loss: 2.414, val loss 2.397\n",
      "860\n",
      "step: 860, train loss: 2.415, val loss 2.420\n",
      "861\n",
      "step: 861, train loss: 2.438, val loss 2.407\n",
      "862\n",
      "step: 862, train loss: 2.385, val loss 2.377\n",
      "863\n",
      "step: 863, train loss: 2.397, val loss 2.402\n",
      "864\n",
      "step: 864, train loss: 2.396, val loss 2.394\n",
      "865\n",
      "step: 865, train loss: 2.396, val loss 2.389\n",
      "866\n",
      "step: 866, train loss: 2.378, val loss 2.385\n",
      "867\n",
      "step: 867, train loss: 2.406, val loss 2.383\n",
      "868\n",
      "step: 868, train loss: 2.396, val loss 2.385\n",
      "869\n",
      "step: 869, train loss: 2.407, val loss 2.426\n",
      "870\n",
      "step: 870, train loss: 2.389, val loss 2.399\n",
      "871\n",
      "step: 871, train loss: 2.405, val loss 2.390\n",
      "872\n",
      "step: 872, train loss: 2.382, val loss 2.387\n",
      "873\n",
      "step: 873, train loss: 2.414, val loss 2.392\n",
      "874\n",
      "step: 874, train loss: 2.388, val loss 2.429\n",
      "875\n",
      "step: 875, train loss: 2.395, val loss 2.409\n",
      "876\n",
      "step: 876, train loss: 2.411, val loss 2.394\n",
      "877\n",
      "step: 877, train loss: 2.403, val loss 2.407\n",
      "878\n",
      "step: 878, train loss: 2.397, val loss 2.407\n",
      "879\n",
      "step: 879, train loss: 2.407, val loss 2.411\n",
      "880\n",
      "step: 880, train loss: 2.410, val loss 2.399\n",
      "881\n",
      "step: 881, train loss: 2.415, val loss 2.436\n",
      "882\n",
      "step: 882, train loss: 2.400, val loss 2.406\n",
      "883\n",
      "step: 883, train loss: 2.398, val loss 2.402\n",
      "884\n",
      "step: 884, train loss: 2.405, val loss 2.438\n",
      "885\n",
      "step: 885, train loss: 2.411, val loss 2.392\n",
      "886\n",
      "step: 886, train loss: 2.404, val loss 2.393\n",
      "887\n",
      "step: 887, train loss: 2.391, val loss 2.384\n",
      "888\n",
      "step: 888, train loss: 2.414, val loss 2.399\n",
      "889\n",
      "step: 889, train loss: 2.371, val loss 2.383\n",
      "890\n",
      "step: 890, train loss: 2.405, val loss 2.390\n",
      "891\n",
      "step: 891, train loss: 2.383, val loss 2.401\n",
      "892\n",
      "step: 892, train loss: 2.400, val loss 2.413\n",
      "893\n",
      "step: 893, train loss: 2.410, val loss 2.395\n",
      "894\n",
      "step: 894, train loss: 2.399, val loss 2.391\n",
      "895\n",
      "step: 895, train loss: 2.395, val loss 2.383\n",
      "896\n",
      "step: 896, train loss: 2.410, val loss 2.431\n",
      "897\n",
      "step: 897, train loss: 2.430, val loss 2.388\n",
      "898\n",
      "step: 898, train loss: 2.417, val loss 2.420\n",
      "899\n",
      "step: 899, train loss: 2.410, val loss 2.383\n",
      "900\n",
      "step: 900, train loss: 2.435, val loss 2.388\n",
      "901\n",
      "step: 901, train loss: 2.385, val loss 2.412\n",
      "902\n",
      "step: 902, train loss: 2.391, val loss 2.401\n",
      "903\n",
      "step: 903, train loss: 2.402, val loss 2.382\n",
      "904\n",
      "step: 904, train loss: 2.378, val loss 2.403\n",
      "905\n",
      "step: 905, train loss: 2.406, val loss 2.413\n",
      "906\n",
      "step: 906, train loss: 2.399, val loss 2.385\n",
      "907\n",
      "step: 907, train loss: 2.376, val loss 2.399\n",
      "908\n",
      "step: 908, train loss: 2.423, val loss 2.411\n",
      "909\n",
      "step: 909, train loss: 2.411, val loss 2.388\n",
      "910\n",
      "step: 910, train loss: 2.381, val loss 2.384\n",
      "911\n",
      "step: 911, train loss: 2.390, val loss 2.391\n",
      "912\n",
      "step: 912, train loss: 2.403, val loss 2.390\n",
      "913\n",
      "step: 913, train loss: 2.380, val loss 2.404\n",
      "914\n",
      "step: 914, train loss: 2.407, val loss 2.402\n",
      "915\n",
      "step: 915, train loss: 2.408, val loss 2.385\n",
      "916\n",
      "step: 916, train loss: 2.398, val loss 2.396\n",
      "917\n",
      "step: 917, train loss: 2.378, val loss 2.414\n",
      "918\n",
      "step: 918, train loss: 2.377, val loss 2.383\n",
      "919\n",
      "step: 919, train loss: 2.401, val loss 2.397\n",
      "920\n",
      "step: 920, train loss: 2.409, val loss 2.386\n",
      "921\n",
      "step: 921, train loss: 2.397, val loss 2.382\n",
      "922\n",
      "step: 922, train loss: 2.393, val loss 2.380\n",
      "923\n",
      "step: 923, train loss: 2.407, val loss 2.392\n",
      "924\n",
      "step: 924, train loss: 2.415, val loss 2.418\n",
      "925\n",
      "step: 925, train loss: 2.416, val loss 2.411\n",
      "926\n",
      "step: 926, train loss: 2.395, val loss 2.369\n",
      "927\n",
      "step: 927, train loss: 2.396, val loss 2.398\n",
      "928\n",
      "step: 928, train loss: 2.376, val loss 2.379\n",
      "929\n",
      "step: 929, train loss: 2.381, val loss 2.388\n",
      "930\n",
      "step: 930, train loss: 2.387, val loss 2.397\n",
      "931\n",
      "step: 931, train loss: 2.388, val loss 2.393\n",
      "932\n",
      "step: 932, train loss: 2.383, val loss 2.374\n",
      "933\n",
      "step: 933, train loss: 2.381, val loss 2.367\n",
      "934\n",
      "step: 934, train loss: 2.390, val loss 2.367\n",
      "935\n",
      "step: 935, train loss: 2.388, val loss 2.387\n",
      "936\n",
      "step: 936, train loss: 2.366, val loss 2.384\n",
      "937\n",
      "step: 937, train loss: 2.398, val loss 2.405\n",
      "938\n",
      "step: 938, train loss: 2.395, val loss 2.414\n",
      "939\n",
      "step: 939, train loss: 2.392, val loss 2.386\n",
      "940\n",
      "step: 940, train loss: 2.378, val loss 2.376\n",
      "941\n",
      "step: 941, train loss: 2.362, val loss 2.388\n",
      "942\n",
      "step: 942, train loss: 2.394, val loss 2.380\n",
      "943\n",
      "step: 943, train loss: 2.370, val loss 2.380\n",
      "944\n",
      "step: 944, train loss: 2.388, val loss 2.400\n",
      "945\n",
      "step: 945, train loss: 2.394, val loss 2.385\n",
      "946\n",
      "step: 946, train loss: 2.391, val loss 2.387\n",
      "947\n",
      "step: 947, train loss: 2.382, val loss 2.395\n",
      "948\n",
      "step: 948, train loss: 2.378, val loss 2.388\n",
      "949\n",
      "step: 949, train loss: 2.372, val loss 2.401\n",
      "950\n",
      "step: 950, train loss: 2.394, val loss 2.376\n",
      "951\n",
      "step: 951, train loss: 2.383, val loss 2.412\n",
      "952\n",
      "step: 952, train loss: 2.393, val loss 2.391\n",
      "953\n",
      "step: 953, train loss: 2.373, val loss 2.399\n",
      "954\n",
      "step: 954, train loss: 2.405, val loss 2.402\n",
      "955\n",
      "step: 955, train loss: 2.389, val loss 2.388\n",
      "956\n",
      "step: 956, train loss: 2.393, val loss 2.386\n",
      "957\n",
      "step: 957, train loss: 2.379, val loss 2.411\n",
      "958\n",
      "step: 958, train loss: 2.375, val loss 2.392\n",
      "959\n",
      "step: 959, train loss: 2.379, val loss 2.406\n",
      "960\n",
      "step: 960, train loss: 2.388, val loss 2.420\n",
      "961\n",
      "step: 961, train loss: 2.383, val loss 2.401\n",
      "962\n",
      "step: 962, train loss: 2.386, val loss 2.374\n",
      "963\n",
      "step: 963, train loss: 2.391, val loss 2.375\n",
      "964\n",
      "step: 964, train loss: 2.391, val loss 2.387\n",
      "965\n",
      "step: 965, train loss: 2.390, val loss 2.400\n",
      "966\n",
      "step: 966, train loss: 2.394, val loss 2.376\n",
      "967\n",
      "step: 967, train loss: 2.390, val loss 2.403\n",
      "968\n",
      "step: 968, train loss: 2.390, val loss 2.375\n",
      "969\n",
      "step: 969, train loss: 2.396, val loss 2.431\n",
      "970\n",
      "step: 970, train loss: 2.406, val loss 2.405\n",
      "971\n",
      "step: 971, train loss: 2.386, val loss 2.384\n",
      "972\n",
      "step: 972, train loss: 2.382, val loss 2.393\n",
      "973\n",
      "step: 973, train loss: 2.397, val loss 2.398\n",
      "974\n",
      "step: 974, train loss: 2.389, val loss 2.387\n",
      "975\n",
      "step: 975, train loss: 2.394, val loss 2.374\n",
      "976\n",
      "step: 976, train loss: 2.376, val loss 2.389\n",
      "977\n",
      "step: 977, train loss: 2.382, val loss 2.411\n",
      "978\n",
      "step: 978, train loss: 2.369, val loss 2.391\n",
      "979\n",
      "step: 979, train loss: 2.374, val loss 2.411\n",
      "980\n",
      "step: 980, train loss: 2.388, val loss 2.391\n",
      "981\n",
      "step: 981, train loss: 2.397, val loss 2.378\n",
      "982\n",
      "step: 982, train loss: 2.421, val loss 2.400\n",
      "983\n",
      "step: 983, train loss: 2.398, val loss 2.388\n",
      "984\n",
      "step: 984, train loss: 2.400, val loss 2.387\n",
      "985\n",
      "step: 985, train loss: 2.390, val loss 2.391\n",
      "986\n",
      "step: 986, train loss: 2.411, val loss 2.391\n",
      "987\n",
      "step: 987, train loss: 2.393, val loss 2.388\n",
      "988\n",
      "step: 988, train loss: 2.363, val loss 2.407\n",
      "989\n",
      "step: 989, train loss: 2.365, val loss 2.376\n",
      "990\n",
      "step: 990, train loss: 2.388, val loss 2.394\n",
      "991\n",
      "step: 991, train loss: 2.381, val loss 2.403\n",
      "992\n",
      "step: 992, train loss: 2.415, val loss 2.388\n",
      "993\n",
      "step: 993, train loss: 2.391, val loss 2.399\n",
      "994\n",
      "step: 994, train loss: 2.400, val loss 2.398\n",
      "995\n",
      "step: 995, train loss: 2.379, val loss 2.370\n",
      "996\n",
      "step: 996, train loss: 2.378, val loss 2.394\n",
      "997\n",
      "step: 997, train loss: 2.388, val loss 2.389\n",
      "998\n",
      "step: 998, train loss: 2.382, val loss 2.386\n",
      "999\n",
      "step: 999, train loss: 2.401, val loss 2.371\n",
      "1000\n",
      "step: 1000, train loss: 2.388, val loss 2.356\n",
      "1001\n",
      "step: 1001, train loss: 2.378, val loss 2.406\n",
      "1002\n",
      "step: 1002, train loss: 2.389, val loss 2.384\n",
      "1003\n",
      "step: 1003, train loss: 2.395, val loss 2.360\n",
      "1004\n",
      "step: 1004, train loss: 2.378, val loss 2.387\n",
      "1005\n",
      "step: 1005, train loss: 2.379, val loss 2.385\n",
      "1006\n",
      "step: 1006, train loss: 2.391, val loss 2.367\n",
      "1007\n",
      "step: 1007, train loss: 2.377, val loss 2.373\n",
      "1008\n",
      "step: 1008, train loss: 2.389, val loss 2.399\n",
      "1009\n",
      "step: 1009, train loss: 2.399, val loss 2.401\n",
      "1010\n",
      "step: 1010, train loss: 2.414, val loss 2.380\n",
      "1011\n",
      "step: 1011, train loss: 2.401, val loss 2.385\n",
      "1012\n",
      "step: 1012, train loss: 2.372, val loss 2.365\n",
      "1013\n",
      "step: 1013, train loss: 2.397, val loss 2.383\n",
      "1014\n",
      "step: 1014, train loss: 2.381, val loss 2.388\n",
      "1015\n",
      "step: 1015, train loss: 2.399, val loss 2.394\n",
      "1016\n",
      "step: 1016, train loss: 2.387, val loss 2.392\n",
      "1017\n",
      "step: 1017, train loss: 2.384, val loss 2.396\n",
      "1018\n",
      "step: 1018, train loss: 2.395, val loss 2.386\n",
      "1019\n",
      "step: 1019, train loss: 2.383, val loss 2.383\n",
      "1020\n",
      "step: 1020, train loss: 2.387, val loss 2.384\n",
      "1021\n",
      "step: 1021, train loss: 2.374, val loss 2.378\n",
      "1022\n",
      "step: 1022, train loss: 2.383, val loss 2.398\n",
      "1023\n",
      "step: 1023, train loss: 2.400, val loss 2.367\n",
      "1024\n",
      "step: 1024, train loss: 2.400, val loss 2.379\n",
      "1025\n",
      "step: 1025, train loss: 2.372, val loss 2.390\n",
      "1026\n",
      "step: 1026, train loss: 2.377, val loss 2.380\n",
      "1027\n",
      "step: 1027, train loss: 2.377, val loss 2.394\n",
      "1028\n",
      "step: 1028, train loss: 2.380, val loss 2.389\n",
      "1029\n",
      "step: 1029, train loss: 2.405, val loss 2.419\n",
      "1030\n",
      "step: 1030, train loss: 2.412, val loss 2.406\n",
      "1031\n",
      "step: 1031, train loss: 2.386, val loss 2.388\n",
      "1032\n",
      "step: 1032, train loss: 2.391, val loss 2.391\n",
      "1033\n",
      "step: 1033, train loss: 2.423, val loss 2.389\n",
      "1034\n",
      "step: 1034, train loss: 2.407, val loss 2.380\n",
      "1035\n",
      "step: 1035, train loss: 2.389, val loss 2.367\n",
      "1036\n",
      "step: 1036, train loss: 2.391, val loss 2.397\n",
      "1037\n",
      "step: 1037, train loss: 2.408, val loss 2.379\n",
      "1038\n",
      "step: 1038, train loss: 2.382, val loss 2.382\n",
      "1039\n",
      "step: 1039, train loss: 2.394, val loss 2.367\n",
      "1040\n",
      "step: 1040, train loss: 2.385, val loss 2.405\n",
      "1041\n",
      "step: 1041, train loss: 2.410, val loss 2.394\n",
      "1042\n",
      "step: 1042, train loss: 2.368, val loss 2.388\n",
      "1043\n",
      "step: 1043, train loss: 2.370, val loss 2.359\n",
      "1044\n",
      "step: 1044, train loss: 2.384, val loss 2.393\n",
      "1045\n",
      "step: 1045, train loss: 2.370, val loss 2.393\n",
      "1046\n",
      "step: 1046, train loss: 2.370, val loss 2.392\n",
      "1047\n",
      "step: 1047, train loss: 2.381, val loss 2.401\n",
      "1048\n",
      "step: 1048, train loss: 2.408, val loss 2.421\n",
      "1049\n",
      "step: 1049, train loss: 2.374, val loss 2.393\n",
      "1050\n",
      "step: 1050, train loss: 2.386, val loss 2.406\n",
      "1051\n",
      "step: 1051, train loss: 2.402, val loss 2.381\n",
      "1052\n",
      "step: 1052, train loss: 2.365, val loss 2.385\n",
      "1053\n",
      "step: 1053, train loss: 2.384, val loss 2.366\n",
      "1054\n",
      "step: 1054, train loss: 2.395, val loss 2.395\n",
      "1055\n",
      "step: 1055, train loss: 2.376, val loss 2.385\n",
      "1056\n",
      "step: 1056, train loss: 2.378, val loss 2.386\n",
      "1057\n",
      "step: 1057, train loss: 2.420, val loss 2.395\n",
      "1058\n",
      "step: 1058, train loss: 2.414, val loss 2.390\n",
      "1059\n",
      "step: 1059, train loss: 2.372, val loss 2.397\n",
      "1060\n",
      "step: 1060, train loss: 2.400, val loss 2.364\n",
      "1061\n",
      "step: 1061, train loss: 2.379, val loss 2.381\n",
      "1062\n",
      "step: 1062, train loss: 2.398, val loss 2.363\n",
      "1063\n",
      "step: 1063, train loss: 2.383, val loss 2.371\n",
      "1064\n",
      "step: 1064, train loss: 2.387, val loss 2.398\n",
      "1065\n",
      "step: 1065, train loss: 2.382, val loss 2.395\n",
      "1066\n",
      "step: 1066, train loss: 2.405, val loss 2.383\n",
      "1067\n",
      "step: 1067, train loss: 2.381, val loss 2.365\n",
      "1068\n",
      "step: 1068, train loss: 2.383, val loss 2.366\n",
      "1069\n",
      "step: 1069, train loss: 2.372, val loss 2.374\n",
      "1070\n",
      "step: 1070, train loss: 2.389, val loss 2.391\n",
      "1071\n",
      "step: 1071, train loss: 2.386, val loss 2.355\n",
      "1072\n",
      "step: 1072, train loss: 2.399, val loss 2.403\n",
      "1073\n",
      "step: 1073, train loss: 2.383, val loss 2.414\n",
      "1074\n",
      "step: 1074, train loss: 2.410, val loss 2.399\n",
      "1075\n",
      "step: 1075, train loss: 2.380, val loss 2.392\n",
      "1076\n",
      "step: 1076, train loss: 2.380, val loss 2.393\n",
      "1077\n",
      "step: 1077, train loss: 2.374, val loss 2.382\n",
      "1078\n",
      "step: 1078, train loss: 2.372, val loss 2.417\n",
      "1079\n",
      "step: 1079, train loss: 2.390, val loss 2.375\n",
      "1080\n",
      "step: 1080, train loss: 2.373, val loss 2.379\n",
      "1081\n",
      "step: 1081, train loss: 2.389, val loss 2.349\n",
      "1082\n",
      "step: 1082, train loss: 2.378, val loss 2.377\n",
      "1083\n",
      "step: 1083, train loss: 2.388, val loss 2.407\n",
      "1084\n",
      "step: 1084, train loss: 2.370, val loss 2.392\n",
      "1085\n",
      "step: 1085, train loss: 2.361, val loss 2.397\n",
      "1086\n",
      "step: 1086, train loss: 2.431, val loss 2.415\n",
      "1087\n",
      "step: 1087, train loss: 2.372, val loss 2.380\n",
      "1088\n",
      "step: 1088, train loss: 2.387, val loss 2.392\n",
      "1089\n",
      "step: 1089, train loss: 2.394, val loss 2.406\n",
      "1090\n",
      "step: 1090, train loss: 2.385, val loss 2.401\n",
      "1091\n",
      "step: 1091, train loss: 2.394, val loss 2.392\n",
      "1092\n",
      "step: 1092, train loss: 2.375, val loss 2.393\n",
      "1093\n",
      "step: 1093, train loss: 2.402, val loss 2.407\n",
      "1094\n",
      "step: 1094, train loss: 2.382, val loss 2.377\n",
      "1095\n",
      "step: 1095, train loss: 2.374, val loss 2.373\n",
      "1096\n",
      "step: 1096, train loss: 2.387, val loss 2.378\n",
      "1097\n",
      "step: 1097, train loss: 2.375, val loss 2.408\n",
      "1098\n",
      "step: 1098, train loss: 2.378, val loss 2.399\n",
      "1099\n",
      "step: 1099, train loss: 2.372, val loss 2.379\n",
      "1100\n",
      "step: 1100, train loss: 2.390, val loss 2.380\n",
      "1101\n",
      "step: 1101, train loss: 2.362, val loss 2.364\n",
      "1102\n",
      "step: 1102, train loss: 2.383, val loss 2.368\n",
      "1103\n",
      "step: 1103, train loss: 2.387, val loss 2.406\n",
      "1104\n",
      "step: 1104, train loss: 2.389, val loss 2.385\n",
      "1105\n",
      "step: 1105, train loss: 2.371, val loss 2.370\n",
      "1106\n",
      "step: 1106, train loss: 2.414, val loss 2.386\n",
      "1107\n",
      "step: 1107, train loss: 2.393, val loss 2.401\n",
      "1108\n",
      "step: 1108, train loss: 2.384, val loss 2.406\n",
      "1109\n",
      "step: 1109, train loss: 2.390, val loss 2.383\n",
      "1110\n",
      "step: 1110, train loss: 2.392, val loss 2.397\n",
      "1111\n",
      "step: 1111, train loss: 2.392, val loss 2.394\n",
      "1112\n",
      "step: 1112, train loss: 2.430, val loss 2.411\n",
      "1113\n",
      "step: 1113, train loss: 2.383, val loss 2.383\n",
      "1114\n",
      "step: 1114, train loss: 2.394, val loss 2.385\n",
      "1115\n",
      "step: 1115, train loss: 2.388, val loss 2.397\n",
      "1116\n",
      "step: 1116, train loss: 2.376, val loss 2.417\n",
      "1117\n",
      "step: 1117, train loss: 2.386, val loss 2.369\n",
      "1118\n",
      "step: 1118, train loss: 2.396, val loss 2.375\n",
      "1119\n",
      "step: 1119, train loss: 2.393, val loss 2.415\n",
      "1120\n",
      "step: 1120, train loss: 2.421, val loss 2.378\n",
      "1121\n",
      "step: 1121, train loss: 2.412, val loss 2.376\n",
      "1122\n",
      "step: 1122, train loss: 2.406, val loss 2.377\n",
      "1123\n",
      "step: 1123, train loss: 2.387, val loss 2.391\n",
      "1124\n",
      "step: 1124, train loss: 2.386, val loss 2.363\n",
      "1125\n",
      "step: 1125, train loss: 2.379, val loss 2.414\n",
      "1126\n",
      "step: 1126, train loss: 2.389, val loss 2.387\n",
      "1127\n",
      "step: 1127, train loss: 2.368, val loss 2.384\n",
      "1128\n",
      "step: 1128, train loss: 2.382, val loss 2.347\n",
      "1129\n",
      "step: 1129, train loss: 2.355, val loss 2.388\n",
      "1130\n",
      "step: 1130, train loss: 2.384, val loss 2.363\n",
      "1131\n",
      "step: 1131, train loss: 2.358, val loss 2.376\n",
      "1132\n",
      "step: 1132, train loss: 2.440, val loss 2.377\n",
      "1133\n",
      "step: 1133, train loss: 2.371, val loss 2.390\n",
      "1134\n",
      "step: 1134, train loss: 2.364, val loss 2.356\n",
      "1135\n",
      "step: 1135, train loss: 2.363, val loss 2.393\n",
      "1136\n",
      "step: 1136, train loss: 2.395, val loss 2.382\n",
      "1137\n",
      "step: 1137, train loss: 2.390, val loss 2.413\n",
      "1138\n",
      "step: 1138, train loss: 2.397, val loss 2.379\n",
      "1139\n",
      "step: 1139, train loss: 2.370, val loss 2.371\n",
      "1140\n",
      "step: 1140, train loss: 2.390, val loss 2.389\n",
      "1141\n",
      "step: 1141, train loss: 2.377, val loss 2.390\n",
      "1142\n",
      "step: 1142, train loss: 2.401, val loss 2.393\n",
      "1143\n",
      "step: 1143, train loss: 2.366, val loss 2.392\n",
      "1144\n",
      "step: 1144, train loss: 2.390, val loss 2.372\n",
      "1145\n",
      "step: 1145, train loss: 2.387, val loss 2.385\n",
      "1146\n",
      "step: 1146, train loss: 2.384, val loss 2.380\n",
      "1147\n",
      "step: 1147, train loss: 2.398, val loss 2.381\n",
      "1148\n",
      "step: 1148, train loss: 2.373, val loss 2.397\n",
      "1149\n",
      "step: 1149, train loss: 2.385, val loss 2.370\n",
      "1150\n",
      "step: 1150, train loss: 2.383, val loss 2.371\n",
      "1151\n",
      "step: 1151, train loss: 2.397, val loss 2.369\n",
      "1152\n",
      "step: 1152, train loss: 2.389, val loss 2.376\n",
      "1153\n",
      "step: 1153, train loss: 2.376, val loss 2.365\n",
      "1154\n",
      "step: 1154, train loss: 2.385, val loss 2.394\n",
      "1155\n",
      "step: 1155, train loss: 2.407, val loss 2.359\n",
      "1156\n",
      "step: 1156, train loss: 2.373, val loss 2.406\n",
      "1157\n",
      "step: 1157, train loss: 2.367, val loss 2.369\n",
      "1158\n",
      "step: 1158, train loss: 2.394, val loss 2.363\n",
      "1159\n",
      "step: 1159, train loss: 2.383, val loss 2.387\n",
      "1160\n",
      "step: 1160, train loss: 2.376, val loss 2.379\n",
      "1161\n",
      "step: 1161, train loss: 2.397, val loss 2.373\n",
      "1162\n",
      "step: 1162, train loss: 2.371, val loss 2.375\n",
      "1163\n",
      "step: 1163, train loss: 2.363, val loss 2.362\n",
      "1164\n",
      "step: 1164, train loss: 2.420, val loss 2.375\n",
      "1165\n",
      "step: 1165, train loss: 2.384, val loss 2.409\n",
      "1166\n",
      "step: 1166, train loss: 2.380, val loss 2.394\n",
      "1167\n",
      "step: 1167, train loss: 2.387, val loss 2.375\n",
      "1168\n",
      "step: 1168, train loss: 2.392, val loss 2.370\n",
      "1169\n",
      "step: 1169, train loss: 2.370, val loss 2.366\n",
      "1170\n",
      "step: 1170, train loss: 2.378, val loss 2.371\n",
      "1171\n",
      "step: 1171, train loss: 2.384, val loss 2.390\n",
      "1172\n",
      "step: 1172, train loss: 2.394, val loss 2.379\n",
      "1173\n",
      "step: 1173, train loss: 2.357, val loss 2.389\n",
      "1174\n",
      "step: 1174, train loss: 2.371, val loss 2.361\n",
      "1175\n",
      "step: 1175, train loss: 2.386, val loss 2.399\n",
      "1176\n",
      "step: 1176, train loss: 2.398, val loss 2.360\n",
      "1177\n",
      "step: 1177, train loss: 2.381, val loss 2.379\n",
      "1178\n",
      "step: 1178, train loss: 2.387, val loss 2.357\n",
      "1179\n",
      "step: 1179, train loss: 2.376, val loss 2.396\n",
      "1180\n",
      "step: 1180, train loss: 2.390, val loss 2.390\n",
      "1181\n",
      "step: 1181, train loss: 2.397, val loss 2.371\n",
      "1182\n",
      "step: 1182, train loss: 2.382, val loss 2.372\n",
      "1183\n",
      "step: 1183, train loss: 2.374, val loss 2.394\n",
      "1184\n",
      "step: 1184, train loss: 2.384, val loss 2.372\n",
      "1185\n",
      "step: 1185, train loss: 2.388, val loss 2.376\n",
      "1186\n",
      "step: 1186, train loss: 2.363, val loss 2.367\n",
      "1187\n",
      "step: 1187, train loss: 2.402, val loss 2.376\n",
      "1188\n",
      "step: 1188, train loss: 2.412, val loss 2.366\n",
      "1189\n",
      "step: 1189, train loss: 2.355, val loss 2.375\n",
      "1190\n",
      "step: 1190, train loss: 2.374, val loss 2.378\n",
      "1191\n",
      "step: 1191, train loss: 2.386, val loss 2.403\n",
      "1192\n",
      "step: 1192, train loss: 2.374, val loss 2.383\n",
      "1193\n",
      "step: 1193, train loss: 2.381, val loss 2.347\n",
      "1194\n",
      "step: 1194, train loss: 2.382, val loss 2.368\n",
      "1195\n",
      "step: 1195, train loss: 2.368, val loss 2.383\n",
      "1196\n",
      "step: 1196, train loss: 2.386, val loss 2.376\n",
      "1197\n",
      "step: 1197, train loss: 2.370, val loss 2.398\n",
      "1198\n",
      "step: 1198, train loss: 2.360, val loss 2.374\n",
      "1199\n",
      "step: 1199, train loss: 2.383, val loss 2.395\n",
      "1200\n",
      "step: 1200, train loss: 2.377, val loss 2.367\n",
      "1201\n",
      "step: 1201, train loss: 2.385, val loss 2.359\n",
      "1202\n",
      "step: 1202, train loss: 2.370, val loss 2.371\n",
      "1203\n",
      "step: 1203, train loss: 2.380, val loss 2.374\n",
      "1204\n",
      "step: 1204, train loss: 2.382, val loss 2.363\n",
      "1205\n",
      "step: 1205, train loss: 2.380, val loss 2.375\n",
      "1206\n",
      "step: 1206, train loss: 2.380, val loss 2.394\n",
      "1207\n",
      "step: 1207, train loss: 2.374, val loss 2.381\n",
      "1208\n",
      "step: 1208, train loss: 2.359, val loss 2.362\n",
      "1209\n",
      "step: 1209, train loss: 2.366, val loss 2.402\n",
      "1210\n",
      "step: 1210, train loss: 2.391, val loss 2.393\n",
      "1211\n",
      "step: 1211, train loss: 2.398, val loss 2.373\n",
      "1212\n",
      "step: 1212, train loss: 2.383, val loss 2.376\n",
      "1213\n",
      "step: 1213, train loss: 2.383, val loss 2.393\n",
      "1214\n",
      "step: 1214, train loss: 2.390, val loss 2.385\n",
      "1215\n",
      "step: 1215, train loss: 2.392, val loss 2.394\n",
      "1216\n",
      "step: 1216, train loss: 2.382, val loss 2.399\n",
      "1217\n",
      "step: 1217, train loss: 2.408, val loss 2.391\n",
      "1218\n",
      "step: 1218, train loss: 2.368, val loss 2.394\n",
      "1219\n",
      "step: 1219, train loss: 2.375, val loss 2.366\n",
      "1220\n",
      "step: 1220, train loss: 2.385, val loss 2.385\n",
      "1221\n",
      "step: 1221, train loss: 2.373, val loss 2.367\n",
      "1222\n",
      "step: 1222, train loss: 2.385, val loss 2.396\n",
      "1223\n",
      "step: 1223, train loss: 2.391, val loss 2.372\n",
      "1224\n",
      "step: 1224, train loss: 2.382, val loss 2.400\n",
      "1225\n",
      "step: 1225, train loss: 2.384, val loss 2.361\n",
      "1226\n",
      "step: 1226, train loss: 2.382, val loss 2.361\n",
      "1227\n",
      "step: 1227, train loss: 2.359, val loss 2.358\n",
      "1228\n",
      "step: 1228, train loss: 2.389, val loss 2.384\n",
      "1229\n",
      "step: 1229, train loss: 2.387, val loss 2.397\n",
      "1230\n",
      "step: 1230, train loss: 2.375, val loss 2.402\n",
      "1231\n",
      "step: 1231, train loss: 2.363, val loss 2.390\n",
      "1232\n",
      "step: 1232, train loss: 2.387, val loss 2.388\n",
      "1233\n",
      "step: 1233, train loss: 2.377, val loss 2.396\n",
      "1234\n",
      "step: 1234, train loss: 2.374, val loss 2.369\n",
      "1235\n",
      "step: 1235, train loss: 2.387, val loss 2.373\n",
      "1236\n",
      "step: 1236, train loss: 2.381, val loss 2.362\n",
      "1237\n",
      "step: 1237, train loss: 2.386, val loss 2.391\n",
      "1238\n",
      "step: 1238, train loss: 2.401, val loss 2.373\n",
      "1239\n",
      "step: 1239, train loss: 2.394, val loss 2.393\n",
      "1240\n",
      "step: 1240, train loss: 2.390, val loss 2.382\n",
      "1241\n",
      "step: 1241, train loss: 2.384, val loss 2.384\n",
      "1242\n",
      "step: 1242, train loss: 2.392, val loss 2.352\n",
      "1243\n",
      "step: 1243, train loss: 2.404, val loss 2.364\n",
      "1244\n",
      "step: 1244, train loss: 2.397, val loss 2.364\n",
      "1245\n",
      "step: 1245, train loss: 2.349, val loss 2.382\n",
      "1246\n",
      "step: 1246, train loss: 2.386, val loss 2.370\n",
      "1247\n",
      "step: 1247, train loss: 2.399, val loss 2.380\n",
      "1248\n",
      "step: 1248, train loss: 2.360, val loss 2.357\n",
      "1249\n",
      "step: 1249, train loss: 2.386, val loss 2.391\n",
      "1250\n",
      "step: 1250, train loss: 2.386, val loss 2.384\n",
      "1251\n",
      "step: 1251, train loss: 2.404, val loss 2.365\n",
      "1252\n",
      "step: 1252, train loss: 2.383, val loss 2.389\n",
      "1253\n",
      "step: 1253, train loss: 2.364, val loss 2.369\n",
      "1254\n",
      "step: 1254, train loss: 2.358, val loss 2.399\n",
      "1255\n",
      "step: 1255, train loss: 2.380, val loss 2.382\n",
      "1256\n",
      "step: 1256, train loss: 2.351, val loss 2.366\n",
      "1257\n",
      "step: 1257, train loss: 2.369, val loss 2.371\n",
      "1258\n",
      "step: 1258, train loss: 2.371, val loss 2.378\n",
      "1259\n",
      "step: 1259, train loss: 2.387, val loss 2.393\n",
      "1260\n",
      "step: 1260, train loss: 2.356, val loss 2.385\n",
      "1261\n",
      "step: 1261, train loss: 2.362, val loss 2.371\n",
      "1262\n",
      "step: 1262, train loss: 2.411, val loss 2.389\n",
      "1263\n",
      "step: 1263, train loss: 2.358, val loss 2.366\n",
      "1264\n",
      "step: 1264, train loss: 2.384, val loss 2.382\n",
      "1265\n",
      "step: 1265, train loss: 2.364, val loss 2.382\n",
      "1266\n",
      "step: 1266, train loss: 2.382, val loss 2.399\n",
      "1267\n",
      "step: 1267, train loss: 2.370, val loss 2.403\n",
      "1268\n",
      "step: 1268, train loss: 2.398, val loss 2.385\n",
      "1269\n",
      "step: 1269, train loss: 2.368, val loss 2.386\n",
      "1270\n",
      "step: 1270, train loss: 2.419, val loss 2.393\n",
      "1271\n",
      "step: 1271, train loss: 2.375, val loss 2.379\n",
      "1272\n",
      "step: 1272, train loss: 2.358, val loss 2.394\n",
      "1273\n",
      "step: 1273, train loss: 2.341, val loss 2.363\n",
      "1274\n",
      "step: 1274, train loss: 2.379, val loss 2.362\n",
      "1275\n",
      "step: 1275, train loss: 2.374, val loss 2.367\n",
      "1276\n",
      "step: 1276, train loss: 2.392, val loss 2.382\n",
      "1277\n",
      "step: 1277, train loss: 2.391, val loss 2.402\n",
      "1278\n",
      "step: 1278, train loss: 2.379, val loss 2.378\n",
      "1279\n",
      "step: 1279, train loss: 2.359, val loss 2.361\n",
      "1280\n",
      "step: 1280, train loss: 2.384, val loss 2.405\n",
      "1281\n",
      "step: 1281, train loss: 2.380, val loss 2.399\n",
      "1282\n",
      "step: 1282, train loss: 2.381, val loss 2.369\n",
      "1283\n",
      "step: 1283, train loss: 2.405, val loss 2.364\n",
      "1284\n",
      "step: 1284, train loss: 2.364, val loss 2.374\n",
      "1285\n",
      "step: 1285, train loss: 2.358, val loss 2.381\n",
      "1286\n",
      "step: 1286, train loss: 2.368, val loss 2.395\n",
      "1287\n",
      "step: 1287, train loss: 2.384, val loss 2.392\n",
      "1288\n",
      "step: 1288, train loss: 2.355, val loss 2.380\n",
      "1289\n",
      "step: 1289, train loss: 2.377, val loss 2.369\n",
      "1290\n",
      "step: 1290, train loss: 2.387, val loss 2.367\n",
      "1291\n",
      "step: 1291, train loss: 2.374, val loss 2.376\n",
      "1292\n",
      "step: 1292, train loss: 2.386, val loss 2.376\n",
      "1293\n",
      "step: 1293, train loss: 2.385, val loss 2.376\n",
      "1294\n",
      "step: 1294, train loss: 2.378, val loss 2.358\n",
      "1295\n",
      "step: 1295, train loss: 2.391, val loss 2.365\n",
      "1296\n",
      "step: 1296, train loss: 2.413, val loss 2.374\n",
      "1297\n",
      "step: 1297, train loss: 2.374, val loss 2.362\n",
      "1298\n",
      "step: 1298, train loss: 2.365, val loss 2.362\n",
      "1299\n",
      "step: 1299, train loss: 2.379, val loss 2.366\n",
      "1300\n",
      "step: 1300, train loss: 2.381, val loss 2.376\n",
      "1301\n",
      "step: 1301, train loss: 2.378, val loss 2.356\n",
      "1302\n",
      "step: 1302, train loss: 2.359, val loss 2.368\n",
      "1303\n",
      "step: 1303, train loss: 2.367, val loss 2.362\n",
      "1304\n",
      "step: 1304, train loss: 2.374, val loss 2.399\n",
      "1305\n",
      "step: 1305, train loss: 2.384, val loss 2.386\n",
      "1306\n",
      "step: 1306, train loss: 2.362, val loss 2.390\n",
      "1307\n",
      "step: 1307, train loss: 2.354, val loss 2.408\n",
      "1308\n",
      "step: 1308, train loss: 2.369, val loss 2.380\n",
      "1309\n",
      "step: 1309, train loss: 2.392, val loss 2.378\n",
      "1310\n",
      "step: 1310, train loss: 2.399, val loss 2.377\n",
      "1311\n",
      "step: 1311, train loss: 2.389, val loss 2.381\n",
      "1312\n",
      "step: 1312, train loss: 2.370, val loss 2.391\n",
      "1313\n",
      "step: 1313, train loss: 2.408, val loss 2.390\n",
      "1314\n",
      "step: 1314, train loss: 2.393, val loss 2.397\n",
      "1315\n",
      "step: 1315, train loss: 2.365, val loss 2.389\n",
      "1316\n",
      "step: 1316, train loss: 2.394, val loss 2.407\n",
      "1317\n",
      "step: 1317, train loss: 2.378, val loss 2.397\n",
      "1318\n",
      "step: 1318, train loss: 2.370, val loss 2.371\n",
      "1319\n",
      "step: 1319, train loss: 2.383, val loss 2.358\n",
      "1320\n",
      "step: 1320, train loss: 2.363, val loss 2.343\n",
      "1321\n",
      "step: 1321, train loss: 2.371, val loss 2.387\n",
      "1322\n",
      "step: 1322, train loss: 2.398, val loss 2.387\n",
      "1323\n",
      "step: 1323, train loss: 2.361, val loss 2.405\n",
      "1324\n",
      "step: 1324, train loss: 2.385, val loss 2.353\n",
      "1325\n",
      "step: 1325, train loss: 2.397, val loss 2.351\n",
      "1326\n",
      "step: 1326, train loss: 2.402, val loss 2.378\n",
      "1327\n",
      "step: 1327, train loss: 2.394, val loss 2.372\n",
      "1328\n",
      "step: 1328, train loss: 2.396, val loss 2.395\n",
      "1329\n",
      "step: 1329, train loss: 2.378, val loss 2.375\n",
      "1330\n",
      "step: 1330, train loss: 2.381, val loss 2.401\n",
      "1331\n",
      "step: 1331, train loss: 2.373, val loss 2.373\n",
      "1332\n",
      "step: 1332, train loss: 2.389, val loss 2.380\n",
      "1333\n",
      "step: 1333, train loss: 2.380, val loss 2.395\n",
      "1334\n",
      "step: 1334, train loss: 2.412, val loss 2.393\n",
      "1335\n",
      "step: 1335, train loss: 2.385, val loss 2.409\n",
      "1336\n",
      "step: 1336, train loss: 2.365, val loss 2.368\n",
      "1337\n",
      "step: 1337, train loss: 2.364, val loss 2.372\n",
      "1338\n",
      "step: 1338, train loss: 2.387, val loss 2.372\n",
      "1339\n",
      "step: 1339, train loss: 2.353, val loss 2.387\n",
      "1340\n",
      "step: 1340, train loss: 2.371, val loss 2.390\n",
      "1341\n",
      "step: 1341, train loss: 2.351, val loss 2.381\n",
      "1342\n",
      "step: 1342, train loss: 2.364, val loss 2.403\n",
      "1343\n",
      "step: 1343, train loss: 2.370, val loss 2.354\n",
      "1344\n",
      "step: 1344, train loss: 2.388, val loss 2.367\n",
      "1345\n",
      "step: 1345, train loss: 2.364, val loss 2.368\n",
      "1346\n",
      "step: 1346, train loss: 2.346, val loss 2.374\n",
      "1347\n",
      "step: 1347, train loss: 2.389, val loss 2.354\n",
      "1348\n",
      "step: 1348, train loss: 2.367, val loss 2.381\n",
      "1349\n",
      "step: 1349, train loss: 2.380, val loss 2.364\n",
      "1350\n",
      "step: 1350, train loss: 2.357, val loss 2.406\n",
      "1351\n",
      "step: 1351, train loss: 2.365, val loss 2.368\n",
      "1352\n",
      "step: 1352, train loss: 2.377, val loss 2.360\n",
      "1353\n",
      "step: 1353, train loss: 2.395, val loss 2.361\n",
      "1354\n",
      "step: 1354, train loss: 2.382, val loss 2.349\n",
      "1355\n",
      "step: 1355, train loss: 2.384, val loss 2.381\n",
      "1356\n",
      "step: 1356, train loss: 2.358, val loss 2.404\n",
      "1357\n",
      "step: 1357, train loss: 2.365, val loss 2.358\n",
      "1358\n",
      "step: 1358, train loss: 2.373, val loss 2.369\n",
      "1359\n",
      "step: 1359, train loss: 2.379, val loss 2.373\n",
      "1360\n",
      "step: 1360, train loss: 2.408, val loss 2.370\n",
      "1361\n",
      "step: 1361, train loss: 2.385, val loss 2.374\n",
      "1362\n",
      "step: 1362, train loss: 2.370, val loss 2.382\n",
      "1363\n",
      "step: 1363, train loss: 2.375, val loss 2.392\n",
      "1364\n",
      "step: 1364, train loss: 2.361, val loss 2.392\n",
      "1365\n",
      "step: 1365, train loss: 2.364, val loss 2.365\n",
      "1366\n",
      "step: 1366, train loss: 2.376, val loss 2.374\n",
      "1367\n",
      "step: 1367, train loss: 2.355, val loss 2.372\n",
      "1368\n",
      "step: 1368, train loss: 2.399, val loss 2.357\n",
      "1369\n",
      "step: 1369, train loss: 2.369, val loss 2.381\n",
      "1370\n",
      "step: 1370, train loss: 2.343, val loss 2.377\n",
      "1371\n",
      "step: 1371, train loss: 2.385, val loss 2.381\n",
      "1372\n",
      "step: 1372, train loss: 2.362, val loss 2.363\n",
      "1373\n",
      "step: 1373, train loss: 2.372, val loss 2.346\n",
      "1374\n",
      "step: 1374, train loss: 2.357, val loss 2.383\n",
      "1375\n",
      "step: 1375, train loss: 2.398, val loss 2.371\n",
      "1376\n",
      "step: 1376, train loss: 2.359, val loss 2.373\n",
      "1377\n",
      "step: 1377, train loss: 2.368, val loss 2.375\n",
      "1378\n",
      "step: 1378, train loss: 2.375, val loss 2.373\n",
      "1379\n",
      "step: 1379, train loss: 2.354, val loss 2.365\n",
      "1380\n",
      "step: 1380, train loss: 2.373, val loss 2.349\n",
      "1381\n",
      "step: 1381, train loss: 2.367, val loss 2.367\n",
      "1382\n",
      "step: 1382, train loss: 2.360, val loss 2.378\n",
      "1383\n",
      "step: 1383, train loss: 2.394, val loss 2.366\n",
      "1384\n",
      "step: 1384, train loss: 2.378, val loss 2.365\n",
      "1385\n",
      "step: 1385, train loss: 2.355, val loss 2.355\n",
      "1386\n",
      "step: 1386, train loss: 2.374, val loss 2.370\n",
      "1387\n",
      "step: 1387, train loss: 2.376, val loss 2.368\n",
      "1388\n",
      "step: 1388, train loss: 2.358, val loss 2.351\n",
      "1389\n",
      "step: 1389, train loss: 2.348, val loss 2.375\n",
      "1390\n",
      "step: 1390, train loss: 2.392, val loss 2.387\n",
      "1391\n",
      "step: 1391, train loss: 2.375, val loss 2.386\n",
      "1392\n",
      "step: 1392, train loss: 2.385, val loss 2.357\n",
      "1393\n",
      "step: 1393, train loss: 2.361, val loss 2.347\n",
      "1394\n",
      "step: 1394, train loss: 2.366, val loss 2.385\n",
      "1395\n",
      "step: 1395, train loss: 2.356, val loss 2.379\n",
      "1396\n",
      "step: 1396, train loss: 2.387, val loss 2.372\n",
      "1397\n",
      "step: 1397, train loss: 2.369, val loss 2.375\n",
      "1398\n",
      "step: 1398, train loss: 2.387, val loss 2.396\n",
      "1399\n",
      "step: 1399, train loss: 2.392, val loss 2.374\n",
      "1400\n",
      "step: 1400, train loss: 2.383, val loss 2.374\n",
      "1401\n",
      "step: 1401, train loss: 2.382, val loss 2.358\n",
      "1402\n",
      "step: 1402, train loss: 2.380, val loss 2.375\n",
      "1403\n",
      "step: 1403, train loss: 2.362, val loss 2.384\n",
      "1404\n",
      "step: 1404, train loss: 2.379, val loss 2.363\n",
      "1405\n",
      "step: 1405, train loss: 2.364, val loss 2.375\n",
      "1406\n",
      "step: 1406, train loss: 2.402, val loss 2.359\n",
      "1407\n",
      "step: 1407, train loss: 2.364, val loss 2.360\n",
      "1408\n",
      "step: 1408, train loss: 2.377, val loss 2.394\n",
      "1409\n",
      "step: 1409, train loss: 2.378, val loss 2.389\n",
      "1410\n",
      "step: 1410, train loss: 2.367, val loss 2.366\n",
      "1411\n",
      "step: 1411, train loss: 2.374, val loss 2.364\n",
      "1412\n",
      "step: 1412, train loss: 2.358, val loss 2.362\n",
      "1413\n",
      "step: 1413, train loss: 2.406, val loss 2.375\n",
      "1414\n",
      "step: 1414, train loss: 2.414, val loss 2.392\n",
      "1415\n",
      "step: 1415, train loss: 2.372, val loss 2.395\n",
      "1416\n",
      "step: 1416, train loss: 2.362, val loss 2.359\n",
      "1417\n",
      "step: 1417, train loss: 2.386, val loss 2.416\n",
      "1418\n",
      "step: 1418, train loss: 2.379, val loss 2.382\n",
      "1419\n",
      "step: 1419, train loss: 2.379, val loss 2.371\n",
      "1420\n",
      "step: 1420, train loss: 2.368, val loss 2.364\n",
      "1421\n",
      "step: 1421, train loss: 2.386, val loss 2.360\n",
      "1422\n",
      "step: 1422, train loss: 2.390, val loss 2.385\n",
      "1423\n",
      "step: 1423, train loss: 2.371, val loss 2.372\n",
      "1424\n",
      "step: 1424, train loss: 2.374, val loss 2.381\n",
      "1425\n",
      "step: 1425, train loss: 2.363, val loss 2.397\n",
      "1426\n",
      "step: 1426, train loss: 2.358, val loss 2.379\n",
      "1427\n",
      "step: 1427, train loss: 2.371, val loss 2.366\n",
      "1428\n",
      "step: 1428, train loss: 2.356, val loss 2.370\n",
      "1429\n",
      "step: 1429, train loss: 2.363, val loss 2.387\n",
      "1430\n",
      "step: 1430, train loss: 2.355, val loss 2.365\n",
      "1431\n",
      "step: 1431, train loss: 2.370, val loss 2.363\n",
      "1432\n",
      "step: 1432, train loss: 2.391, val loss 2.372\n",
      "1433\n",
      "step: 1433, train loss: 2.365, val loss 2.364\n",
      "1434\n",
      "step: 1434, train loss: 2.371, val loss 2.375\n",
      "1435\n",
      "step: 1435, train loss: 2.375, val loss 2.405\n",
      "1436\n",
      "step: 1436, train loss: 2.355, val loss 2.346\n",
      "1437\n",
      "step: 1437, train loss: 2.355, val loss 2.373\n",
      "1438\n",
      "step: 1438, train loss: 2.362, val loss 2.344\n",
      "1439\n",
      "step: 1439, train loss: 2.392, val loss 2.386\n",
      "1440\n",
      "step: 1440, train loss: 2.358, val loss 2.346\n",
      "1441\n",
      "step: 1441, train loss: 2.358, val loss 2.390\n",
      "1442\n",
      "step: 1442, train loss: 2.358, val loss 2.334\n",
      "1443\n",
      "step: 1443, train loss: 2.375, val loss 2.359\n",
      "1444\n",
      "step: 1444, train loss: 2.370, val loss 2.336\n",
      "1445\n",
      "step: 1445, train loss: 2.348, val loss 2.352\n",
      "1446\n",
      "step: 1446, train loss: 2.361, val loss 2.348\n",
      "1447\n",
      "step: 1447, train loss: 2.353, val loss 2.368\n",
      "1448\n",
      "step: 1448, train loss: 2.367, val loss 2.364\n",
      "1449\n",
      "step: 1449, train loss: 2.357, val loss 2.366\n",
      "1450\n",
      "step: 1450, train loss: 2.349, val loss 2.354\n",
      "1451\n",
      "step: 1451, train loss: 2.344, val loss 2.345\n",
      "1452\n",
      "step: 1452, train loss: 2.366, val loss 2.341\n",
      "1453\n",
      "step: 1453, train loss: 2.380, val loss 2.359\n",
      "1454\n",
      "step: 1454, train loss: 2.338, val loss 2.384\n",
      "1455\n",
      "step: 1455, train loss: 2.383, val loss 2.365\n",
      "1456\n",
      "step: 1456, train loss: 2.364, val loss 2.341\n",
      "1457\n",
      "step: 1457, train loss: 2.361, val loss 2.348\n",
      "1458\n",
      "step: 1458, train loss: 2.382, val loss 2.337\n",
      "1459\n",
      "step: 1459, train loss: 2.391, val loss 2.378\n",
      "1460\n",
      "step: 1460, train loss: 2.359, val loss 2.338\n",
      "1461\n",
      "step: 1461, train loss: 2.393, val loss 2.383\n",
      "1462\n",
      "step: 1462, train loss: 2.358, val loss 2.364\n",
      "1463\n",
      "step: 1463, train loss: 2.353, val loss 2.341\n",
      "1464\n",
      "step: 1464, train loss: 2.374, val loss 2.366\n",
      "1465\n",
      "step: 1465, train loss: 2.359, val loss 2.379\n",
      "1466\n",
      "step: 1466, train loss: 2.349, val loss 2.365\n",
      "1467\n",
      "step: 1467, train loss: 2.347, val loss 2.353\n",
      "1468\n",
      "step: 1468, train loss: 2.369, val loss 2.353\n",
      "1469\n",
      "step: 1469, train loss: 2.374, val loss 2.345\n",
      "1470\n",
      "step: 1470, train loss: 2.359, val loss 2.359\n",
      "1471\n",
      "step: 1471, train loss: 2.390, val loss 2.366\n",
      "1472\n",
      "step: 1472, train loss: 2.358, val loss 2.393\n",
      "1473\n",
      "step: 1473, train loss: 2.356, val loss 2.331\n",
      "1474\n",
      "step: 1474, train loss: 2.349, val loss 2.354\n",
      "1475\n",
      "step: 1475, train loss: 2.372, val loss 2.359\n",
      "1476\n",
      "step: 1476, train loss: 2.364, val loss 2.362\n",
      "1477\n",
      "step: 1477, train loss: 2.374, val loss 2.362\n",
      "1478\n",
      "step: 1478, train loss: 2.366, val loss 2.368\n",
      "1479\n",
      "step: 1479, train loss: 2.352, val loss 2.371\n",
      "1480\n",
      "step: 1480, train loss: 2.363, val loss 2.359\n",
      "1481\n",
      "step: 1481, train loss: 2.382, val loss 2.406\n",
      "1482\n",
      "step: 1482, train loss: 2.352, val loss 2.355\n",
      "1483\n",
      "step: 1483, train loss: 2.356, val loss 2.367\n",
      "1484\n",
      "step: 1484, train loss: 2.395, val loss 2.385\n",
      "1485\n",
      "step: 1485, train loss: 2.386, val loss 2.379\n",
      "1486\n",
      "step: 1486, train loss: 2.374, val loss 2.366\n",
      "1487\n",
      "step: 1487, train loss: 2.369, val loss 2.374\n",
      "1488\n",
      "step: 1488, train loss: 2.362, val loss 2.364\n",
      "1489\n",
      "step: 1489, train loss: 2.367, val loss 2.359\n",
      "1490\n",
      "step: 1490, train loss: 2.355, val loss 2.356\n",
      "1491\n",
      "step: 1491, train loss: 2.378, val loss 2.367\n",
      "1492\n",
      "step: 1492, train loss: 2.362, val loss 2.378\n",
      "1493\n",
      "step: 1493, train loss: 2.394, val loss 2.378\n",
      "1494\n",
      "step: 1494, train loss: 2.388, val loss 2.345\n",
      "1495\n",
      "step: 1495, train loss: 2.393, val loss 2.347\n",
      "1496\n",
      "step: 1496, train loss: 2.378, val loss 2.361\n",
      "1497\n",
      "step: 1497, train loss: 2.376, val loss 2.379\n",
      "1498\n",
      "step: 1498, train loss: 2.372, val loss 2.368\n",
      "1499\n",
      "step: 1499, train loss: 2.341, val loss 2.381\n",
      "1500\n",
      "step: 1500, train loss: 2.342, val loss 2.365\n",
      "1501\n",
      "step: 1501, train loss: 2.359, val loss 2.356\n",
      "1502\n",
      "step: 1502, train loss: 2.377, val loss 2.376\n",
      "1503\n",
      "step: 1503, train loss: 2.379, val loss 2.355\n",
      "1504\n",
      "step: 1504, train loss: 2.368, val loss 2.350\n",
      "1505\n",
      "step: 1505, train loss: 2.363, val loss 2.396\n",
      "1506\n",
      "step: 1506, train loss: 2.375, val loss 2.383\n",
      "1507\n",
      "step: 1507, train loss: 2.364, val loss 2.349\n",
      "1508\n",
      "step: 1508, train loss: 2.367, val loss 2.377\n",
      "1509\n",
      "step: 1509, train loss: 2.389, val loss 2.360\n",
      "1510\n",
      "step: 1510, train loss: 2.363, val loss 2.385\n",
      "1511\n",
      "step: 1511, train loss: 2.348, val loss 2.364\n",
      "1512\n",
      "step: 1512, train loss: 2.389, val loss 2.334\n",
      "1513\n",
      "step: 1513, train loss: 2.357, val loss 2.398\n",
      "1514\n",
      "step: 1514, train loss: 2.355, val loss 2.370\n",
      "1515\n",
      "step: 1515, train loss: 2.374, val loss 2.379\n",
      "1516\n",
      "step: 1516, train loss: 2.379, val loss 2.369\n",
      "1517\n",
      "step: 1517, train loss: 2.359, val loss 2.360\n",
      "1518\n",
      "step: 1518, train loss: 2.365, val loss 2.376\n",
      "1519\n",
      "step: 1519, train loss: 2.371, val loss 2.360\n",
      "1520\n",
      "step: 1520, train loss: 2.362, val loss 2.363\n",
      "1521\n",
      "step: 1521, train loss: 2.375, val loss 2.387\n",
      "1522\n",
      "step: 1522, train loss: 2.358, val loss 2.384\n",
      "1523\n",
      "step: 1523, train loss: 2.350, val loss 2.346\n",
      "1524\n",
      "step: 1524, train loss: 2.356, val loss 2.370\n",
      "1525\n",
      "step: 1525, train loss: 2.385, val loss 2.371\n",
      "1526\n",
      "step: 1526, train loss: 2.340, val loss 2.386\n",
      "1527\n",
      "step: 1527, train loss: 2.361, val loss 2.393\n",
      "1528\n",
      "step: 1528, train loss: 2.378, val loss 2.373\n",
      "1529\n",
      "step: 1529, train loss: 2.384, val loss 2.359\n",
      "1530\n",
      "step: 1530, train loss: 2.365, val loss 2.367\n",
      "1531\n",
      "step: 1531, train loss: 2.354, val loss 2.358\n",
      "1532\n",
      "step: 1532, train loss: 2.385, val loss 2.358\n",
      "1533\n",
      "step: 1533, train loss: 2.404, val loss 2.353\n",
      "1534\n",
      "step: 1534, train loss: 2.368, val loss 2.355\n",
      "1535\n",
      "step: 1535, train loss: 2.392, val loss 2.364\n",
      "1536\n",
      "step: 1536, train loss: 2.362, val loss 2.396\n",
      "1537\n",
      "step: 1537, train loss: 2.363, val loss 2.373\n",
      "1538\n",
      "step: 1538, train loss: 2.354, val loss 2.366\n",
      "1539\n",
      "step: 1539, train loss: 2.358, val loss 2.371\n",
      "1540\n",
      "step: 1540, train loss: 2.358, val loss 2.378\n",
      "1541\n",
      "step: 1541, train loss: 2.382, val loss 2.384\n",
      "1542\n",
      "step: 1542, train loss: 2.381, val loss 2.401\n",
      "1543\n",
      "step: 1543, train loss: 2.375, val loss 2.363\n",
      "1544\n",
      "step: 1544, train loss: 2.352, val loss 2.359\n",
      "1545\n",
      "step: 1545, train loss: 2.361, val loss 2.361\n",
      "1546\n",
      "step: 1546, train loss: 2.352, val loss 2.352\n",
      "1547\n",
      "step: 1547, train loss: 2.352, val loss 2.333\n",
      "1548\n",
      "step: 1548, train loss: 2.367, val loss 2.346\n",
      "1549\n",
      "step: 1549, train loss: 2.370, val loss 2.351\n",
      "1550\n",
      "step: 1550, train loss: 2.376, val loss 2.374\n",
      "1551\n",
      "step: 1551, train loss: 2.370, val loss 2.351\n",
      "1552\n",
      "step: 1552, train loss: 2.395, val loss 2.355\n",
      "1553\n",
      "step: 1553, train loss: 2.378, val loss 2.358\n",
      "1554\n",
      "step: 1554, train loss: 2.352, val loss 2.348\n",
      "1555\n",
      "step: 1555, train loss: 2.345, val loss 2.367\n",
      "1556\n",
      "step: 1556, train loss: 2.367, val loss 2.367\n",
      "1557\n",
      "step: 1557, train loss: 2.362, val loss 2.340\n",
      "1558\n",
      "step: 1558, train loss: 2.362, val loss 2.356\n",
      "1559\n",
      "step: 1559, train loss: 2.378, val loss 2.368\n",
      "1560\n",
      "step: 1560, train loss: 2.379, val loss 2.369\n",
      "1561\n",
      "step: 1561, train loss: 2.369, val loss 2.355\n",
      "1562\n",
      "step: 1562, train loss: 2.365, val loss 2.367\n",
      "1563\n",
      "step: 1563, train loss: 2.352, val loss 2.356\n",
      "1564\n",
      "step: 1564, train loss: 2.368, val loss 2.351\n",
      "1565\n",
      "step: 1565, train loss: 2.344, val loss 2.383\n",
      "1566\n",
      "step: 1566, train loss: 2.354, val loss 2.367\n",
      "1567\n",
      "step: 1567, train loss: 2.346, val loss 2.363\n",
      "1568\n",
      "step: 1568, train loss: 2.391, val loss 2.332\n",
      "1569\n",
      "step: 1569, train loss: 2.357, val loss 2.365\n",
      "1570\n",
      "step: 1570, train loss: 2.364, val loss 2.362\n",
      "1571\n",
      "step: 1571, train loss: 2.356, val loss 2.393\n",
      "1572\n",
      "step: 1572, train loss: 2.356, val loss 2.343\n",
      "1573\n",
      "step: 1573, train loss: 2.350, val loss 2.346\n",
      "1574\n",
      "step: 1574, train loss: 2.370, val loss 2.355\n",
      "1575\n",
      "step: 1575, train loss: 2.388, val loss 2.353\n",
      "1576\n",
      "step: 1576, train loss: 2.342, val loss 2.336\n",
      "1577\n",
      "step: 1577, train loss: 2.365, val loss 2.367\n",
      "1578\n",
      "step: 1578, train loss: 2.366, val loss 2.351\n",
      "1579\n",
      "step: 1579, train loss: 2.350, val loss 2.346\n",
      "1580\n",
      "step: 1580, train loss: 2.352, val loss 2.404\n",
      "1581\n",
      "step: 1581, train loss: 2.341, val loss 2.365\n",
      "1582\n",
      "step: 1582, train loss: 2.367, val loss 2.364\n",
      "1583\n",
      "step: 1583, train loss: 2.384, val loss 2.355\n",
      "1584\n",
      "step: 1584, train loss: 2.360, val loss 2.355\n",
      "1585\n",
      "step: 1585, train loss: 2.356, val loss 2.362\n",
      "1586\n",
      "step: 1586, train loss: 2.367, val loss 2.376\n",
      "1587\n",
      "step: 1587, train loss: 2.384, val loss 2.384\n",
      "1588\n",
      "step: 1588, train loss: 2.394, val loss 2.360\n",
      "1589\n",
      "step: 1589, train loss: 2.361, val loss 2.369\n",
      "1590\n",
      "step: 1590, train loss: 2.367, val loss 2.381\n",
      "1591\n",
      "step: 1591, train loss: 2.374, val loss 2.359\n",
      "1592\n",
      "step: 1592, train loss: 2.369, val loss 2.362\n",
      "1593\n",
      "step: 1593, train loss: 2.366, val loss 2.360\n",
      "1594\n",
      "step: 1594, train loss: 2.362, val loss 2.414\n",
      "1595\n",
      "step: 1595, train loss: 2.380, val loss 2.369\n",
      "1596\n",
      "step: 1596, train loss: 2.346, val loss 2.362\n",
      "1597\n",
      "step: 1597, train loss: 2.372, val loss 2.365\n",
      "1598\n",
      "step: 1598, train loss: 2.379, val loss 2.379\n",
      "1599\n",
      "step: 1599, train loss: 2.338, val loss 2.345\n",
      "1600\n",
      "step: 1600, train loss: 2.355, val loss 2.350\n",
      "1601\n",
      "step: 1601, train loss: 2.367, val loss 2.377\n",
      "1602\n",
      "step: 1602, train loss: 2.346, val loss 2.353\n",
      "1603\n",
      "step: 1603, train loss: 2.363, val loss 2.379\n",
      "1604\n",
      "step: 1604, train loss: 2.360, val loss 2.358\n",
      "1605\n",
      "step: 1605, train loss: 2.370, val loss 2.364\n",
      "1606\n",
      "step: 1606, train loss: 2.357, val loss 2.372\n",
      "1607\n",
      "step: 1607, train loss: 2.365, val loss 2.349\n",
      "1608\n",
      "step: 1608, train loss: 2.367, val loss 2.364\n",
      "1609\n",
      "step: 1609, train loss: 2.362, val loss 2.370\n",
      "1610\n",
      "step: 1610, train loss: 2.370, val loss 2.347\n",
      "1611\n",
      "step: 1611, train loss: 2.361, val loss 2.388\n",
      "1612\n",
      "step: 1612, train loss: 2.350, val loss 2.361\n",
      "1613\n",
      "step: 1613, train loss: 2.340, val loss 2.332\n",
      "1614\n",
      "step: 1614, train loss: 2.366, val loss 2.358\n",
      "1615\n",
      "step: 1615, train loss: 2.360, val loss 2.348\n",
      "1616\n",
      "step: 1616, train loss: 2.357, val loss 2.349\n",
      "1617\n",
      "step: 1617, train loss: 2.364, val loss 2.343\n",
      "1618\n",
      "step: 1618, train loss: 2.349, val loss 2.377\n",
      "1619\n",
      "step: 1619, train loss: 2.347, val loss 2.336\n",
      "1620\n",
      "step: 1620, train loss: 2.350, val loss 2.354\n",
      "1621\n",
      "step: 1621, train loss: 2.377, val loss 2.364\n",
      "1622\n",
      "step: 1622, train loss: 2.357, val loss 2.379\n",
      "1623\n",
      "step: 1623, train loss: 2.359, val loss 2.357\n",
      "1624\n",
      "step: 1624, train loss: 2.350, val loss 2.372\n",
      "1625\n",
      "step: 1625, train loss: 2.336, val loss 2.366\n",
      "1626\n",
      "step: 1626, train loss: 2.343, val loss 2.369\n",
      "1627\n",
      "step: 1627, train loss: 2.387, val loss 2.346\n",
      "1628\n",
      "step: 1628, train loss: 2.365, val loss 2.351\n",
      "1629\n",
      "step: 1629, train loss: 2.346, val loss 2.380\n",
      "1630\n",
      "step: 1630, train loss: 2.354, val loss 2.358\n",
      "1631\n",
      "step: 1631, train loss: 2.356, val loss 2.381\n",
      "1632\n",
      "step: 1632, train loss: 2.350, val loss 2.380\n",
      "1633\n",
      "step: 1633, train loss: 2.334, val loss 2.361\n",
      "1634\n",
      "step: 1634, train loss: 2.377, val loss 2.331\n",
      "1635\n",
      "step: 1635, train loss: 2.339, val loss 2.332\n",
      "1636\n",
      "step: 1636, train loss: 2.342, val loss 2.360\n",
      "1637\n",
      "step: 1637, train loss: 2.352, val loss 2.362\n",
      "1638\n",
      "step: 1638, train loss: 2.354, val loss 2.364\n",
      "1639\n",
      "step: 1639, train loss: 2.354, val loss 2.369\n",
      "1640\n",
      "step: 1640, train loss: 2.369, val loss 2.399\n",
      "1641\n",
      "step: 1641, train loss: 2.354, val loss 2.360\n",
      "1642\n",
      "step: 1642, train loss: 2.343, val loss 2.366\n",
      "1643\n",
      "step: 1643, train loss: 2.377, val loss 2.355\n",
      "1644\n",
      "step: 1644, train loss: 2.372, val loss 2.361\n",
      "1645\n",
      "step: 1645, train loss: 2.355, val loss 2.362\n",
      "1646\n",
      "step: 1646, train loss: 2.369, val loss 2.357\n",
      "1647\n",
      "step: 1647, train loss: 2.368, val loss 2.345\n",
      "1648\n",
      "step: 1648, train loss: 2.344, val loss 2.356\n",
      "1649\n",
      "step: 1649, train loss: 2.357, val loss 2.369\n",
      "1650\n",
      "step: 1650, train loss: 2.349, val loss 2.397\n",
      "1651\n",
      "step: 1651, train loss: 2.370, val loss 2.346\n",
      "1652\n",
      "step: 1652, train loss: 2.340, val loss 2.363\n",
      "1653\n",
      "step: 1653, train loss: 2.334, val loss 2.347\n",
      "1654\n",
      "step: 1654, train loss: 2.351, val loss 2.378\n",
      "1655\n",
      "step: 1655, train loss: 2.337, val loss 2.356\n",
      "1656\n",
      "step: 1656, train loss: 2.355, val loss 2.374\n",
      "1657\n",
      "step: 1657, train loss: 2.349, val loss 2.335\n",
      "1658\n",
      "step: 1658, train loss: 2.351, val loss 2.386\n",
      "1659\n",
      "step: 1659, train loss: 2.346, val loss 2.381\n",
      "1660\n",
      "step: 1660, train loss: 2.378, val loss 2.380\n",
      "1661\n",
      "step: 1661, train loss: 2.358, val loss 2.381\n",
      "1662\n",
      "step: 1662, train loss: 2.397, val loss 2.372\n",
      "1663\n",
      "step: 1663, train loss: 2.390, val loss 2.368\n",
      "1664\n",
      "step: 1664, train loss: 2.357, val loss 2.369\n",
      "1665\n",
      "step: 1665, train loss: 2.368, val loss 2.352\n",
      "1666\n",
      "step: 1666, train loss: 2.347, val loss 2.376\n",
      "1667\n",
      "step: 1667, train loss: 2.381, val loss 2.346\n",
      "1668\n",
      "step: 1668, train loss: 2.389, val loss 2.375\n",
      "1669\n",
      "step: 1669, train loss: 2.346, val loss 2.355\n",
      "1670\n",
      "step: 1670, train loss: 2.355, val loss 2.359\n",
      "1671\n",
      "step: 1671, train loss: 2.367, val loss 2.352\n",
      "1672\n",
      "step: 1672, train loss: 2.338, val loss 2.369\n",
      "1673\n",
      "step: 1673, train loss: 2.369, val loss 2.378\n",
      "1674\n",
      "step: 1674, train loss: 2.377, val loss 2.359\n",
      "1675\n",
      "step: 1675, train loss: 2.348, val loss 2.363\n",
      "1676\n",
      "step: 1676, train loss: 2.347, val loss 2.361\n",
      "1677\n",
      "step: 1677, train loss: 2.359, val loss 2.364\n",
      "1678\n",
      "step: 1678, train loss: 2.363, val loss 2.366\n",
      "1679\n",
      "step: 1679, train loss: 2.378, val loss 2.359\n",
      "1680\n",
      "step: 1680, train loss: 2.341, val loss 2.341\n",
      "1681\n",
      "step: 1681, train loss: 2.343, val loss 2.349\n",
      "1682\n",
      "step: 1682, train loss: 2.385, val loss 2.331\n",
      "1683\n",
      "step: 1683, train loss: 2.373, val loss 2.371\n",
      "1684\n",
      "step: 1684, train loss: 2.369, val loss 2.404\n",
      "1685\n",
      "step: 1685, train loss: 2.347, val loss 2.333\n",
      "1686\n",
      "step: 1686, train loss: 2.345, val loss 2.369\n",
      "1687\n",
      "step: 1687, train loss: 2.353, val loss 2.344\n",
      "1688\n",
      "step: 1688, train loss: 2.352, val loss 2.360\n",
      "1689\n",
      "step: 1689, train loss: 2.367, val loss 2.352\n",
      "1690\n",
      "step: 1690, train loss: 2.364, val loss 2.361\n",
      "1691\n",
      "step: 1691, train loss: 2.342, val loss 2.352\n",
      "1692\n",
      "step: 1692, train loss: 2.351, val loss 2.374\n",
      "1693\n",
      "step: 1693, train loss: 2.350, val loss 2.353\n",
      "1694\n",
      "step: 1694, train loss: 2.361, val loss 2.342\n",
      "1695\n",
      "step: 1695, train loss: 2.362, val loss 2.385\n",
      "1696\n",
      "step: 1696, train loss: 2.352, val loss 2.349\n",
      "1697\n",
      "step: 1697, train loss: 2.364, val loss 2.384\n",
      "1698\n",
      "step: 1698, train loss: 2.366, val loss 2.357\n",
      "1699\n",
      "step: 1699, train loss: 2.363, val loss 2.345\n",
      "1700\n",
      "step: 1700, train loss: 2.373, val loss 2.373\n",
      "1701\n",
      "step: 1701, train loss: 2.345, val loss 2.352\n",
      "1702\n",
      "step: 1702, train loss: 2.329, val loss 2.360\n",
      "1703\n",
      "step: 1703, train loss: 2.379, val loss 2.350\n",
      "1704\n",
      "step: 1704, train loss: 2.355, val loss 2.347\n",
      "1705\n",
      "step: 1705, train loss: 2.345, val loss 2.347\n",
      "1706\n",
      "step: 1706, train loss: 2.332, val loss 2.362\n",
      "1707\n",
      "step: 1707, train loss: 2.368, val loss 2.357\n",
      "1708\n",
      "step: 1708, train loss: 2.347, val loss 2.386\n",
      "1709\n",
      "step: 1709, train loss: 2.382, val loss 2.360\n",
      "1710\n",
      "step: 1710, train loss: 2.369, val loss 2.356\n",
      "1711\n",
      "step: 1711, train loss: 2.354, val loss 2.368\n",
      "1712\n",
      "step: 1712, train loss: 2.338, val loss 2.337\n",
      "1713\n",
      "step: 1713, train loss: 2.356, val loss 2.344\n",
      "1714\n",
      "step: 1714, train loss: 2.345, val loss 2.333\n",
      "1715\n",
      "step: 1715, train loss: 2.399, val loss 2.338\n",
      "1716\n",
      "step: 1716, train loss: 2.358, val loss 2.334\n",
      "1717\n",
      "step: 1717, train loss: 2.341, val loss 2.369\n",
      "1718\n",
      "step: 1718, train loss: 2.342, val loss 2.342\n",
      "1719\n",
      "step: 1719, train loss: 2.352, val loss 2.344\n",
      "1720\n",
      "step: 1720, train loss: 2.356, val loss 2.334\n",
      "1721\n",
      "step: 1721, train loss: 2.349, val loss 2.367\n",
      "1722\n",
      "step: 1722, train loss: 2.352, val loss 2.366\n",
      "1723\n",
      "step: 1723, train loss: 2.348, val loss 2.388\n",
      "1724\n",
      "step: 1724, train loss: 2.369, val loss 2.340\n",
      "1725\n",
      "step: 1725, train loss: 2.374, val loss 2.357\n",
      "1726\n",
      "step: 1726, train loss: 2.339, val loss 2.349\n",
      "1727\n",
      "step: 1727, train loss: 2.343, val loss 2.364\n",
      "1728\n",
      "step: 1728, train loss: 2.359, val loss 2.331\n",
      "1729\n",
      "step: 1729, train loss: 2.330, val loss 2.336\n",
      "1730\n",
      "step: 1730, train loss: 2.343, val loss 2.356\n",
      "1731\n",
      "step: 1731, train loss: 2.358, val loss 2.336\n",
      "1732\n",
      "step: 1732, train loss: 2.353, val loss 2.352\n",
      "1733\n",
      "step: 1733, train loss: 2.346, val loss 2.332\n",
      "1734\n",
      "step: 1734, train loss: 2.363, val loss 2.343\n",
      "1735\n",
      "step: 1735, train loss: 2.350, val loss 2.360\n",
      "1736\n",
      "step: 1736, train loss: 2.343, val loss 2.362\n",
      "1737\n",
      "step: 1737, train loss: 2.350, val loss 2.343\n",
      "1738\n",
      "step: 1738, train loss: 2.373, val loss 2.364\n",
      "1739\n",
      "step: 1739, train loss: 2.369, val loss 2.329\n",
      "1740\n",
      "step: 1740, train loss: 2.344, val loss 2.365\n",
      "1741\n",
      "step: 1741, train loss: 2.352, val loss 2.374\n",
      "1742\n",
      "step: 1742, train loss: 2.354, val loss 2.348\n",
      "1743\n",
      "step: 1743, train loss: 2.357, val loss 2.329\n",
      "1744\n",
      "step: 1744, train loss: 2.349, val loss 2.347\n",
      "1745\n",
      "step: 1745, train loss: 2.332, val loss 2.337\n",
      "1746\n",
      "step: 1746, train loss: 2.358, val loss 2.347\n",
      "1747\n",
      "step: 1747, train loss: 2.335, val loss 2.329\n",
      "1748\n",
      "step: 1748, train loss: 2.353, val loss 2.368\n",
      "1749\n",
      "step: 1749, train loss: 2.352, val loss 2.354\n",
      "1750\n",
      "step: 1750, train loss: 2.345, val loss 2.360\n",
      "1751\n",
      "step: 1751, train loss: 2.345, val loss 2.374\n",
      "1752\n",
      "step: 1752, train loss: 2.350, val loss 2.386\n",
      "1753\n",
      "step: 1753, train loss: 2.357, val loss 2.351\n",
      "1754\n",
      "step: 1754, train loss: 2.368, val loss 2.371\n",
      "1755\n",
      "step: 1755, train loss: 2.337, val loss 2.334\n",
      "1756\n",
      "step: 1756, train loss: 2.376, val loss 2.343\n",
      "1757\n",
      "step: 1757, train loss: 2.355, val loss 2.354\n",
      "1758\n",
      "step: 1758, train loss: 2.355, val loss 2.365\n",
      "1759\n",
      "step: 1759, train loss: 2.354, val loss 2.342\n",
      "1760\n",
      "step: 1760, train loss: 2.373, val loss 2.373\n",
      "1761\n",
      "step: 1761, train loss: 2.369, val loss 2.376\n",
      "1762\n",
      "step: 1762, train loss: 2.350, val loss 2.361\n",
      "1763\n",
      "step: 1763, train loss: 2.355, val loss 2.379\n",
      "1764\n",
      "step: 1764, train loss: 2.362, val loss 2.342\n",
      "1765\n",
      "step: 1765, train loss: 2.346, val loss 2.355\n",
      "1766\n",
      "step: 1766, train loss: 2.353, val loss 2.346\n",
      "1767\n",
      "step: 1767, train loss: 2.367, val loss 2.348\n",
      "1768\n",
      "step: 1768, train loss: 2.347, val loss 2.349\n",
      "1769\n",
      "step: 1769, train loss: 2.335, val loss 2.348\n",
      "1770\n",
      "step: 1770, train loss: 2.341, val loss 2.363\n",
      "1771\n",
      "step: 1771, train loss: 2.365, val loss 2.356\n",
      "1772\n",
      "step: 1772, train loss: 2.364, val loss 2.341\n",
      "1773\n",
      "step: 1773, train loss: 2.373, val loss 2.350\n",
      "1774\n",
      "step: 1774, train loss: 2.354, val loss 2.354\n",
      "1775\n",
      "step: 1775, train loss: 2.349, val loss 2.353\n",
      "1776\n",
      "step: 1776, train loss: 2.336, val loss 2.371\n",
      "1777\n",
      "step: 1777, train loss: 2.337, val loss 2.383\n",
      "1778\n",
      "step: 1778, train loss: 2.347, val loss 2.340\n",
      "1779\n",
      "step: 1779, train loss: 2.359, val loss 2.333\n",
      "1780\n",
      "step: 1780, train loss: 2.350, val loss 2.350\n",
      "1781\n",
      "step: 1781, train loss: 2.356, val loss 2.360\n",
      "1782\n",
      "step: 1782, train loss: 2.335, val loss 2.347\n",
      "1783\n",
      "step: 1783, train loss: 2.357, val loss 2.320\n",
      "1784\n",
      "step: 1784, train loss: 2.366, val loss 2.341\n",
      "1785\n",
      "step: 1785, train loss: 2.376, val loss 2.361\n",
      "1786\n",
      "step: 1786, train loss: 2.354, val loss 2.346\n",
      "1787\n",
      "step: 1787, train loss: 2.358, val loss 2.364\n",
      "1788\n",
      "step: 1788, train loss: 2.374, val loss 2.358\n",
      "1789\n",
      "step: 1789, train loss: 2.367, val loss 2.360\n",
      "1790\n",
      "step: 1790, train loss: 2.378, val loss 2.353\n",
      "1791\n",
      "step: 1791, train loss: 2.335, val loss 2.358\n",
      "1792\n",
      "step: 1792, train loss: 2.381, val loss 2.342\n",
      "1793\n",
      "step: 1793, train loss: 2.362, val loss 2.331\n",
      "1794\n",
      "step: 1794, train loss: 2.368, val loss 2.334\n",
      "1795\n",
      "step: 1795, train loss: 2.334, val loss 2.383\n",
      "1796\n",
      "step: 1796, train loss: 2.337, val loss 2.316\n",
      "1797\n",
      "step: 1797, train loss: 2.335, val loss 2.356\n",
      "1798\n",
      "step: 1798, train loss: 2.349, val loss 2.343\n",
      "1799\n",
      "step: 1799, train loss: 2.353, val loss 2.338\n",
      "1800\n",
      "step: 1800, train loss: 2.337, val loss 2.351\n",
      "1801\n",
      "step: 1801, train loss: 2.343, val loss 2.395\n",
      "1802\n",
      "step: 1802, train loss: 2.339, val loss 2.349\n",
      "1803\n",
      "step: 1803, train loss: 2.355, val loss 2.352\n",
      "1804\n",
      "step: 1804, train loss: 2.370, val loss 2.330\n",
      "1805\n",
      "step: 1805, train loss: 2.361, val loss 2.349\n",
      "1806\n",
      "step: 1806, train loss: 2.355, val loss 2.368\n",
      "1807\n",
      "step: 1807, train loss: 2.344, val loss 2.330\n",
      "1808\n",
      "step: 1808, train loss: 2.340, val loss 2.359\n",
      "1809\n",
      "step: 1809, train loss: 2.345, val loss 2.315\n",
      "1810\n",
      "step: 1810, train loss: 2.349, val loss 2.358\n",
      "1811\n",
      "step: 1811, train loss: 2.330, val loss 2.353\n",
      "1812\n",
      "step: 1812, train loss: 2.340, val loss 2.360\n",
      "1813\n",
      "step: 1813, train loss: 2.366, val loss 2.345\n",
      "1814\n",
      "step: 1814, train loss: 2.363, val loss 2.347\n",
      "1815\n",
      "step: 1815, train loss: 2.355, val loss 2.338\n",
      "1816\n",
      "step: 1816, train loss: 2.339, val loss 2.325\n",
      "1817\n",
      "step: 1817, train loss: 2.349, val loss 2.352\n",
      "1818\n",
      "step: 1818, train loss: 2.385, val loss 2.366\n",
      "1819\n",
      "step: 1819, train loss: 2.337, val loss 2.351\n",
      "1820\n",
      "step: 1820, train loss: 2.356, val loss 2.347\n",
      "1821\n",
      "step: 1821, train loss: 2.375, val loss 2.354\n",
      "1822\n",
      "step: 1822, train loss: 2.355, val loss 2.341\n",
      "1823\n",
      "step: 1823, train loss: 2.359, val loss 2.352\n",
      "1824\n",
      "step: 1824, train loss: 2.352, val loss 2.348\n",
      "1825\n",
      "step: 1825, train loss: 2.349, val loss 2.356\n",
      "1826\n",
      "step: 1826, train loss: 2.373, val loss 2.326\n",
      "1827\n",
      "step: 1827, train loss: 2.355, val loss 2.366\n",
      "1828\n",
      "step: 1828, train loss: 2.355, val loss 2.351\n",
      "1829\n",
      "step: 1829, train loss: 2.340, val loss 2.357\n",
      "1830\n",
      "step: 1830, train loss: 2.385, val loss 2.344\n",
      "1831\n",
      "step: 1831, train loss: 2.327, val loss 2.351\n",
      "1832\n",
      "step: 1832, train loss: 2.344, val loss 2.351\n",
      "1833\n",
      "step: 1833, train loss: 2.348, val loss 2.357\n",
      "1834\n",
      "step: 1834, train loss: 2.357, val loss 2.372\n",
      "1835\n",
      "step: 1835, train loss: 2.358, val loss 2.329\n",
      "1836\n",
      "step: 1836, train loss: 2.348, val loss 2.351\n",
      "1837\n",
      "step: 1837, train loss: 2.355, val loss 2.341\n",
      "1838\n",
      "step: 1838, train loss: 2.318, val loss 2.393\n",
      "1839\n",
      "step: 1839, train loss: 2.365, val loss 2.336\n",
      "1840\n",
      "step: 1840, train loss: 2.339, val loss 2.341\n",
      "1841\n",
      "step: 1841, train loss: 2.359, val loss 2.344\n",
      "1842\n",
      "step: 1842, train loss: 2.341, val loss 2.372\n",
      "1843\n",
      "step: 1843, train loss: 2.369, val loss 2.359\n",
      "1844\n",
      "step: 1844, train loss: 2.371, val loss 2.352\n",
      "1845\n",
      "step: 1845, train loss: 2.341, val loss 2.344\n",
      "1846\n",
      "step: 1846, train loss: 2.362, val loss 2.339\n",
      "1847\n",
      "step: 1847, train loss: 2.377, val loss 2.325\n",
      "1848\n",
      "step: 1848, train loss: 2.328, val loss 2.337\n",
      "1849\n",
      "step: 1849, train loss: 2.340, val loss 2.339\n",
      "1850\n",
      "step: 1850, train loss: 2.349, val loss 2.349\n",
      "1851\n",
      "step: 1851, train loss: 2.354, val loss 2.360\n",
      "1852\n",
      "step: 1852, train loss: 2.357, val loss 2.369\n",
      "1853\n",
      "step: 1853, train loss: 2.326, val loss 2.360\n",
      "1854\n",
      "step: 1854, train loss: 2.356, val loss 2.371\n",
      "1855\n",
      "step: 1855, train loss: 2.369, val loss 2.355\n",
      "1856\n",
      "step: 1856, train loss: 2.330, val loss 2.346\n",
      "1857\n",
      "step: 1857, train loss: 2.363, val loss 2.367\n",
      "1858\n",
      "step: 1858, train loss: 2.364, val loss 2.341\n",
      "1859\n",
      "step: 1859, train loss: 2.379, val loss 2.351\n",
      "1860\n",
      "step: 1860, train loss: 2.339, val loss 2.336\n",
      "1861\n",
      "step: 1861, train loss: 2.347, val loss 2.334\n",
      "1862\n",
      "step: 1862, train loss: 2.343, val loss 2.349\n",
      "1863\n",
      "step: 1863, train loss: 2.361, val loss 2.388\n",
      "1864\n",
      "step: 1864, train loss: 2.349, val loss 2.340\n",
      "1865\n",
      "step: 1865, train loss: 2.362, val loss 2.339\n",
      "1866\n",
      "step: 1866, train loss: 2.359, val loss 2.330\n",
      "1867\n",
      "step: 1867, train loss: 2.345, val loss 2.332\n",
      "1868\n",
      "step: 1868, train loss: 2.349, val loss 2.356\n",
      "1869\n",
      "step: 1869, train loss: 2.353, val loss 2.346\n",
      "1870\n",
      "step: 1870, train loss: 2.363, val loss 2.346\n",
      "1871\n",
      "step: 1871, train loss: 2.337, val loss 2.323\n",
      "1872\n",
      "step: 1872, train loss: 2.360, val loss 2.358\n",
      "1873\n",
      "step: 1873, train loss: 2.365, val loss 2.320\n",
      "1874\n",
      "step: 1874, train loss: 2.343, val loss 2.342\n",
      "1875\n",
      "step: 1875, train loss: 2.361, val loss 2.349\n",
      "1876\n",
      "step: 1876, train loss: 2.340, val loss 2.334\n",
      "1877\n",
      "step: 1877, train loss: 2.351, val loss 2.376\n",
      "1878\n",
      "step: 1878, train loss: 2.341, val loss 2.354\n",
      "1879\n",
      "step: 1879, train loss: 2.344, val loss 2.341\n",
      "1880\n",
      "step: 1880, train loss: 2.339, val loss 2.357\n",
      "1881\n",
      "step: 1881, train loss: 2.339, val loss 2.338\n",
      "1882\n",
      "step: 1882, train loss: 2.346, val loss 2.349\n",
      "1883\n",
      "step: 1883, train loss: 2.359, val loss 2.339\n",
      "1884\n",
      "step: 1884, train loss: 2.351, val loss 2.348\n",
      "1885\n",
      "step: 1885, train loss: 2.326, val loss 2.337\n",
      "1886\n",
      "step: 1886, train loss: 2.364, val loss 2.356\n",
      "1887\n",
      "step: 1887, train loss: 2.364, val loss 2.331\n",
      "1888\n",
      "step: 1888, train loss: 2.338, val loss 2.339\n",
      "1889\n",
      "step: 1889, train loss: 2.351, val loss 2.349\n",
      "1890\n",
      "step: 1890, train loss: 2.331, val loss 2.335\n",
      "1891\n",
      "step: 1891, train loss: 2.339, val loss 2.348\n",
      "1892\n",
      "step: 1892, train loss: 2.337, val loss 2.368\n",
      "1893\n",
      "step: 1893, train loss: 2.354, val loss 2.315\n",
      "1894\n",
      "step: 1894, train loss: 2.372, val loss 2.349\n",
      "1895\n",
      "step: 1895, train loss: 2.354, val loss 2.347\n",
      "1896\n",
      "step: 1896, train loss: 2.349, val loss 2.387\n",
      "1897\n",
      "step: 1897, train loss: 2.359, val loss 2.357\n",
      "1898\n",
      "step: 1898, train loss: 2.356, val loss 2.349\n",
      "1899\n",
      "step: 1899, train loss: 2.360, val loss 2.355\n",
      "1900\n",
      "step: 1900, train loss: 2.357, val loss 2.404\n",
      "1901\n",
      "step: 1901, train loss: 2.346, val loss 2.345\n",
      "1902\n",
      "step: 1902, train loss: 2.370, val loss 2.334\n",
      "1903\n",
      "step: 1903, train loss: 2.334, val loss 2.374\n",
      "1904\n",
      "step: 1904, train loss: 2.351, val loss 2.375\n",
      "1905\n",
      "step: 1905, train loss: 2.352, val loss 2.352\n",
      "1906\n",
      "step: 1906, train loss: 2.341, val loss 2.369\n",
      "1907\n",
      "step: 1907, train loss: 2.392, val loss 2.345\n",
      "1908\n",
      "step: 1908, train loss: 2.366, val loss 2.348\n",
      "1909\n",
      "step: 1909, train loss: 2.356, val loss 2.343\n",
      "1910\n",
      "step: 1910, train loss: 2.345, val loss 2.336\n",
      "1911\n",
      "step: 1911, train loss: 2.367, val loss 2.375\n",
      "1912\n",
      "step: 1912, train loss: 2.354, val loss 2.371\n",
      "1913\n",
      "step: 1913, train loss: 2.352, val loss 2.346\n",
      "1914\n",
      "step: 1914, train loss: 2.344, val loss 2.381\n",
      "1915\n",
      "step: 1915, train loss: 2.373, val loss 2.360\n",
      "1916\n",
      "step: 1916, train loss: 2.334, val loss 2.385\n",
      "1917\n",
      "step: 1917, train loss: 2.373, val loss 2.355\n",
      "1918\n",
      "step: 1918, train loss: 2.352, val loss 2.344\n",
      "1919\n",
      "step: 1919, train loss: 2.351, val loss 2.380\n",
      "1920\n",
      "step: 1920, train loss: 2.351, val loss 2.336\n",
      "1921\n",
      "step: 1921, train loss: 2.367, val loss 2.350\n",
      "1922\n",
      "step: 1922, train loss: 2.362, val loss 2.332\n",
      "1923\n",
      "step: 1923, train loss: 2.351, val loss 2.330\n",
      "1924\n",
      "step: 1924, train loss: 2.361, val loss 2.340\n",
      "1925\n",
      "step: 1925, train loss: 2.394, val loss 2.350\n",
      "1926\n",
      "step: 1926, train loss: 2.363, val loss 2.360\n",
      "1927\n",
      "step: 1927, train loss: 2.332, val loss 2.369\n",
      "1928\n",
      "step: 1928, train loss: 2.361, val loss 2.363\n",
      "1929\n",
      "step: 1929, train loss: 2.367, val loss 2.342\n",
      "1930\n",
      "step: 1930, train loss: 2.351, val loss 2.347\n",
      "1931\n",
      "step: 1931, train loss: 2.355, val loss 2.329\n",
      "1932\n",
      "step: 1932, train loss: 2.366, val loss 2.340\n",
      "1933\n",
      "step: 1933, train loss: 2.358, val loss 2.374\n",
      "1934\n",
      "step: 1934, train loss: 2.356, val loss 2.364\n",
      "1935\n",
      "step: 1935, train loss: 2.347, val loss 2.343\n",
      "1936\n",
      "step: 1936, train loss: 2.354, val loss 2.338\n",
      "1937\n",
      "step: 1937, train loss: 2.330, val loss 2.362\n",
      "1938\n",
      "step: 1938, train loss: 2.338, val loss 2.346\n",
      "1939\n",
      "step: 1939, train loss: 2.335, val loss 2.334\n",
      "1940\n",
      "step: 1940, train loss: 2.376, val loss 2.329\n",
      "1941\n",
      "step: 1941, train loss: 2.343, val loss 2.391\n",
      "1942\n",
      "step: 1942, train loss: 2.362, val loss 2.334\n",
      "1943\n",
      "step: 1943, train loss: 2.372, val loss 2.342\n",
      "1944\n",
      "step: 1944, train loss: 2.359, val loss 2.355\n",
      "1945\n",
      "step: 1945, train loss: 2.366, val loss 2.358\n",
      "1946\n",
      "step: 1946, train loss: 2.346, val loss 2.373\n",
      "1947\n",
      "step: 1947, train loss: 2.376, val loss 2.349\n",
      "1948\n",
      "step: 1948, train loss: 2.368, val loss 2.388\n",
      "1949\n",
      "step: 1949, train loss: 2.361, val loss 2.344\n",
      "1950\n",
      "step: 1950, train loss: 2.356, val loss 2.369\n",
      "1951\n",
      "step: 1951, train loss: 2.390, val loss 2.349\n",
      "1952\n",
      "step: 1952, train loss: 2.358, val loss 2.371\n",
      "1953\n",
      "step: 1953, train loss: 2.370, val loss 2.351\n",
      "1954\n",
      "step: 1954, train loss: 2.379, val loss 2.367\n",
      "1955\n",
      "step: 1955, train loss: 2.362, val loss 2.340\n",
      "1956\n",
      "step: 1956, train loss: 2.355, val loss 2.357\n",
      "1957\n",
      "step: 1957, train loss: 2.366, val loss 2.356\n",
      "1958\n",
      "step: 1958, train loss: 2.349, val loss 2.358\n",
      "1959\n",
      "step: 1959, train loss: 2.355, val loss 2.356\n",
      "1960\n",
      "step: 1960, train loss: 2.343, val loss 2.339\n",
      "1961\n",
      "step: 1961, train loss: 2.366, val loss 2.369\n",
      "1962\n",
      "step: 1962, train loss: 2.373, val loss 2.365\n",
      "1963\n",
      "step: 1963, train loss: 2.377, val loss 2.354\n",
      "1964\n",
      "step: 1964, train loss: 2.338, val loss 2.374\n",
      "1965\n",
      "step: 1965, train loss: 2.359, val loss 2.394\n",
      "1966\n",
      "step: 1966, train loss: 2.357, val loss 2.356\n",
      "1967\n",
      "step: 1967, train loss: 2.381, val loss 2.363\n",
      "1968\n",
      "step: 1968, train loss: 2.351, val loss 2.374\n",
      "1969\n",
      "step: 1969, train loss: 2.344, val loss 2.367\n",
      "1970\n",
      "step: 1970, train loss: 2.341, val loss 2.342\n",
      "1971\n",
      "step: 1971, train loss: 2.361, val loss 2.374\n",
      "1972\n",
      "step: 1972, train loss: 2.367, val loss 2.377\n",
      "1973\n",
      "step: 1973, train loss: 2.352, val loss 2.340\n",
      "1974\n",
      "step: 1974, train loss: 2.368, val loss 2.357\n",
      "1975\n",
      "step: 1975, train loss: 2.367, val loss 2.369\n",
      "1976\n",
      "step: 1976, train loss: 2.366, val loss 2.367\n",
      "1977\n",
      "step: 1977, train loss: 2.367, val loss 2.369\n",
      "1978\n",
      "step: 1978, train loss: 2.347, val loss 2.342\n",
      "1979\n",
      "step: 1979, train loss: 2.376, val loss 2.338\n",
      "1980\n",
      "step: 1980, train loss: 2.328, val loss 2.366\n",
      "1981\n",
      "step: 1981, train loss: 2.356, val loss 2.368\n",
      "1982\n",
      "step: 1982, train loss: 2.348, val loss 2.363\n",
      "1983\n",
      "step: 1983, train loss: 2.344, val loss 2.351\n",
      "1984\n",
      "step: 1984, train loss: 2.356, val loss 2.337\n",
      "1985\n",
      "step: 1985, train loss: 2.366, val loss 2.342\n",
      "1986\n",
      "step: 1986, train loss: 2.334, val loss 2.356\n",
      "1987\n",
      "step: 1987, train loss: 2.336, val loss 2.390\n",
      "1988\n",
      "step: 1988, train loss: 2.341, val loss 2.384\n",
      "1989\n",
      "step: 1989, train loss: 2.334, val loss 2.360\n",
      "1990\n",
      "step: 1990, train loss: 2.362, val loss 2.351\n",
      "1991\n",
      "step: 1991, train loss: 2.349, val loss 2.368\n",
      "1992\n",
      "step: 1992, train loss: 2.362, val loss 2.341\n",
      "1993\n",
      "step: 1993, train loss: 2.354, val loss 2.347\n",
      "1994\n",
      "step: 1994, train loss: 2.375, val loss 2.343\n",
      "1995\n",
      "step: 1995, train loss: 2.355, val loss 2.345\n",
      "1996\n",
      "step: 1996, train loss: 2.347, val loss 2.357\n",
      "1997\n",
      "step: 1997, train loss: 2.336, val loss 2.365\n",
      "1998\n",
      "step: 1998, train loss: 2.348, val loss 2.347\n",
      "1999\n",
      "step: 1999, train loss: 2.355, val loss 2.353\n",
      "2.31911563873291\n",
      "model saved\n",
      "Code block executed in 1007.4492 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    \n",
    "    losses = estimate_loss()\n",
    "\n",
    "    if iter % 20 == 0: print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits,loss = model(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-03.pkl', 'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "print('model saved')\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Code block executed in {elapsed_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471d0f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Can you see me?\n",
      "erodonom anctit t of\n",
      "who An tey arens),of thee y jorequt gearman ugeriatof and es walivell s, evene\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello! Can you see me?\"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19179d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Can you see me?.um s , may the a. ir an Ae t theny oyhemonelil t t fr\n",
      "or th f5iun\n",
      "man 9; a\n",
      "dhino\n",
      "omftes tm thusseve\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello! Can you see me?\"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4d845",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83840077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "step: 0, train loss: 2.527, val loss 2.558\n",
      "1\n",
      "step: 1, train loss: 2.559, val loss 2.578\n",
      "2\n",
      "step: 2, train loss: 2.586, val loss 2.581\n",
      "3\n",
      "step: 3, train loss: 2.586, val loss 2.627\n",
      "4\n",
      "step: 4, train loss: 2.583, val loss 2.575\n",
      "5\n",
      "step: 5, train loss: 2.610, val loss 2.565\n",
      "6\n",
      "step: 6, train loss: 2.588, val loss 2.549\n",
      "7\n",
      "step: 7, train loss: 2.550, val loss 2.579\n",
      "8\n",
      "step: 8, train loss: 2.595, val loss 2.548\n",
      "9\n",
      "step: 9, train loss: 2.576, val loss 2.544\n",
      "10\n",
      "step: 10, train loss: 2.543, val loss 2.556\n",
      "11\n",
      "step: 11, train loss: 2.566, val loss 2.543\n",
      "12\n",
      "step: 12, train loss: 2.549, val loss 2.557\n",
      "13\n",
      "step: 13, train loss: 2.569, val loss 2.546\n",
      "14\n",
      "step: 14, train loss: 2.580, val loss 2.540\n",
      "15\n",
      "step: 15, train loss: 2.575, val loss 2.531\n",
      "16\n",
      "step: 16, train loss: 2.567, val loss 2.542\n",
      "17\n",
      "step: 17, train loss: 2.547, val loss 2.544\n",
      "18\n",
      "step: 18, train loss: 2.538, val loss 2.537\n",
      "19\n",
      "step: 19, train loss: 2.543, val loss 2.546\n",
      "20\n",
      "step: 20, train loss: 2.538, val loss 2.569\n",
      "21\n",
      "step: 21, train loss: 2.522, val loss 2.518\n",
      "22\n",
      "step: 22, train loss: 2.540, val loss 2.515\n",
      "23\n",
      "step: 23, train loss: 2.550, val loss 2.511\n",
      "24\n",
      "step: 24, train loss: 2.514, val loss 2.517\n",
      "25\n",
      "step: 25, train loss: 2.541, val loss 2.547\n",
      "26\n",
      "step: 26, train loss: 2.526, val loss 2.534\n",
      "27\n",
      "step: 27, train loss: 2.529, val loss 2.556\n",
      "28\n",
      "step: 28, train loss: 2.508, val loss 2.548\n",
      "29\n",
      "step: 29, train loss: 2.507, val loss 2.532\n",
      "30\n",
      "step: 30, train loss: 2.481, val loss 2.555\n",
      "31\n",
      "step: 31, train loss: 2.505, val loss 2.508\n",
      "32\n",
      "step: 32, train loss: 2.518, val loss 2.506\n",
      "33\n",
      "step: 33, train loss: 2.529, val loss 2.523\n",
      "34\n",
      "step: 34, train loss: 2.532, val loss 2.537\n",
      "35\n",
      "step: 35, train loss: 2.492, val loss 2.493\n",
      "36\n",
      "step: 36, train loss: 2.512, val loss 2.558\n",
      "37\n",
      "step: 37, train loss: 2.512, val loss 2.551\n",
      "38\n",
      "step: 38, train loss: 2.504, val loss 2.539\n",
      "39\n",
      "step: 39, train loss: 2.521, val loss 2.525\n",
      "40\n",
      "step: 40, train loss: 2.541, val loss 2.477\n",
      "41\n",
      "step: 41, train loss: 2.532, val loss 2.484\n",
      "42\n",
      "step: 42, train loss: 2.519, val loss 2.522\n",
      "43\n",
      "step: 43, train loss: 2.476, val loss 2.526\n",
      "44\n",
      "step: 44, train loss: 2.496, val loss 2.493\n",
      "45\n",
      "step: 45, train loss: 2.498, val loss 2.507\n",
      "46\n",
      "step: 46, train loss: 2.528, val loss 2.500\n",
      "47\n",
      "step: 47, train loss: 2.454, val loss 2.498\n",
      "48\n",
      "step: 48, train loss: 2.497, val loss 2.493\n",
      "49\n",
      "step: 49, train loss: 2.475, val loss 2.460\n",
      "50\n",
      "step: 50, train loss: 2.515, val loss 2.473\n",
      "51\n",
      "step: 51, train loss: 2.495, val loss 2.501\n",
      "52\n",
      "step: 52, train loss: 2.492, val loss 2.480\n",
      "53\n",
      "step: 53, train loss: 2.514, val loss 2.494\n",
      "54\n",
      "step: 54, train loss: 2.444, val loss 2.478\n",
      "55\n",
      "step: 55, train loss: 2.497, val loss 2.550\n",
      "56\n",
      "step: 56, train loss: 2.484, val loss 2.478\n",
      "57\n",
      "step: 57, train loss: 2.465, val loss 2.494\n",
      "58\n",
      "step: 58, train loss: 2.518, val loss 2.490\n",
      "59\n",
      "step: 59, train loss: 2.477, val loss 2.485\n",
      "60\n",
      "step: 60, train loss: 2.506, val loss 2.513\n",
      "61\n",
      "step: 61, train loss: 2.472, val loss 2.495\n",
      "62\n",
      "step: 62, train loss: 2.496, val loss 2.495\n",
      "63\n",
      "step: 63, train loss: 2.459, val loss 2.477\n",
      "64\n",
      "step: 64, train loss: 2.487, val loss 2.455\n",
      "65\n",
      "step: 65, train loss: 2.471, val loss 2.444\n",
      "66\n",
      "step: 66, train loss: 2.479, val loss 2.455\n",
      "67\n",
      "step: 67, train loss: 2.446, val loss 2.447\n",
      "68\n",
      "step: 68, train loss: 2.469, val loss 2.470\n",
      "69\n",
      "step: 69, train loss: 2.462, val loss 2.438\n",
      "70\n",
      "step: 70, train loss: 2.468, val loss 2.473\n",
      "71\n",
      "step: 71, train loss: 2.503, val loss 2.461\n",
      "72\n",
      "step: 72, train loss: 2.477, val loss 2.461\n",
      "73\n",
      "step: 73, train loss: 2.448, val loss 2.480\n",
      "74\n",
      "step: 74, train loss: 2.452, val loss 2.469\n",
      "75\n",
      "step: 75, train loss: 2.452, val loss 2.455\n",
      "76\n",
      "step: 76, train loss: 2.469, val loss 2.466\n",
      "77\n",
      "step: 77, train loss: 2.455, val loss 2.456\n",
      "78\n",
      "step: 78, train loss: 2.450, val loss 2.475\n",
      "79\n",
      "step: 79, train loss: 2.457, val loss 2.476\n",
      "80\n",
      "step: 80, train loss: 2.446, val loss 2.477\n",
      "81\n",
      "step: 81, train loss: 2.486, val loss 2.493\n",
      "82\n",
      "step: 82, train loss: 2.458, val loss 2.473\n",
      "83\n",
      "step: 83, train loss: 2.450, val loss 2.472\n",
      "84\n",
      "step: 84, train loss: 2.462, val loss 2.454\n",
      "85\n",
      "step: 85, train loss: 2.483, val loss 2.462\n",
      "86\n",
      "step: 86, train loss: 2.455, val loss 2.466\n",
      "87\n",
      "step: 87, train loss: 2.515, val loss 2.475\n",
      "88\n",
      "step: 88, train loss: 2.462, val loss 2.519\n",
      "89\n",
      "step: 89, train loss: 2.427, val loss 2.490\n",
      "90\n",
      "step: 90, train loss: 2.445, val loss 2.445\n",
      "91\n",
      "step: 91, train loss: 2.467, val loss 2.459\n",
      "92\n",
      "step: 92, train loss: 2.450, val loss 2.452\n",
      "93\n",
      "step: 93, train loss: 2.483, val loss 2.488\n",
      "94\n",
      "step: 94, train loss: 2.503, val loss 2.436\n",
      "95\n",
      "step: 95, train loss: 2.461, val loss 2.462\n",
      "96\n",
      "step: 96, train loss: 2.459, val loss 2.488\n",
      "97\n",
      "step: 97, train loss: 2.453, val loss 2.456\n",
      "98\n",
      "step: 98, train loss: 2.479, val loss 2.469\n",
      "99\n",
      "step: 99, train loss: 2.470, val loss 2.476\n",
      "100\n",
      "step: 100, train loss: 2.443, val loss 2.467\n",
      "101\n",
      "step: 101, train loss: 2.463, val loss 2.459\n",
      "102\n",
      "step: 102, train loss: 2.480, val loss 2.470\n",
      "103\n",
      "step: 103, train loss: 2.451, val loss 2.463\n",
      "104\n",
      "step: 104, train loss: 2.460, val loss 2.456\n",
      "105\n",
      "step: 105, train loss: 2.416, val loss 2.452\n",
      "106\n",
      "step: 106, train loss: 2.446, val loss 2.426\n",
      "107\n",
      "step: 107, train loss: 2.440, val loss 2.457\n",
      "108\n",
      "step: 108, train loss: 2.440, val loss 2.453\n",
      "109\n",
      "step: 109, train loss: 2.490, val loss 2.439\n",
      "110\n",
      "step: 110, train loss: 2.487, val loss 2.432\n",
      "111\n",
      "step: 111, train loss: 2.486, val loss 2.466\n",
      "112\n",
      "step: 112, train loss: 2.434, val loss 2.441\n",
      "113\n",
      "step: 113, train loss: 2.466, val loss 2.446\n",
      "114\n",
      "step: 114, train loss: 2.469, val loss 2.489\n",
      "115\n",
      "step: 115, train loss: 2.471, val loss 2.463\n",
      "116\n",
      "step: 116, train loss: 2.483, val loss 2.467\n",
      "117\n",
      "step: 117, train loss: 2.503, val loss 2.473\n",
      "118\n",
      "step: 118, train loss: 2.476, val loss 2.470\n",
      "119\n",
      "step: 119, train loss: 2.443, val loss 2.477\n",
      "120\n",
      "step: 120, train loss: 2.441, val loss 2.475\n",
      "121\n",
      "step: 121, train loss: 2.467, val loss 2.460\n",
      "122\n",
      "step: 122, train loss: 2.469, val loss 2.457\n",
      "123\n",
      "step: 123, train loss: 2.445, val loss 2.450\n",
      "124\n",
      "step: 124, train loss: 2.456, val loss 2.439\n",
      "125\n",
      "step: 125, train loss: 2.481, val loss 2.441\n",
      "126\n",
      "step: 126, train loss: 2.427, val loss 2.445\n",
      "127\n",
      "step: 127, train loss: 2.497, val loss 2.434\n",
      "128\n",
      "step: 128, train loss: 2.437, val loss 2.468\n",
      "129\n",
      "step: 129, train loss: 2.437, val loss 2.443\n",
      "130\n",
      "step: 130, train loss: 2.412, val loss 2.441\n",
      "131\n",
      "step: 131, train loss: 2.452, val loss 2.462\n",
      "132\n",
      "step: 132, train loss: 2.436, val loss 2.453\n",
      "133\n",
      "step: 133, train loss: 2.473, val loss 2.450\n",
      "134\n",
      "step: 134, train loss: 2.461, val loss 2.441\n",
      "135\n",
      "step: 135, train loss: 2.449, val loss 2.432\n",
      "136\n",
      "step: 136, train loss: 2.414, val loss 2.439\n",
      "137\n",
      "step: 137, train loss: 2.459, val loss 2.443\n",
      "138\n",
      "step: 138, train loss: 2.437, val loss 2.423\n",
      "139\n",
      "step: 139, train loss: 2.471, val loss 2.440\n",
      "140\n",
      "step: 140, train loss: 2.502, val loss 2.442\n",
      "141\n",
      "step: 141, train loss: 2.441, val loss 2.444\n",
      "142\n",
      "step: 142, train loss: 2.455, val loss 2.434\n",
      "143\n",
      "step: 143, train loss: 2.441, val loss 2.454\n",
      "144\n",
      "step: 144, train loss: 2.448, val loss 2.441\n",
      "145\n",
      "step: 145, train loss: 2.469, val loss 2.457\n",
      "146\n",
      "step: 146, train loss: 2.445, val loss 2.449\n",
      "147\n",
      "step: 147, train loss: 2.435, val loss 2.457\n",
      "148\n",
      "step: 148, train loss: 2.433, val loss 2.427\n",
      "149\n",
      "step: 149, train loss: 2.448, val loss 2.451\n",
      "150\n",
      "step: 150, train loss: 2.441, val loss 2.484\n",
      "151\n",
      "step: 151, train loss: 2.442, val loss 2.452\n",
      "152\n",
      "step: 152, train loss: 2.434, val loss 2.409\n",
      "153\n",
      "step: 153, train loss: 2.401, val loss 2.428\n",
      "154\n",
      "step: 154, train loss: 2.426, val loss 2.418\n",
      "155\n",
      "step: 155, train loss: 2.448, val loss 2.414\n",
      "156\n",
      "step: 156, train loss: 2.422, val loss 2.423\n",
      "157\n",
      "step: 157, train loss: 2.440, val loss 2.425\n",
      "158\n",
      "step: 158, train loss: 2.425, val loss 2.418\n",
      "159\n",
      "step: 159, train loss: 2.407, val loss 2.436\n",
      "160\n",
      "step: 160, train loss: 2.442, val loss 2.416\n",
      "161\n",
      "step: 161, train loss: 2.412, val loss 2.406\n",
      "162\n",
      "step: 162, train loss: 2.431, val loss 2.442\n",
      "163\n",
      "step: 163, train loss: 2.422, val loss 2.403\n",
      "164\n",
      "step: 164, train loss: 2.431, val loss 2.415\n",
      "165\n",
      "step: 165, train loss: 2.421, val loss 2.430\n",
      "166\n",
      "step: 166, train loss: 2.428, val loss 2.420\n",
      "167\n",
      "step: 167, train loss: 2.432, val loss 2.394\n",
      "168\n",
      "step: 168, train loss: 2.415, val loss 2.432\n",
      "169\n",
      "step: 169, train loss: 2.391, val loss 2.415\n",
      "170\n",
      "step: 170, train loss: 2.425, val loss 2.475\n",
      "171\n",
      "step: 171, train loss: 2.457, val loss 2.441\n",
      "172\n",
      "step: 172, train loss: 2.430, val loss 2.426\n",
      "173\n",
      "step: 173, train loss: 2.456, val loss 2.485\n",
      "174\n",
      "step: 174, train loss: 2.426, val loss 2.445\n",
      "175\n",
      "step: 175, train loss: 2.447, val loss 2.467\n",
      "176\n",
      "step: 176, train loss: 2.454, val loss 2.435\n",
      "177\n",
      "step: 177, train loss: 2.438, val loss 2.435\n",
      "178\n",
      "step: 178, train loss: 2.432, val loss 2.434\n",
      "179\n",
      "step: 179, train loss: 2.436, val loss 2.433\n",
      "180\n",
      "step: 180, train loss: 2.465, val loss 2.441\n",
      "181\n",
      "step: 181, train loss: 2.422, val loss 2.463\n",
      "182\n",
      "step: 182, train loss: 2.449, val loss 2.425\n",
      "183\n",
      "step: 183, train loss: 2.441, val loss 2.409\n",
      "184\n",
      "step: 184, train loss: 2.436, val loss 2.443\n",
      "185\n",
      "step: 185, train loss: 2.407, val loss 2.438\n",
      "186\n",
      "step: 186, train loss: 2.405, val loss 2.439\n",
      "187\n",
      "step: 187, train loss: 2.448, val loss 2.445\n",
      "188\n",
      "step: 188, train loss: 2.420, val loss 2.462\n",
      "189\n",
      "step: 189, train loss: 2.420, val loss 2.437\n",
      "190\n",
      "step: 190, train loss: 2.415, val loss 2.419\n",
      "191\n",
      "step: 191, train loss: 2.449, val loss 2.438\n",
      "192\n",
      "step: 192, train loss: 2.419, val loss 2.435\n",
      "193\n",
      "step: 193, train loss: 2.470, val loss 2.446\n",
      "194\n",
      "step: 194, train loss: 2.426, val loss 2.436\n",
      "195\n",
      "step: 195, train loss: 2.429, val loss 2.421\n",
      "196\n",
      "step: 196, train loss: 2.438, val loss 2.427\n",
      "197\n",
      "step: 197, train loss: 2.421, val loss 2.439\n",
      "198\n",
      "step: 198, train loss: 2.423, val loss 2.435\n",
      "199\n",
      "step: 199, train loss: 2.421, val loss 2.397\n",
      "200\n",
      "step: 200, train loss: 2.444, val loss 2.443\n",
      "201\n",
      "step: 201, train loss: 2.412, val loss 2.410\n",
      "202\n",
      "step: 202, train loss: 2.424, val loss 2.449\n",
      "203\n",
      "step: 203, train loss: 2.395, val loss 2.409\n",
      "204\n",
      "step: 204, train loss: 2.428, val loss 2.443\n",
      "205\n",
      "step: 205, train loss: 2.426, val loss 2.436\n",
      "206\n",
      "step: 206, train loss: 2.409, val loss 2.441\n",
      "207\n",
      "step: 207, train loss: 2.469, val loss 2.439\n",
      "208\n",
      "step: 208, train loss: 2.428, val loss 2.429\n",
      "209\n",
      "step: 209, train loss: 2.436, val loss 2.424\n",
      "210\n",
      "step: 210, train loss: 2.439, val loss 2.431\n",
      "211\n",
      "step: 211, train loss: 2.402, val loss 2.415\n",
      "212\n",
      "step: 212, train loss: 2.449, val loss 2.431\n",
      "213\n",
      "step: 213, train loss: 2.416, val loss 2.472\n",
      "214\n",
      "step: 214, train loss: 2.407, val loss 2.436\n",
      "215\n",
      "step: 215, train loss: 2.456, val loss 2.400\n",
      "216\n",
      "step: 216, train loss: 2.409, val loss 2.425\n",
      "217\n",
      "step: 217, train loss: 2.472, val loss 2.433\n",
      "218\n",
      "step: 218, train loss: 2.411, val loss 2.436\n",
      "219\n",
      "step: 219, train loss: 2.447, val loss 2.415\n",
      "220\n",
      "step: 220, train loss: 2.412, val loss 2.450\n",
      "221\n",
      "step: 221, train loss: 2.382, val loss 2.415\n",
      "222\n",
      "step: 222, train loss: 2.415, val loss 2.432\n",
      "223\n",
      "step: 223, train loss: 2.437, val loss 2.400\n",
      "224\n",
      "step: 224, train loss: 2.426, val loss 2.453\n",
      "225\n",
      "step: 225, train loss: 2.457, val loss 2.441\n",
      "226\n",
      "step: 226, train loss: 2.407, val loss 2.421\n",
      "227\n",
      "step: 227, train loss: 2.403, val loss 2.415\n",
      "228\n",
      "step: 228, train loss: 2.455, val loss 2.410\n",
      "229\n",
      "step: 229, train loss: 2.426, val loss 2.424\n",
      "230\n",
      "step: 230, train loss: 2.452, val loss 2.402\n",
      "231\n",
      "step: 231, train loss: 2.431, val loss 2.403\n",
      "232\n",
      "step: 232, train loss: 2.424, val loss 2.417\n",
      "233\n",
      "step: 233, train loss: 2.399, val loss 2.414\n",
      "234\n",
      "step: 234, train loss: 2.438, val loss 2.417\n",
      "235\n",
      "step: 235, train loss: 2.418, val loss 2.416\n",
      "236\n",
      "step: 236, train loss: 2.419, val loss 2.398\n",
      "237\n",
      "step: 237, train loss: 2.419, val loss 2.449\n",
      "238\n",
      "step: 238, train loss: 2.430, val loss 2.423\n",
      "239\n",
      "step: 239, train loss: 2.432, val loss 2.409\n",
      "240\n",
      "step: 240, train loss: 2.409, val loss 2.438\n",
      "241\n",
      "step: 241, train loss: 2.422, val loss 2.427\n",
      "242\n",
      "step: 242, train loss: 2.422, val loss 2.407\n",
      "243\n",
      "step: 243, train loss: 2.401, val loss 2.425\n",
      "244\n",
      "step: 244, train loss: 2.398, val loss 2.415\n",
      "245\n",
      "step: 245, train loss: 2.399, val loss 2.423\n",
      "246\n",
      "step: 246, train loss: 2.454, val loss 2.488\n",
      "247\n",
      "step: 247, train loss: 2.424, val loss 2.442\n",
      "248\n",
      "step: 248, train loss: 2.420, val loss 2.439\n",
      "249\n",
      "step: 249, train loss: 2.451, val loss 2.444\n",
      "250\n",
      "step: 250, train loss: 2.420, val loss 2.436\n",
      "251\n",
      "step: 251, train loss: 2.421, val loss 2.426\n",
      "252\n",
      "step: 252, train loss: 2.430, val loss 2.431\n",
      "253\n",
      "step: 253, train loss: 2.438, val loss 2.426\n",
      "254\n",
      "step: 254, train loss: 2.413, val loss 2.420\n",
      "255\n",
      "step: 255, train loss: 2.420, val loss 2.429\n",
      "256\n",
      "step: 256, train loss: 2.453, val loss 2.430\n",
      "257\n",
      "step: 257, train loss: 2.430, val loss 2.427\n",
      "258\n",
      "step: 258, train loss: 2.423, val loss 2.424\n",
      "259\n",
      "step: 259, train loss: 2.412, val loss 2.411\n",
      "260\n",
      "step: 260, train loss: 2.408, val loss 2.420\n",
      "261\n",
      "step: 261, train loss: 2.432, val loss 2.426\n",
      "262\n",
      "step: 262, train loss: 2.423, val loss 2.388\n",
      "263\n",
      "step: 263, train loss: 2.411, val loss 2.429\n",
      "264\n",
      "step: 264, train loss: 2.456, val loss 2.439\n",
      "265\n",
      "step: 265, train loss: 2.444, val loss 2.412\n",
      "266\n",
      "step: 266, train loss: 2.422, val loss 2.442\n",
      "267\n",
      "step: 267, train loss: 2.440, val loss 2.428\n",
      "268\n",
      "step: 268, train loss: 2.421, val loss 2.420\n",
      "269\n",
      "step: 269, train loss: 2.387, val loss 2.413\n",
      "270\n",
      "step: 270, train loss: 2.424, val loss 2.407\n",
      "271\n",
      "step: 271, train loss: 2.411, val loss 2.434\n",
      "272\n",
      "step: 272, train loss: 2.416, val loss 2.411\n",
      "273\n",
      "step: 273, train loss: 2.383, val loss 2.409\n",
      "274\n",
      "step: 274, train loss: 2.421, val loss 2.412\n",
      "275\n",
      "step: 275, train loss: 2.416, val loss 2.416\n",
      "276\n",
      "step: 276, train loss: 2.395, val loss 2.379\n",
      "277\n",
      "step: 277, train loss: 2.375, val loss 2.428\n",
      "278\n",
      "step: 278, train loss: 2.404, val loss 2.377\n",
      "279\n",
      "step: 279, train loss: 2.383, val loss 2.418\n",
      "280\n",
      "step: 280, train loss: 2.409, val loss 2.435\n",
      "281\n",
      "step: 281, train loss: 2.435, val loss 2.423\n",
      "282\n",
      "step: 282, train loss: 2.392, val loss 2.451\n",
      "283\n",
      "step: 283, train loss: 2.416, val loss 2.390\n",
      "284\n",
      "step: 284, train loss: 2.385, val loss 2.442\n",
      "285\n",
      "step: 285, train loss: 2.420, val loss 2.404\n",
      "286\n",
      "step: 286, train loss: 2.437, val loss 2.386\n",
      "287\n",
      "step: 287, train loss: 2.390, val loss 2.404\n",
      "288\n",
      "step: 288, train loss: 2.385, val loss 2.395\n",
      "289\n",
      "step: 289, train loss: 2.415, val loss 2.442\n",
      "290\n",
      "step: 290, train loss: 2.390, val loss 2.425\n",
      "291\n",
      "step: 291, train loss: 2.404, val loss 2.416\n",
      "292\n",
      "step: 292, train loss: 2.388, val loss 2.421\n",
      "293\n",
      "step: 293, train loss: 2.409, val loss 2.390\n",
      "294\n",
      "step: 294, train loss: 2.423, val loss 2.422\n",
      "295\n",
      "step: 295, train loss: 2.403, val loss 2.419\n",
      "296\n",
      "step: 296, train loss: 2.399, val loss 2.418\n",
      "297\n",
      "step: 297, train loss: 2.427, val loss 2.383\n",
      "298\n",
      "step: 298, train loss: 2.398, val loss 2.404\n",
      "299\n",
      "step: 299, train loss: 2.418, val loss 2.476\n",
      "300\n",
      "step: 300, train loss: 2.388, val loss 2.416\n",
      "301\n",
      "step: 301, train loss: 2.404, val loss 2.396\n",
      "302\n",
      "step: 302, train loss: 2.419, val loss 2.391\n",
      "303\n",
      "step: 303, train loss: 2.413, val loss 2.429\n",
      "304\n",
      "step: 304, train loss: 2.443, val loss 2.424\n",
      "305\n",
      "step: 305, train loss: 2.401, val loss 2.401\n",
      "306\n",
      "step: 306, train loss: 2.406, val loss 2.403\n",
      "307\n",
      "step: 307, train loss: 2.409, val loss 2.464\n",
      "308\n",
      "step: 308, train loss: 2.406, val loss 2.429\n",
      "309\n",
      "step: 309, train loss: 2.394, val loss 2.405\n",
      "310\n",
      "step: 310, train loss: 2.388, val loss 2.421\n",
      "311\n",
      "step: 311, train loss: 2.437, val loss 2.375\n",
      "312\n",
      "step: 312, train loss: 2.396, val loss 2.424\n",
      "313\n",
      "step: 313, train loss: 2.390, val loss 2.400\n",
      "314\n",
      "step: 314, train loss: 2.408, val loss 2.421\n",
      "315\n",
      "step: 315, train loss: 2.421, val loss 2.402\n",
      "316\n",
      "step: 316, train loss: 2.404, val loss 2.413\n",
      "317\n",
      "step: 317, train loss: 2.380, val loss 2.393\n",
      "318\n",
      "step: 318, train loss: 2.406, val loss 2.403\n",
      "319\n",
      "step: 319, train loss: 2.402, val loss 2.394\n",
      "320\n",
      "step: 320, train loss: 2.412, val loss 2.424\n",
      "321\n",
      "step: 321, train loss: 2.421, val loss 2.414\n",
      "322\n",
      "step: 322, train loss: 2.403, val loss 2.413\n",
      "323\n",
      "step: 323, train loss: 2.417, val loss 2.415\n",
      "324\n",
      "step: 324, train loss: 2.404, val loss 2.387\n",
      "325\n",
      "step: 325, train loss: 2.402, val loss 2.415\n",
      "326\n",
      "step: 326, train loss: 2.412, val loss 2.429\n",
      "327\n",
      "step: 327, train loss: 2.390, val loss 2.443\n",
      "328\n",
      "step: 328, train loss: 2.410, val loss 2.414\n",
      "329\n",
      "step: 329, train loss: 2.442, val loss 2.420\n",
      "330\n",
      "step: 330, train loss: 2.399, val loss 2.386\n",
      "331\n",
      "step: 331, train loss: 2.381, val loss 2.445\n",
      "332\n",
      "step: 332, train loss: 2.401, val loss 2.392\n",
      "333\n",
      "step: 333, train loss: 2.413, val loss 2.380\n",
      "334\n",
      "step: 334, train loss: 2.408, val loss 2.399\n",
      "335\n",
      "step: 335, train loss: 2.441, val loss 2.412\n",
      "336\n",
      "step: 336, train loss: 2.405, val loss 2.389\n",
      "337\n",
      "step: 337, train loss: 2.368, val loss 2.435\n",
      "338\n",
      "step: 338, train loss: 2.377, val loss 2.384\n",
      "339\n",
      "step: 339, train loss: 2.416, val loss 2.388\n",
      "340\n",
      "step: 340, train loss: 2.436, val loss 2.433\n",
      "341\n",
      "step: 341, train loss: 2.405, val loss 2.409\n",
      "342\n",
      "step: 342, train loss: 2.395, val loss 2.421\n",
      "343\n",
      "step: 343, train loss: 2.429, val loss 2.403\n",
      "344\n",
      "step: 344, train loss: 2.451, val loss 2.403\n",
      "345\n",
      "step: 345, train loss: 2.391, val loss 2.440\n",
      "346\n",
      "step: 346, train loss: 2.409, val loss 2.381\n",
      "347\n",
      "step: 347, train loss: 2.391, val loss 2.432\n",
      "348\n",
      "step: 348, train loss: 2.406, val loss 2.430\n",
      "349\n",
      "step: 349, train loss: 2.432, val loss 2.403\n",
      "350\n",
      "step: 350, train loss: 2.430, val loss 2.414\n",
      "351\n",
      "step: 351, train loss: 2.429, val loss 2.380\n",
      "352\n",
      "step: 352, train loss: 2.418, val loss 2.422\n",
      "353\n",
      "step: 353, train loss: 2.426, val loss 2.391\n",
      "354\n",
      "step: 354, train loss: 2.411, val loss 2.400\n",
      "355\n",
      "step: 355, train loss: 2.391, val loss 2.391\n",
      "356\n",
      "step: 356, train loss: 2.432, val loss 2.395\n",
      "357\n",
      "step: 357, train loss: 2.415, val loss 2.384\n",
      "358\n",
      "step: 358, train loss: 2.405, val loss 2.418\n",
      "359\n",
      "step: 359, train loss: 2.402, val loss 2.419\n",
      "360\n",
      "step: 360, train loss: 2.394, val loss 2.374\n",
      "361\n",
      "step: 361, train loss: 2.410, val loss 2.414\n",
      "362\n",
      "step: 362, train loss: 2.392, val loss 2.399\n",
      "363\n",
      "step: 363, train loss: 2.410, val loss 2.415\n",
      "364\n",
      "step: 364, train loss: 2.428, val loss 2.392\n",
      "365\n",
      "step: 365, train loss: 2.391, val loss 2.394\n",
      "366\n",
      "step: 366, train loss: 2.434, val loss 2.421\n",
      "367\n",
      "step: 367, train loss: 2.424, val loss 2.403\n",
      "368\n",
      "step: 368, train loss: 2.431, val loss 2.406\n",
      "369\n",
      "step: 369, train loss: 2.411, val loss 2.405\n",
      "370\n",
      "step: 370, train loss: 2.414, val loss 2.434\n",
      "371\n",
      "step: 371, train loss: 2.407, val loss 2.404\n",
      "372\n",
      "step: 372, train loss: 2.422, val loss 2.444\n",
      "373\n",
      "step: 373, train loss: 2.403, val loss 2.412\n",
      "374\n",
      "step: 374, train loss: 2.367, val loss 2.411\n",
      "375\n",
      "step: 375, train loss: 2.429, val loss 2.402\n",
      "376\n",
      "step: 376, train loss: 2.413, val loss 2.398\n",
      "377\n",
      "step: 377, train loss: 2.453, val loss 2.428\n",
      "378\n",
      "step: 378, train loss: 2.425, val loss 2.383\n",
      "379\n",
      "step: 379, train loss: 2.414, val loss 2.460\n",
      "380\n",
      "step: 380, train loss: 2.362, val loss 2.404\n",
      "381\n",
      "step: 381, train loss: 2.404, val loss 2.378\n",
      "382\n",
      "step: 382, train loss: 2.401, val loss 2.465\n",
      "383\n",
      "step: 383, train loss: 2.384, val loss 2.387\n",
      "384\n",
      "step: 384, train loss: 2.377, val loss 2.376\n",
      "385\n",
      "step: 385, train loss: 2.390, val loss 2.398\n",
      "386\n",
      "step: 386, train loss: 2.398, val loss 2.420\n",
      "387\n",
      "step: 387, train loss: 2.398, val loss 2.401\n",
      "388\n",
      "step: 388, train loss: 2.402, val loss 2.416\n",
      "389\n",
      "step: 389, train loss: 2.427, val loss 2.397\n",
      "390\n",
      "step: 390, train loss: 2.394, val loss 2.371\n",
      "391\n",
      "step: 391, train loss: 2.392, val loss 2.381\n",
      "392\n",
      "step: 392, train loss: 2.426, val loss 2.389\n",
      "393\n",
      "step: 393, train loss: 2.397, val loss 2.408\n",
      "394\n",
      "step: 394, train loss: 2.384, val loss 2.397\n",
      "395\n",
      "step: 395, train loss: 2.386, val loss 2.381\n",
      "396\n",
      "step: 396, train loss: 2.401, val loss 2.412\n",
      "397\n",
      "step: 397, train loss: 2.395, val loss 2.395\n",
      "398\n",
      "step: 398, train loss: 2.386, val loss 2.367\n",
      "399\n",
      "step: 399, train loss: 2.403, val loss 2.421\n",
      "400\n",
      "step: 400, train loss: 2.408, val loss 2.405\n",
      "401\n",
      "step: 401, train loss: 2.413, val loss 2.435\n",
      "402\n",
      "step: 402, train loss: 2.413, val loss 2.415\n",
      "403\n",
      "step: 403, train loss: 2.429, val loss 2.445\n",
      "404\n",
      "step: 404, train loss: 2.412, val loss 2.386\n",
      "405\n",
      "step: 405, train loss: 2.450, val loss 2.413\n",
      "406\n",
      "step: 406, train loss: 2.417, val loss 2.393\n",
      "407\n",
      "step: 407, train loss: 2.464, val loss 2.438\n",
      "408\n",
      "step: 408, train loss: 2.398, val loss 2.385\n",
      "409\n",
      "step: 409, train loss: 2.411, val loss 2.386\n",
      "410\n",
      "step: 410, train loss: 2.402, val loss 2.395\n",
      "411\n",
      "step: 411, train loss: 2.424, val loss 2.407\n",
      "412\n",
      "step: 412, train loss: 2.363, val loss 2.406\n",
      "413\n",
      "step: 413, train loss: 2.402, val loss 2.407\n",
      "414\n",
      "step: 414, train loss: 2.395, val loss 2.402\n",
      "415\n",
      "step: 415, train loss: 2.385, val loss 2.419\n",
      "416\n",
      "step: 416, train loss: 2.394, val loss 2.410\n",
      "417\n",
      "step: 417, train loss: 2.389, val loss 2.408\n",
      "418\n",
      "step: 418, train loss: 2.394, val loss 2.375\n",
      "419\n",
      "step: 419, train loss: 2.399, val loss 2.396\n",
      "420\n",
      "step: 420, train loss: 2.390, val loss 2.381\n",
      "421\n",
      "step: 421, train loss: 2.402, val loss 2.399\n",
      "422\n",
      "step: 422, train loss: 2.391, val loss 2.406\n",
      "423\n",
      "step: 423, train loss: 2.403, val loss 2.367\n",
      "424\n",
      "step: 424, train loss: 2.419, val loss 2.390\n",
      "425\n",
      "step: 425, train loss: 2.378, val loss 2.428\n",
      "426\n",
      "step: 426, train loss: 2.402, val loss 2.366\n",
      "427\n",
      "step: 427, train loss: 2.403, val loss 2.379\n",
      "428\n",
      "step: 428, train loss: 2.407, val loss 2.397\n",
      "429\n",
      "step: 429, train loss: 2.449, val loss 2.403\n",
      "430\n",
      "step: 430, train loss: 2.391, val loss 2.409\n",
      "431\n",
      "step: 431, train loss: 2.399, val loss 2.451\n",
      "432\n",
      "step: 432, train loss: 2.420, val loss 2.418\n",
      "433\n",
      "step: 433, train loss: 2.413, val loss 2.391\n",
      "434\n",
      "step: 434, train loss: 2.425, val loss 2.390\n",
      "435\n",
      "step: 435, train loss: 2.401, val loss 2.393\n",
      "436\n",
      "step: 436, train loss: 2.391, val loss 2.402\n",
      "437\n",
      "step: 437, train loss: 2.399, val loss 2.388\n",
      "438\n",
      "step: 438, train loss: 2.378, val loss 2.419\n",
      "439\n",
      "step: 439, train loss: 2.402, val loss 2.380\n",
      "440\n",
      "step: 440, train loss: 2.388, val loss 2.391\n",
      "441\n",
      "step: 441, train loss: 2.409, val loss 2.405\n",
      "442\n",
      "step: 442, train loss: 2.396, val loss 2.381\n",
      "443\n",
      "step: 443, train loss: 2.424, val loss 2.420\n",
      "444\n",
      "step: 444, train loss: 2.398, val loss 2.404\n",
      "445\n",
      "step: 445, train loss: 2.382, val loss 2.409\n",
      "446\n",
      "step: 446, train loss: 2.426, val loss 2.404\n",
      "447\n",
      "step: 447, train loss: 2.411, val loss 2.422\n",
      "448\n",
      "step: 448, train loss: 2.443, val loss 2.404\n",
      "449\n",
      "step: 449, train loss: 2.455, val loss 2.397\n",
      "450\n",
      "step: 450, train loss: 2.402, val loss 2.463\n",
      "451\n",
      "step: 451, train loss: 2.423, val loss 2.405\n",
      "452\n",
      "step: 452, train loss: 2.400, val loss 2.414\n",
      "453\n",
      "step: 453, train loss: 2.416, val loss 2.407\n",
      "454\n",
      "step: 454, train loss: 2.400, val loss 2.400\n",
      "455\n",
      "step: 455, train loss: 2.420, val loss 2.405\n",
      "456\n",
      "step: 456, train loss: 2.412, val loss 2.410\n",
      "457\n",
      "step: 457, train loss: 2.389, val loss 2.402\n",
      "458\n",
      "step: 458, train loss: 2.409, val loss 2.427\n",
      "459\n",
      "step: 459, train loss: 2.413, val loss 2.390\n",
      "460\n",
      "step: 460, train loss: 2.418, val loss 2.396\n",
      "461\n",
      "step: 461, train loss: 2.400, val loss 2.401\n",
      "462\n",
      "step: 462, train loss: 2.361, val loss 2.401\n",
      "463\n",
      "step: 463, train loss: 2.388, val loss 2.363\n",
      "464\n",
      "step: 464, train loss: 2.394, val loss 2.396\n",
      "465\n",
      "step: 465, train loss: 2.411, val loss 2.387\n",
      "466\n",
      "step: 466, train loss: 2.420, val loss 2.400\n",
      "467\n",
      "step: 467, train loss: 2.399, val loss 2.443\n",
      "468\n",
      "step: 468, train loss: 2.370, val loss 2.391\n",
      "469\n",
      "step: 469, train loss: 2.396, val loss 2.396\n",
      "470\n",
      "step: 470, train loss: 2.397, val loss 2.395\n",
      "471\n",
      "step: 471, train loss: 2.407, val loss 2.392\n",
      "472\n",
      "step: 472, train loss: 2.396, val loss 2.400\n",
      "473\n",
      "step: 473, train loss: 2.368, val loss 2.382\n",
      "474\n",
      "step: 474, train loss: 2.393, val loss 2.378\n",
      "475\n",
      "step: 475, train loss: 2.382, val loss 2.420\n",
      "476\n",
      "step: 476, train loss: 2.417, val loss 2.409\n",
      "477\n",
      "step: 477, train loss: 2.364, val loss 2.397\n",
      "478\n",
      "step: 478, train loss: 2.405, val loss 2.418\n",
      "479\n",
      "step: 479, train loss: 2.408, val loss 2.369\n",
      "480\n",
      "step: 480, train loss: 2.382, val loss 2.404\n",
      "481\n",
      "step: 481, train loss: 2.394, val loss 2.391\n",
      "482\n",
      "step: 482, train loss: 2.384, val loss 2.390\n",
      "483\n",
      "step: 483, train loss: 2.376, val loss 2.378\n",
      "484\n",
      "step: 484, train loss: 2.402, val loss 2.371\n",
      "485\n",
      "step: 485, train loss: 2.379, val loss 2.390\n",
      "486\n",
      "step: 486, train loss: 2.391, val loss 2.380\n",
      "487\n",
      "step: 487, train loss: 2.407, val loss 2.396\n",
      "488\n",
      "step: 488, train loss: 2.379, val loss 2.383\n",
      "489\n",
      "step: 489, train loss: 2.414, val loss 2.383\n",
      "490\n",
      "step: 490, train loss: 2.387, val loss 2.391\n",
      "491\n",
      "step: 491, train loss: 2.372, val loss 2.413\n",
      "492\n",
      "step: 492, train loss: 2.383, val loss 2.373\n",
      "493\n",
      "step: 493, train loss: 2.398, val loss 2.392\n",
      "494\n",
      "step: 494, train loss: 2.383, val loss 2.382\n",
      "495\n",
      "step: 495, train loss: 2.412, val loss 2.424\n",
      "496\n",
      "step: 496, train loss: 2.361, val loss 2.429\n",
      "497\n",
      "step: 497, train loss: 2.426, val loss 2.394\n",
      "498\n",
      "step: 498, train loss: 2.439, val loss 2.390\n",
      "499\n",
      "step: 499, train loss: 2.406, val loss 2.444\n",
      "500\n",
      "step: 500, train loss: 2.383, val loss 2.402\n",
      "501\n",
      "step: 501, train loss: 2.395, val loss 2.407\n",
      "502\n",
      "step: 502, train loss: 2.389, val loss 2.399\n",
      "503\n",
      "step: 503, train loss: 2.392, val loss 2.424\n",
      "504\n",
      "step: 504, train loss: 2.384, val loss 2.403\n",
      "505\n",
      "step: 505, train loss: 2.376, val loss 2.368\n",
      "506\n",
      "step: 506, train loss: 2.397, val loss 2.407\n",
      "507\n",
      "step: 507, train loss: 2.405, val loss 2.402\n",
      "508\n",
      "step: 508, train loss: 2.404, val loss 2.416\n",
      "509\n",
      "step: 509, train loss: 2.383, val loss 2.370\n",
      "510\n",
      "step: 510, train loss: 2.424, val loss 2.407\n",
      "511\n",
      "step: 511, train loss: 2.393, val loss 2.416\n",
      "512\n",
      "step: 512, train loss: 2.390, val loss 2.413\n",
      "513\n",
      "step: 513, train loss: 2.433, val loss 2.413\n",
      "514\n",
      "step: 514, train loss: 2.400, val loss 2.444\n",
      "515\n",
      "step: 515, train loss: 2.421, val loss 2.387\n",
      "516\n",
      "step: 516, train loss: 2.386, val loss 2.371\n",
      "517\n",
      "step: 517, train loss: 2.404, val loss 2.399\n",
      "518\n",
      "step: 518, train loss: 2.404, val loss 2.378\n",
      "519\n",
      "step: 519, train loss: 2.369, val loss 2.404\n",
      "520\n",
      "step: 520, train loss: 2.357, val loss 2.392\n",
      "521\n",
      "step: 521, train loss: 2.367, val loss 2.360\n",
      "522\n",
      "step: 522, train loss: 2.397, val loss 2.379\n",
      "523\n",
      "step: 523, train loss: 2.424, val loss 2.389\n",
      "524\n",
      "step: 524, train loss: 2.430, val loss 2.404\n",
      "525\n",
      "step: 525, train loss: 2.423, val loss 2.399\n",
      "526\n",
      "step: 526, train loss: 2.378, val loss 2.433\n",
      "527\n",
      "step: 527, train loss: 2.365, val loss 2.442\n",
      "528\n",
      "step: 528, train loss: 2.374, val loss 2.372\n",
      "529\n",
      "step: 529, train loss: 2.376, val loss 2.377\n",
      "530\n",
      "step: 530, train loss: 2.389, val loss 2.416\n",
      "531\n",
      "step: 531, train loss: 2.379, val loss 2.363\n",
      "532\n",
      "step: 532, train loss: 2.370, val loss 2.385\n",
      "533\n",
      "step: 533, train loss: 2.393, val loss 2.373\n",
      "534\n",
      "step: 534, train loss: 2.393, val loss 2.386\n",
      "535\n",
      "step: 535, train loss: 2.399, val loss 2.397\n",
      "536\n",
      "step: 536, train loss: 2.405, val loss 2.389\n",
      "537\n",
      "step: 537, train loss: 2.428, val loss 2.378\n",
      "538\n",
      "step: 538, train loss: 2.403, val loss 2.374\n",
      "539\n",
      "step: 539, train loss: 2.412, val loss 2.408\n",
      "540\n",
      "step: 540, train loss: 2.382, val loss 2.411\n",
      "541\n",
      "step: 541, train loss: 2.393, val loss 2.389\n",
      "542\n",
      "step: 542, train loss: 2.405, val loss 2.369\n",
      "543\n",
      "step: 543, train loss: 2.353, val loss 2.374\n",
      "544\n",
      "step: 544, train loss: 2.390, val loss 2.411\n",
      "545\n",
      "step: 545, train loss: 2.377, val loss 2.390\n",
      "546\n",
      "step: 546, train loss: 2.362, val loss 2.358\n",
      "547\n",
      "step: 547, train loss: 2.386, val loss 2.387\n",
      "548\n",
      "step: 548, train loss: 2.410, val loss 2.385\n",
      "549\n",
      "step: 549, train loss: 2.402, val loss 2.389\n",
      "550\n",
      "step: 550, train loss: 2.406, val loss 2.371\n",
      "551\n",
      "step: 551, train loss: 2.387, val loss 2.399\n",
      "552\n",
      "step: 552, train loss: 2.406, val loss 2.385\n",
      "553\n",
      "step: 553, train loss: 2.412, val loss 2.420\n",
      "554\n",
      "step: 554, train loss: 2.370, val loss 2.392\n",
      "555\n",
      "step: 555, train loss: 2.368, val loss 2.392\n",
      "556\n",
      "step: 556, train loss: 2.400, val loss 2.384\n",
      "557\n",
      "step: 557, train loss: 2.399, val loss 2.383\n",
      "558\n",
      "step: 558, train loss: 2.406, val loss 2.403\n",
      "559\n",
      "step: 559, train loss: 2.407, val loss 2.396\n",
      "560\n",
      "step: 560, train loss: 2.412, val loss 2.400\n",
      "561\n",
      "step: 561, train loss: 2.400, val loss 2.410\n",
      "562\n",
      "step: 562, train loss: 2.397, val loss 2.369\n",
      "563\n",
      "step: 563, train loss: 2.404, val loss 2.397\n",
      "564\n",
      "step: 564, train loss: 2.422, val loss 2.398\n",
      "565\n",
      "step: 565, train loss: 2.411, val loss 2.403\n",
      "566\n",
      "step: 566, train loss: 2.406, val loss 2.381\n",
      "567\n",
      "step: 567, train loss: 2.410, val loss 2.396\n",
      "568\n",
      "step: 568, train loss: 2.376, val loss 2.370\n",
      "569\n",
      "step: 569, train loss: 2.386, val loss 2.385\n",
      "570\n",
      "step: 570, train loss: 2.382, val loss 2.386\n",
      "571\n",
      "step: 571, train loss: 2.375, val loss 2.382\n",
      "572\n",
      "step: 572, train loss: 2.393, val loss 2.400\n",
      "573\n",
      "step: 573, train loss: 2.382, val loss 2.405\n",
      "574\n",
      "step: 574, train loss: 2.381, val loss 2.404\n",
      "575\n",
      "step: 575, train loss: 2.389, val loss 2.377\n",
      "576\n",
      "step: 576, train loss: 2.350, val loss 2.391\n",
      "577\n",
      "step: 577, train loss: 2.397, val loss 2.382\n",
      "578\n",
      "step: 578, train loss: 2.383, val loss 2.369\n",
      "579\n",
      "step: 579, train loss: 2.371, val loss 2.366\n",
      "580\n",
      "step: 580, train loss: 2.383, val loss 2.380\n",
      "581\n",
      "step: 581, train loss: 2.400, val loss 2.382\n",
      "582\n",
      "step: 582, train loss: 2.384, val loss 2.370\n",
      "583\n",
      "step: 583, train loss: 2.403, val loss 2.400\n",
      "584\n",
      "step: 584, train loss: 2.371, val loss 2.385\n",
      "585\n",
      "step: 585, train loss: 2.409, val loss 2.394\n",
      "586\n",
      "step: 586, train loss: 2.362, val loss 2.369\n",
      "587\n",
      "step: 587, train loss: 2.386, val loss 2.378\n",
      "588\n",
      "step: 588, train loss: 2.372, val loss 2.406\n",
      "589\n",
      "step: 589, train loss: 2.391, val loss 2.424\n",
      "590\n",
      "step: 590, train loss: 2.356, val loss 2.392\n",
      "591\n",
      "step: 591, train loss: 2.435, val loss 2.377\n",
      "592\n",
      "step: 592, train loss: 2.364, val loss 2.392\n",
      "593\n",
      "step: 593, train loss: 2.391, val loss 2.369\n",
      "594\n",
      "step: 594, train loss: 2.391, val loss 2.370\n",
      "595\n",
      "step: 595, train loss: 2.377, val loss 2.394\n",
      "596\n",
      "step: 596, train loss: 2.385, val loss 2.391\n",
      "597\n",
      "step: 597, train loss: 2.378, val loss 2.368\n",
      "598\n",
      "step: 598, train loss: 2.359, val loss 2.373\n",
      "599\n",
      "step: 599, train loss: 2.368, val loss 2.363\n",
      "600\n",
      "step: 600, train loss: 2.395, val loss 2.370\n",
      "601\n",
      "step: 601, train loss: 2.377, val loss 2.370\n",
      "602\n",
      "step: 602, train loss: 2.386, val loss 2.358\n",
      "603\n",
      "step: 603, train loss: 2.394, val loss 2.393\n",
      "604\n",
      "step: 604, train loss: 2.420, val loss 2.363\n",
      "605\n",
      "step: 605, train loss: 2.379, val loss 2.435\n",
      "606\n",
      "step: 606, train loss: 2.400, val loss 2.397\n",
      "607\n",
      "step: 607, train loss: 2.403, val loss 2.412\n",
      "608\n",
      "step: 608, train loss: 2.363, val loss 2.381\n",
      "609\n",
      "step: 609, train loss: 2.365, val loss 2.356\n",
      "610\n",
      "step: 610, train loss: 2.394, val loss 2.367\n",
      "611\n",
      "step: 611, train loss: 2.353, val loss 2.388\n",
      "612\n",
      "step: 612, train loss: 2.365, val loss 2.394\n",
      "613\n",
      "step: 613, train loss: 2.381, val loss 2.397\n",
      "614\n",
      "step: 614, train loss: 2.369, val loss 2.363\n",
      "615\n",
      "step: 615, train loss: 2.388, val loss 2.398\n",
      "616\n",
      "step: 616, train loss: 2.359, val loss 2.378\n",
      "617\n",
      "step: 617, train loss: 2.381, val loss 2.424\n",
      "618\n",
      "step: 618, train loss: 2.376, val loss 2.385\n",
      "619\n",
      "step: 619, train loss: 2.394, val loss 2.376\n",
      "620\n",
      "step: 620, train loss: 2.378, val loss 2.395\n",
      "621\n",
      "step: 621, train loss: 2.372, val loss 2.381\n",
      "622\n",
      "step: 622, train loss: 2.403, val loss 2.392\n",
      "623\n",
      "step: 623, train loss: 2.417, val loss 2.376\n",
      "624\n",
      "step: 624, train loss: 2.377, val loss 2.360\n",
      "625\n",
      "step: 625, train loss: 2.391, val loss 2.376\n",
      "626\n",
      "step: 626, train loss: 2.399, val loss 2.426\n",
      "627\n",
      "step: 627, train loss: 2.392, val loss 2.430\n",
      "628\n",
      "step: 628, train loss: 2.388, val loss 2.380\n",
      "629\n",
      "step: 629, train loss: 2.370, val loss 2.392\n",
      "630\n",
      "step: 630, train loss: 2.382, val loss 2.384\n",
      "631\n",
      "step: 631, train loss: 2.367, val loss 2.401\n",
      "632\n",
      "step: 632, train loss: 2.404, val loss 2.370\n",
      "633\n",
      "step: 633, train loss: 2.366, val loss 2.376\n",
      "634\n",
      "step: 634, train loss: 2.367, val loss 2.402\n",
      "635\n",
      "step: 635, train loss: 2.354, val loss 2.411\n",
      "636\n",
      "step: 636, train loss: 2.394, val loss 2.369\n",
      "637\n",
      "step: 637, train loss: 2.364, val loss 2.379\n",
      "638\n",
      "step: 638, train loss: 2.408, val loss 2.413\n",
      "639\n",
      "step: 639, train loss: 2.375, val loss 2.402\n",
      "640\n",
      "step: 640, train loss: 2.350, val loss 2.357\n",
      "641\n",
      "step: 641, train loss: 2.400, val loss 2.365\n",
      "642\n",
      "step: 642, train loss: 2.370, val loss 2.360\n",
      "643\n",
      "step: 643, train loss: 2.379, val loss 2.369\n",
      "644\n",
      "step: 644, train loss: 2.360, val loss 2.367\n",
      "645\n",
      "step: 645, train loss: 2.371, val loss 2.397\n",
      "646\n",
      "step: 646, train loss: 2.390, val loss 2.378\n",
      "647\n",
      "step: 647, train loss: 2.402, val loss 2.418\n",
      "648\n",
      "step: 648, train loss: 2.402, val loss 2.383\n",
      "649\n",
      "step: 649, train loss: 2.402, val loss 2.362\n",
      "650\n",
      "step: 650, train loss: 2.382, val loss 2.375\n",
      "651\n",
      "step: 651, train loss: 2.387, val loss 2.370\n",
      "652\n",
      "step: 652, train loss: 2.380, val loss 2.390\n",
      "653\n",
      "step: 653, train loss: 2.386, val loss 2.412\n",
      "654\n",
      "step: 654, train loss: 2.392, val loss 2.393\n",
      "655\n",
      "step: 655, train loss: 2.380, val loss 2.399\n",
      "656\n",
      "step: 656, train loss: 2.378, val loss 2.398\n",
      "657\n",
      "step: 657, train loss: 2.359, val loss 2.387\n",
      "658\n",
      "step: 658, train loss: 2.344, val loss 2.374\n",
      "659\n",
      "step: 659, train loss: 2.374, val loss 2.387\n",
      "660\n",
      "step: 660, train loss: 2.385, val loss 2.383\n",
      "661\n",
      "step: 661, train loss: 2.391, val loss 2.348\n",
      "662\n",
      "step: 662, train loss: 2.398, val loss 2.365\n",
      "663\n",
      "step: 663, train loss: 2.371, val loss 2.356\n",
      "664\n",
      "step: 664, train loss: 2.407, val loss 2.381\n",
      "665\n",
      "step: 665, train loss: 2.384, val loss 2.377\n",
      "666\n",
      "step: 666, train loss: 2.396, val loss 2.385\n",
      "667\n",
      "step: 667, train loss: 2.417, val loss 2.380\n",
      "668\n",
      "step: 668, train loss: 2.370, val loss 2.395\n",
      "669\n",
      "step: 669, train loss: 2.398, val loss 2.381\n",
      "670\n",
      "step: 670, train loss: 2.364, val loss 2.398\n",
      "671\n",
      "step: 671, train loss: 2.387, val loss 2.388\n",
      "672\n",
      "step: 672, train loss: 2.388, val loss 2.383\n",
      "673\n",
      "step: 673, train loss: 2.393, val loss 2.399\n",
      "674\n",
      "step: 674, train loss: 2.378, val loss 2.392\n",
      "675\n",
      "step: 675, train loss: 2.385, val loss 2.385\n",
      "676\n",
      "step: 676, train loss: 2.367, val loss 2.398\n",
      "677\n",
      "step: 677, train loss: 2.400, val loss 2.369\n",
      "678\n",
      "step: 678, train loss: 2.371, val loss 2.344\n",
      "679\n",
      "step: 679, train loss: 2.398, val loss 2.374\n",
      "680\n",
      "step: 680, train loss: 2.390, val loss 2.377\n",
      "681\n",
      "step: 681, train loss: 2.376, val loss 2.369\n",
      "682\n",
      "step: 682, train loss: 2.368, val loss 2.350\n",
      "683\n",
      "step: 683, train loss: 2.359, val loss 2.387\n",
      "684\n",
      "step: 684, train loss: 2.386, val loss 2.373\n",
      "685\n",
      "step: 685, train loss: 2.383, val loss 2.382\n",
      "686\n",
      "step: 686, train loss: 2.352, val loss 2.365\n",
      "687\n",
      "step: 687, train loss: 2.420, val loss 2.362\n",
      "688\n",
      "step: 688, train loss: 2.372, val loss 2.373\n",
      "689\n",
      "step: 689, train loss: 2.379, val loss 2.378\n",
      "690\n",
      "step: 690, train loss: 2.394, val loss 2.381\n",
      "691\n",
      "step: 691, train loss: 2.357, val loss 2.392\n",
      "692\n",
      "step: 692, train loss: 2.372, val loss 2.368\n",
      "693\n",
      "step: 693, train loss: 2.373, val loss 2.371\n",
      "694\n",
      "step: 694, train loss: 2.375, val loss 2.376\n",
      "695\n",
      "step: 695, train loss: 2.356, val loss 2.374\n",
      "696\n",
      "step: 696, train loss: 2.356, val loss 2.371\n",
      "697\n",
      "step: 697, train loss: 2.340, val loss 2.375\n",
      "698\n",
      "step: 698, train loss: 2.378, val loss 2.371\n",
      "699\n",
      "step: 699, train loss: 2.374, val loss 2.378\n",
      "700\n",
      "step: 700, train loss: 2.355, val loss 2.368\n",
      "701\n",
      "step: 701, train loss: 2.355, val loss 2.383\n",
      "702\n",
      "step: 702, train loss: 2.382, val loss 2.372\n",
      "703\n",
      "step: 703, train loss: 2.396, val loss 2.370\n",
      "704\n",
      "step: 704, train loss: 2.407, val loss 2.361\n",
      "705\n",
      "step: 705, train loss: 2.376, val loss 2.371\n",
      "706\n",
      "step: 706, train loss: 2.369, val loss 2.366\n",
      "707\n",
      "step: 707, train loss: 2.367, val loss 2.384\n",
      "708\n",
      "step: 708, train loss: 2.360, val loss 2.412\n",
      "709\n",
      "step: 709, train loss: 2.365, val loss 2.356\n",
      "710\n",
      "step: 710, train loss: 2.373, val loss 2.378\n",
      "711\n",
      "step: 711, train loss: 2.367, val loss 2.378\n",
      "712\n",
      "step: 712, train loss: 2.384, val loss 2.369\n",
      "713\n",
      "step: 713, train loss: 2.412, val loss 2.372\n",
      "714\n",
      "step: 714, train loss: 2.400, val loss 2.346\n",
      "715\n",
      "step: 715, train loss: 2.357, val loss 2.350\n",
      "716\n",
      "step: 716, train loss: 2.384, val loss 2.385\n",
      "717\n",
      "step: 717, train loss: 2.380, val loss 2.391\n",
      "718\n",
      "step: 718, train loss: 2.367, val loss 2.387\n",
      "719\n",
      "step: 719, train loss: 2.370, val loss 2.388\n",
      "720\n",
      "step: 720, train loss: 2.364, val loss 2.382\n",
      "721\n",
      "step: 721, train loss: 2.385, val loss 2.380\n",
      "722\n",
      "step: 722, train loss: 2.362, val loss 2.364\n",
      "723\n",
      "step: 723, train loss: 2.363, val loss 2.384\n",
      "724\n",
      "step: 724, train loss: 2.368, val loss 2.393\n",
      "725\n",
      "step: 725, train loss: 2.367, val loss 2.373\n",
      "726\n",
      "step: 726, train loss: 2.392, val loss 2.343\n",
      "727\n",
      "step: 727, train loss: 2.395, val loss 2.375\n",
      "728\n",
      "step: 728, train loss: 2.360, val loss 2.383\n",
      "729\n",
      "step: 729, train loss: 2.365, val loss 2.357\n",
      "730\n",
      "step: 730, train loss: 2.369, val loss 2.397\n",
      "731\n",
      "step: 731, train loss: 2.390, val loss 2.360\n",
      "732\n",
      "step: 732, train loss: 2.380, val loss 2.371\n",
      "733\n",
      "step: 733, train loss: 2.367, val loss 2.342\n",
      "734\n",
      "step: 734, train loss: 2.358, val loss 2.385\n",
      "735\n",
      "step: 735, train loss: 2.373, val loss 2.366\n",
      "736\n",
      "step: 736, train loss: 2.372, val loss 2.391\n",
      "737\n",
      "step: 737, train loss: 2.378, val loss 2.359\n",
      "738\n",
      "step: 738, train loss: 2.371, val loss 2.375\n",
      "739\n",
      "step: 739, train loss: 2.383, val loss 2.415\n",
      "740\n",
      "step: 740, train loss: 2.362, val loss 2.372\n",
      "741\n",
      "step: 741, train loss: 2.348, val loss 2.355\n",
      "742\n",
      "step: 742, train loss: 2.393, val loss 2.368\n",
      "743\n",
      "step: 743, train loss: 2.368, val loss 2.370\n",
      "744\n",
      "step: 744, train loss: 2.363, val loss 2.370\n",
      "745\n",
      "step: 745, train loss: 2.361, val loss 2.386\n",
      "746\n",
      "step: 746, train loss: 2.375, val loss 2.379\n",
      "747\n",
      "step: 747, train loss: 2.366, val loss 2.365\n",
      "748\n",
      "step: 748, train loss: 2.404, val loss 2.354\n",
      "749\n",
      "step: 749, train loss: 2.367, val loss 2.356\n",
      "750\n",
      "step: 750, train loss: 2.376, val loss 2.348\n",
      "751\n",
      "step: 751, train loss: 2.375, val loss 2.354\n",
      "752\n",
      "step: 752, train loss: 2.359, val loss 2.386\n",
      "753\n",
      "step: 753, train loss: 2.340, val loss 2.362\n",
      "754\n",
      "step: 754, train loss: 2.373, val loss 2.345\n",
      "755\n",
      "step: 755, train loss: 2.354, val loss 2.373\n",
      "756\n",
      "step: 756, train loss: 2.378, val loss 2.393\n",
      "757\n",
      "step: 757, train loss: 2.358, val loss 2.350\n",
      "758\n",
      "step: 758, train loss: 2.376, val loss 2.352\n",
      "759\n",
      "step: 759, train loss: 2.356, val loss 2.350\n",
      "760\n",
      "step: 760, train loss: 2.372, val loss 2.375\n",
      "761\n",
      "step: 761, train loss: 2.364, val loss 2.387\n",
      "762\n",
      "step: 762, train loss: 2.307, val loss 2.342\n",
      "763\n",
      "step: 763, train loss: 2.344, val loss 2.368\n",
      "764\n",
      "step: 764, train loss: 2.370, val loss 2.363\n",
      "765\n",
      "step: 765, train loss: 2.364, val loss 2.373\n",
      "766\n",
      "step: 766, train loss: 2.376, val loss 2.362\n",
      "767\n",
      "step: 767, train loss: 2.357, val loss 2.351\n",
      "768\n",
      "step: 768, train loss: 2.344, val loss 2.351\n",
      "769\n",
      "step: 769, train loss: 2.344, val loss 2.341\n",
      "770\n",
      "step: 770, train loss: 2.345, val loss 2.351\n",
      "771\n",
      "step: 771, train loss: 2.386, val loss 2.359\n",
      "772\n",
      "step: 772, train loss: 2.359, val loss 2.352\n",
      "773\n",
      "step: 773, train loss: 2.362, val loss 2.364\n",
      "774\n",
      "step: 774, train loss: 2.357, val loss 2.394\n",
      "775\n",
      "step: 775, train loss: 2.390, val loss 2.361\n",
      "776\n",
      "step: 776, train loss: 2.352, val loss 2.360\n",
      "777\n",
      "step: 777, train loss: 2.385, val loss 2.357\n",
      "778\n",
      "step: 778, train loss: 2.367, val loss 2.358\n",
      "779\n",
      "step: 779, train loss: 2.389, val loss 2.348\n",
      "780\n",
      "step: 780, train loss: 2.383, val loss 2.379\n",
      "781\n",
      "step: 781, train loss: 2.368, val loss 2.385\n",
      "782\n",
      "step: 782, train loss: 2.367, val loss 2.380\n",
      "783\n",
      "step: 783, train loss: 2.358, val loss 2.366\n",
      "784\n",
      "step: 784, train loss: 2.375, val loss 2.376\n",
      "785\n",
      "step: 785, train loss: 2.383, val loss 2.384\n",
      "786\n",
      "step: 786, train loss: 2.374, val loss 2.391\n",
      "787\n",
      "step: 787, train loss: 2.374, val loss 2.375\n",
      "788\n",
      "step: 788, train loss: 2.393, val loss 2.369\n",
      "789\n",
      "step: 789, train loss: 2.367, val loss 2.394\n",
      "790\n",
      "step: 790, train loss: 2.354, val loss 2.408\n",
      "791\n",
      "step: 791, train loss: 2.396, val loss 2.347\n",
      "792\n",
      "step: 792, train loss: 2.380, val loss 2.366\n",
      "793\n",
      "step: 793, train loss: 2.363, val loss 2.392\n",
      "794\n",
      "step: 794, train loss: 2.363, val loss 2.381\n",
      "795\n",
      "step: 795, train loss: 2.358, val loss 2.363\n",
      "796\n",
      "step: 796, train loss: 2.352, val loss 2.347\n",
      "797\n",
      "step: 797, train loss: 2.383, val loss 2.347\n",
      "798\n",
      "step: 798, train loss: 2.352, val loss 2.350\n",
      "799\n",
      "step: 799, train loss: 2.359, val loss 2.388\n",
      "800\n",
      "step: 800, train loss: 2.359, val loss 2.357\n",
      "801\n",
      "step: 801, train loss: 2.378, val loss 2.367\n",
      "802\n",
      "step: 802, train loss: 2.376, val loss 2.357\n",
      "803\n",
      "step: 803, train loss: 2.353, val loss 2.369\n",
      "804\n",
      "step: 804, train loss: 2.372, val loss 2.378\n",
      "805\n",
      "step: 805, train loss: 2.340, val loss 2.345\n",
      "806\n",
      "step: 806, train loss: 2.365, val loss 2.381\n",
      "807\n",
      "step: 807, train loss: 2.389, val loss 2.393\n",
      "808\n",
      "step: 808, train loss: 2.332, val loss 2.340\n",
      "809\n",
      "step: 809, train loss: 2.391, val loss 2.358\n",
      "810\n",
      "step: 810, train loss: 2.360, val loss 2.378\n",
      "811\n",
      "step: 811, train loss: 2.349, val loss 2.352\n",
      "812\n",
      "step: 812, train loss: 2.354, val loss 2.355\n",
      "813\n",
      "step: 813, train loss: 2.348, val loss 2.354\n",
      "814\n",
      "step: 814, train loss: 2.351, val loss 2.346\n",
      "815\n",
      "step: 815, train loss: 2.335, val loss 2.348\n",
      "816\n",
      "step: 816, train loss: 2.356, val loss 2.362\n",
      "817\n",
      "step: 817, train loss: 2.376, val loss 2.350\n",
      "818\n",
      "step: 818, train loss: 2.355, val loss 2.334\n",
      "819\n",
      "step: 819, train loss: 2.336, val loss 2.367\n",
      "820\n",
      "step: 820, train loss: 2.345, val loss 2.360\n",
      "821\n",
      "step: 821, train loss: 2.355, val loss 2.377\n",
      "822\n",
      "step: 822, train loss: 2.351, val loss 2.356\n",
      "823\n",
      "step: 823, train loss: 2.359, val loss 2.392\n",
      "824\n",
      "step: 824, train loss: 2.333, val loss 2.368\n",
      "825\n",
      "step: 825, train loss: 2.333, val loss 2.340\n",
      "826\n",
      "step: 826, train loss: 2.365, val loss 2.353\n",
      "827\n",
      "step: 827, train loss: 2.386, val loss 2.352\n",
      "828\n",
      "step: 828, train loss: 2.355, val loss 2.352\n",
      "829\n",
      "step: 829, train loss: 2.360, val loss 2.349\n",
      "830\n",
      "step: 830, train loss: 2.398, val loss 2.363\n",
      "831\n",
      "step: 831, train loss: 2.345, val loss 2.390\n",
      "832\n",
      "step: 832, train loss: 2.362, val loss 2.342\n",
      "833\n",
      "step: 833, train loss: 2.356, val loss 2.337\n",
      "834\n",
      "step: 834, train loss: 2.339, val loss 2.371\n",
      "835\n",
      "step: 835, train loss: 2.371, val loss 2.367\n",
      "836\n",
      "step: 836, train loss: 2.343, val loss 2.342\n",
      "837\n",
      "step: 837, train loss: 2.374, val loss 2.353\n",
      "838\n",
      "step: 838, train loss: 2.333, val loss 2.361\n",
      "839\n",
      "step: 839, train loss: 2.334, val loss 2.338\n",
      "840\n",
      "step: 840, train loss: 2.346, val loss 2.347\n",
      "841\n",
      "step: 841, train loss: 2.338, val loss 2.372\n",
      "842\n",
      "step: 842, train loss: 2.367, val loss 2.336\n",
      "843\n",
      "step: 843, train loss: 2.322, val loss 2.322\n",
      "844\n",
      "step: 844, train loss: 2.360, val loss 2.366\n",
      "845\n",
      "step: 845, train loss: 2.333, val loss 2.364\n",
      "846\n",
      "step: 846, train loss: 2.375, val loss 2.363\n",
      "847\n",
      "step: 847, train loss: 2.380, val loss 2.341\n",
      "848\n",
      "step: 848, train loss: 2.358, val loss 2.350\n",
      "849\n",
      "step: 849, train loss: 2.359, val loss 2.364\n",
      "850\n",
      "step: 850, train loss: 2.370, val loss 2.376\n",
      "851\n",
      "step: 851, train loss: 2.330, val loss 2.366\n",
      "852\n",
      "step: 852, train loss: 2.377, val loss 2.339\n",
      "853\n",
      "step: 853, train loss: 2.332, val loss 2.360\n",
      "854\n",
      "step: 854, train loss: 2.335, val loss 2.379\n",
      "855\n",
      "step: 855, train loss: 2.349, val loss 2.362\n",
      "856\n",
      "step: 856, train loss: 2.356, val loss 2.353\n",
      "857\n",
      "step: 857, train loss: 2.371, val loss 2.359\n",
      "858\n",
      "step: 858, train loss: 2.350, val loss 2.336\n",
      "859\n",
      "step: 859, train loss: 2.345, val loss 2.362\n",
      "860\n",
      "step: 860, train loss: 2.347, val loss 2.352\n",
      "861\n",
      "step: 861, train loss: 2.336, val loss 2.353\n",
      "862\n",
      "step: 862, train loss: 2.335, val loss 2.353\n",
      "863\n",
      "step: 863, train loss: 2.335, val loss 2.348\n",
      "864\n",
      "step: 864, train loss: 2.369, val loss 2.338\n",
      "865\n",
      "step: 865, train loss: 2.350, val loss 2.343\n",
      "866\n",
      "step: 866, train loss: 2.337, val loss 2.334\n",
      "867\n",
      "step: 867, train loss: 2.334, val loss 2.389\n",
      "868\n",
      "step: 868, train loss: 2.357, val loss 2.370\n",
      "869\n",
      "step: 869, train loss: 2.352, val loss 2.338\n",
      "870\n",
      "step: 870, train loss: 2.343, val loss 2.353\n",
      "871\n",
      "step: 871, train loss: 2.321, val loss 2.363\n",
      "872\n",
      "step: 872, train loss: 2.374, val loss 2.348\n",
      "873\n",
      "step: 873, train loss: 2.348, val loss 2.355\n",
      "874\n",
      "step: 874, train loss: 2.346, val loss 2.340\n",
      "875\n",
      "step: 875, train loss: 2.391, val loss 2.331\n",
      "876\n",
      "step: 876, train loss: 2.350, val loss 2.332\n",
      "877\n",
      "step: 877, train loss: 2.351, val loss 2.337\n",
      "878\n",
      "step: 878, train loss: 2.359, val loss 2.346\n",
      "879\n",
      "step: 879, train loss: 2.356, val loss 2.364\n",
      "880\n",
      "step: 880, train loss: 2.323, val loss 2.331\n",
      "881\n",
      "step: 881, train loss: 2.337, val loss 2.358\n",
      "882\n",
      "step: 882, train loss: 2.326, val loss 2.336\n",
      "883\n",
      "step: 883, train loss: 2.359, val loss 2.363\n",
      "884\n",
      "step: 884, train loss: 2.351, val loss 2.393\n",
      "885\n",
      "step: 885, train loss: 2.356, val loss 2.350\n",
      "886\n",
      "step: 886, train loss: 2.322, val loss 2.334\n",
      "887\n",
      "step: 887, train loss: 2.343, val loss 2.337\n",
      "888\n",
      "step: 888, train loss: 2.359, val loss 2.358\n",
      "889\n",
      "step: 889, train loss: 2.344, val loss 2.319\n",
      "890\n",
      "step: 890, train loss: 2.338, val loss 2.348\n",
      "891\n",
      "step: 891, train loss: 2.332, val loss 2.347\n",
      "892\n",
      "step: 892, train loss: 2.320, val loss 2.335\n",
      "893\n",
      "step: 893, train loss: 2.364, val loss 2.339\n",
      "894\n",
      "step: 894, train loss: 2.344, val loss 2.335\n",
      "895\n",
      "step: 895, train loss: 2.347, val loss 2.336\n",
      "896\n",
      "step: 896, train loss: 2.346, val loss 2.352\n",
      "897\n",
      "step: 897, train loss: 2.325, val loss 2.359\n",
      "898\n",
      "step: 898, train loss: 2.330, val loss 2.347\n",
      "899\n",
      "step: 899, train loss: 2.373, val loss 2.342\n",
      "900\n",
      "step: 900, train loss: 2.357, val loss 2.362\n",
      "901\n",
      "step: 901, train loss: 2.354, val loss 2.346\n",
      "902\n",
      "step: 902, train loss: 2.340, val loss 2.355\n",
      "903\n",
      "step: 903, train loss: 2.350, val loss 2.364\n",
      "904\n",
      "step: 904, train loss: 2.390, val loss 2.357\n",
      "905\n",
      "step: 905, train loss: 2.366, val loss 2.355\n",
      "906\n",
      "step: 906, train loss: 2.344, val loss 2.357\n",
      "907\n",
      "step: 907, train loss: 2.361, val loss 2.353\n",
      "908\n",
      "step: 908, train loss: 2.353, val loss 2.362\n",
      "909\n",
      "step: 909, train loss: 2.340, val loss 2.340\n",
      "910\n",
      "step: 910, train loss: 2.349, val loss 2.351\n",
      "911\n",
      "step: 911, train loss: 2.327, val loss 2.342\n",
      "912\n",
      "step: 912, train loss: 2.367, val loss 2.367\n",
      "913\n",
      "step: 913, train loss: 2.357, val loss 2.372\n",
      "914\n",
      "step: 914, train loss: 2.362, val loss 2.378\n",
      "915\n",
      "step: 915, train loss: 2.361, val loss 2.406\n",
      "916\n",
      "step: 916, train loss: 2.383, val loss 2.351\n",
      "917\n",
      "step: 917, train loss: 2.356, val loss 2.381\n",
      "918\n",
      "step: 918, train loss: 2.368, val loss 2.359\n",
      "919\n",
      "step: 919, train loss: 2.351, val loss 2.358\n",
      "920\n",
      "step: 920, train loss: 2.361, val loss 2.356\n",
      "921\n",
      "step: 921, train loss: 2.373, val loss 2.334\n",
      "922\n",
      "step: 922, train loss: 2.372, val loss 2.377\n",
      "923\n",
      "step: 923, train loss: 2.350, val loss 2.356\n",
      "924\n",
      "step: 924, train loss: 2.370, val loss 2.360\n",
      "925\n",
      "step: 925, train loss: 2.393, val loss 2.402\n",
      "926\n",
      "step: 926, train loss: 2.384, val loss 2.369\n",
      "927\n",
      "step: 927, train loss: 2.359, val loss 2.388\n",
      "928\n",
      "step: 928, train loss: 2.361, val loss 2.362\n",
      "929\n",
      "step: 929, train loss: 2.341, val loss 2.368\n",
      "930\n",
      "step: 930, train loss: 2.330, val loss 2.362\n",
      "931\n",
      "step: 931, train loss: 2.340, val loss 2.366\n",
      "932\n",
      "step: 932, train loss: 2.314, val loss 2.359\n",
      "933\n",
      "step: 933, train loss: 2.350, val loss 2.350\n",
      "934\n",
      "step: 934, train loss: 2.353, val loss 2.332\n",
      "935\n",
      "step: 935, train loss: 2.366, val loss 2.319\n",
      "936\n",
      "step: 936, train loss: 2.360, val loss 2.360\n",
      "937\n",
      "step: 937, train loss: 2.345, val loss 2.346\n",
      "938\n",
      "step: 938, train loss: 2.326, val loss 2.335\n",
      "939\n",
      "step: 939, train loss: 2.351, val loss 2.342\n",
      "940\n",
      "step: 940, train loss: 2.346, val loss 2.332\n",
      "941\n",
      "step: 941, train loss: 2.360, val loss 2.355\n",
      "942\n",
      "step: 942, train loss: 2.338, val loss 2.321\n",
      "943\n",
      "step: 943, train loss: 2.376, val loss 2.341\n",
      "944\n",
      "step: 944, train loss: 2.347, val loss 2.385\n",
      "945\n",
      "step: 945, train loss: 2.339, val loss 2.362\n",
      "946\n",
      "step: 946, train loss: 2.358, val loss 2.338\n",
      "947\n",
      "step: 947, train loss: 2.354, val loss 2.366\n",
      "948\n",
      "step: 948, train loss: 2.328, val loss 2.354\n",
      "949\n",
      "step: 949, train loss: 2.369, val loss 2.345\n",
      "950\n",
      "step: 950, train loss: 2.406, val loss 2.353\n",
      "951\n",
      "step: 951, train loss: 2.338, val loss 2.334\n",
      "952\n",
      "step: 952, train loss: 2.342, val loss 2.354\n",
      "953\n",
      "step: 953, train loss: 2.376, val loss 2.340\n",
      "954\n",
      "step: 954, train loss: 2.349, val loss 2.326\n",
      "955\n",
      "step: 955, train loss: 2.388, val loss 2.352\n",
      "956\n",
      "step: 956, train loss: 2.389, val loss 2.360\n",
      "957\n",
      "step: 957, train loss: 2.382, val loss 2.361\n",
      "958\n",
      "step: 958, train loss: 2.361, val loss 2.378\n",
      "959\n",
      "step: 959, train loss: 2.390, val loss 2.345\n",
      "960\n",
      "step: 960, train loss: 2.416, val loss 2.356\n",
      "961\n",
      "step: 961, train loss: 2.355, val loss 2.347\n",
      "962\n",
      "step: 962, train loss: 2.396, val loss 2.371\n",
      "963\n",
      "step: 963, train loss: 2.351, val loss 2.354\n",
      "964\n",
      "step: 964, train loss: 2.357, val loss 2.356\n",
      "965\n",
      "step: 965, train loss: 2.366, val loss 2.384\n",
      "966\n",
      "step: 966, train loss: 2.359, val loss 2.328\n",
      "967\n",
      "step: 967, train loss: 2.388, val loss 2.360\n",
      "968\n",
      "step: 968, train loss: 2.341, val loss 2.331\n",
      "969\n",
      "step: 969, train loss: 2.369, val loss 2.399\n",
      "970\n",
      "step: 970, train loss: 2.334, val loss 2.323\n",
      "971\n",
      "step: 971, train loss: 2.341, val loss 2.349\n",
      "972\n",
      "step: 972, train loss: 2.352, val loss 2.346\n",
      "973\n",
      "step: 973, train loss: 2.329, val loss 2.346\n",
      "974\n",
      "step: 974, train loss: 2.314, val loss 2.347\n",
      "975\n",
      "step: 975, train loss: 2.356, val loss 2.336\n",
      "976\n",
      "step: 976, train loss: 2.340, val loss 2.373\n",
      "977\n",
      "step: 977, train loss: 2.339, val loss 2.336\n",
      "978\n",
      "step: 978, train loss: 2.352, val loss 2.349\n",
      "979\n",
      "step: 979, train loss: 2.346, val loss 2.361\n",
      "980\n",
      "step: 980, train loss: 2.344, val loss 2.340\n",
      "981\n",
      "step: 981, train loss: 2.355, val loss 2.331\n",
      "982\n",
      "step: 982, train loss: 2.325, val loss 2.332\n",
      "983\n",
      "step: 983, train loss: 2.355, val loss 2.350\n",
      "984\n",
      "step: 984, train loss: 2.337, val loss 2.344\n",
      "985\n",
      "step: 985, train loss: 2.344, val loss 2.337\n",
      "986\n",
      "step: 986, train loss: 2.348, val loss 2.351\n",
      "987\n",
      "step: 987, train loss: 2.371, val loss 2.329\n",
      "988\n",
      "step: 988, train loss: 2.345, val loss 2.317\n",
      "989\n",
      "step: 989, train loss: 2.329, val loss 2.348\n",
      "990\n",
      "step: 990, train loss: 2.324, val loss 2.372\n",
      "991\n",
      "step: 991, train loss: 2.333, val loss 2.334\n",
      "992\n",
      "step: 992, train loss: 2.329, val loss 2.337\n",
      "993\n",
      "step: 993, train loss: 2.376, val loss 2.310\n",
      "994\n",
      "step: 994, train loss: 2.334, val loss 2.341\n",
      "995\n",
      "step: 995, train loss: 2.366, val loss 2.337\n",
      "996\n",
      "step: 996, train loss: 2.319, val loss 2.336\n",
      "997\n",
      "step: 997, train loss: 2.326, val loss 2.327\n",
      "998\n",
      "step: 998, train loss: 2.307, val loss 2.327\n",
      "999\n",
      "step: 999, train loss: 2.297, val loss 2.356\n",
      "1000\n",
      "step: 1000, train loss: 2.328, val loss 2.321\n",
      "1001\n",
      "step: 1001, train loss: 2.336, val loss 2.314\n",
      "1002\n",
      "step: 1002, train loss: 2.322, val loss 2.346\n",
      "1003\n",
      "step: 1003, train loss: 2.346, val loss 2.332\n",
      "1004\n",
      "step: 1004, train loss: 2.323, val loss 2.315\n",
      "1005\n",
      "step: 1005, train loss: 2.329, val loss 2.324\n",
      "1006\n",
      "step: 1006, train loss: 2.341, val loss 2.369\n",
      "1007\n",
      "step: 1007, train loss: 2.340, val loss 2.349\n",
      "1008\n",
      "step: 1008, train loss: 2.335, val loss 2.317\n",
      "1009\n",
      "step: 1009, train loss: 2.342, val loss 2.352\n",
      "1010\n",
      "step: 1010, train loss: 2.325, val loss 2.356\n",
      "1011\n",
      "step: 1011, train loss: 2.345, val loss 2.341\n",
      "1012\n",
      "step: 1012, train loss: 2.339, val loss 2.327\n",
      "1013\n",
      "step: 1013, train loss: 2.350, val loss 2.372\n",
      "1014\n",
      "step: 1014, train loss: 2.352, val loss 2.357\n",
      "1015\n",
      "step: 1015, train loss: 2.327, val loss 2.382\n",
      "1016\n",
      "step: 1016, train loss: 2.345, val loss 2.362\n",
      "1017\n",
      "step: 1017, train loss: 2.347, val loss 2.317\n",
      "1018\n",
      "step: 1018, train loss: 2.373, val loss 2.317\n",
      "1019\n",
      "step: 1019, train loss: 2.336, val loss 2.329\n",
      "1020\n",
      "step: 1020, train loss: 2.347, val loss 2.342\n",
      "1021\n",
      "step: 1021, train loss: 2.318, val loss 2.307\n",
      "1022\n",
      "step: 1022, train loss: 2.316, val loss 2.319\n",
      "1023\n",
      "step: 1023, train loss: 2.341, val loss 2.316\n",
      "1024\n",
      "step: 1024, train loss: 2.335, val loss 2.340\n",
      "1025\n",
      "step: 1025, train loss: 2.319, val loss 2.332\n",
      "1026\n",
      "step: 1026, train loss: 2.349, val loss 2.339\n",
      "1027\n",
      "step: 1027, train loss: 2.344, val loss 2.312\n",
      "1028\n",
      "step: 1028, train loss: 2.344, val loss 2.349\n",
      "1029\n",
      "step: 1029, train loss: 2.396, val loss 2.322\n",
      "1030\n",
      "step: 1030, train loss: 2.332, val loss 2.343\n",
      "1031\n",
      "step: 1031, train loss: 2.315, val loss 2.343\n",
      "1032\n",
      "step: 1032, train loss: 2.344, val loss 2.329\n",
      "1033\n",
      "step: 1033, train loss: 2.344, val loss 2.354\n",
      "1034\n",
      "step: 1034, train loss: 2.360, val loss 2.320\n",
      "1035\n",
      "step: 1035, train loss: 2.355, val loss 2.312\n",
      "1036\n",
      "step: 1036, train loss: 2.326, val loss 2.359\n",
      "1037\n",
      "step: 1037, train loss: 2.333, val loss 2.377\n",
      "1038\n",
      "step: 1038, train loss: 2.324, val loss 2.327\n",
      "1039\n",
      "step: 1039, train loss: 2.331, val loss 2.347\n",
      "1040\n",
      "step: 1040, train loss: 2.328, val loss 2.340\n",
      "1041\n",
      "step: 1041, train loss: 2.359, val loss 2.361\n",
      "1042\n",
      "step: 1042, train loss: 2.359, val loss 2.343\n",
      "1043\n",
      "step: 1043, train loss: 2.346, val loss 2.344\n",
      "1044\n",
      "step: 1044, train loss: 2.339, val loss 2.359\n",
      "1045\n",
      "step: 1045, train loss: 2.337, val loss 2.339\n",
      "1046\n",
      "step: 1046, train loss: 2.345, val loss 2.309\n",
      "1047\n",
      "step: 1047, train loss: 2.339, val loss 2.345\n",
      "1048\n",
      "step: 1048, train loss: 2.339, val loss 2.331\n",
      "1049\n",
      "step: 1049, train loss: 2.331, val loss 2.330\n",
      "1050\n",
      "step: 1050, train loss: 2.339, val loss 2.354\n",
      "1051\n",
      "step: 1051, train loss: 2.350, val loss 2.318\n",
      "1052\n",
      "step: 1052, train loss: 2.333, val loss 2.336\n",
      "1053\n",
      "step: 1053, train loss: 2.344, val loss 2.340\n",
      "1054\n",
      "step: 1054, train loss: 2.323, val loss 2.335\n",
      "1055\n",
      "step: 1055, train loss: 2.331, val loss 2.323\n",
      "1056\n",
      "step: 1056, train loss: 2.333, val loss 2.357\n",
      "1057\n",
      "step: 1057, train loss: 2.322, val loss 2.301\n",
      "1058\n",
      "step: 1058, train loss: 2.317, val loss 2.301\n",
      "1059\n",
      "step: 1059, train loss: 2.313, val loss 2.342\n",
      "1060\n",
      "step: 1060, train loss: 2.368, val loss 2.355\n",
      "1061\n",
      "step: 1061, train loss: 2.356, val loss 2.318\n",
      "1062\n",
      "step: 1062, train loss: 2.354, val loss 2.337\n",
      "1063\n",
      "step: 1063, train loss: 2.328, val loss 2.327\n",
      "1064\n",
      "step: 1064, train loss: 2.326, val loss 2.329\n",
      "1065\n",
      "step: 1065, train loss: 2.316, val loss 2.334\n",
      "1066\n",
      "step: 1066, train loss: 2.337, val loss 2.318\n",
      "1067\n",
      "step: 1067, train loss: 2.328, val loss 2.334\n",
      "1068\n",
      "step: 1068, train loss: 2.320, val loss 2.353\n",
      "1069\n",
      "step: 1069, train loss: 2.346, val loss 2.306\n",
      "1070\n",
      "step: 1070, train loss: 2.310, val loss 2.316\n",
      "1071\n",
      "step: 1071, train loss: 2.326, val loss 2.331\n",
      "1072\n",
      "step: 1072, train loss: 2.332, val loss 2.309\n",
      "1073\n",
      "step: 1073, train loss: 2.357, val loss 2.363\n",
      "1074\n",
      "step: 1074, train loss: 2.341, val loss 2.335\n",
      "1075\n",
      "step: 1075, train loss: 2.347, val loss 2.324\n",
      "1076\n",
      "step: 1076, train loss: 2.339, val loss 2.335\n",
      "1077\n",
      "step: 1077, train loss: 2.376, val loss 2.353\n",
      "1078\n",
      "step: 1078, train loss: 2.322, val loss 2.338\n",
      "1079\n",
      "step: 1079, train loss: 2.344, val loss 2.341\n",
      "1080\n",
      "step: 1080, train loss: 2.336, val loss 2.313\n",
      "1081\n",
      "step: 1081, train loss: 2.336, val loss 2.323\n",
      "1082\n",
      "step: 1082, train loss: 2.334, val loss 2.308\n",
      "1083\n",
      "step: 1083, train loss: 2.329, val loss 2.315\n",
      "1084\n",
      "step: 1084, train loss: 2.327, val loss 2.322\n",
      "1085\n",
      "step: 1085, train loss: 2.384, val loss 2.332\n",
      "1086\n",
      "step: 1086, train loss: 2.353, val loss 2.346\n",
      "1087\n",
      "step: 1087, train loss: 2.339, val loss 2.328\n",
      "1088\n",
      "step: 1088, train loss: 2.341, val loss 2.330\n",
      "1089\n",
      "step: 1089, train loss: 2.324, val loss 2.339\n",
      "1090\n",
      "step: 1090, train loss: 2.329, val loss 2.329\n",
      "1091\n",
      "step: 1091, train loss: 2.329, val loss 2.313\n",
      "1092\n",
      "step: 1092, train loss: 2.318, val loss 2.339\n",
      "1093\n",
      "step: 1093, train loss: 2.328, val loss 2.335\n",
      "1094\n",
      "step: 1094, train loss: 2.325, val loss 2.333\n",
      "1095\n",
      "step: 1095, train loss: 2.331, val loss 2.326\n",
      "1096\n",
      "step: 1096, train loss: 2.322, val loss 2.354\n",
      "1097\n",
      "step: 1097, train loss: 2.355, val loss 2.316\n",
      "1098\n",
      "step: 1098, train loss: 2.306, val loss 2.325\n",
      "1099\n",
      "step: 1099, train loss: 2.327, val loss 2.334\n",
      "1100\n",
      "step: 1100, train loss: 2.338, val loss 2.354\n",
      "1101\n",
      "step: 1101, train loss: 2.317, val loss 2.332\n",
      "1102\n",
      "step: 1102, train loss: 2.303, val loss 2.344\n",
      "1103\n",
      "step: 1103, train loss: 2.324, val loss 2.344\n",
      "1104\n",
      "step: 1104, train loss: 2.342, val loss 2.287\n",
      "1105\n",
      "step: 1105, train loss: 2.346, val loss 2.328\n",
      "1106\n",
      "step: 1106, train loss: 2.335, val loss 2.322\n",
      "1107\n",
      "step: 1107, train loss: 2.328, val loss 2.324\n",
      "1108\n",
      "step: 1108, train loss: 2.309, val loss 2.333\n",
      "1109\n",
      "step: 1109, train loss: 2.302, val loss 2.338\n",
      "1110\n",
      "step: 1110, train loss: 2.355, val loss 2.317\n",
      "1111\n",
      "step: 1111, train loss: 2.322, val loss 2.333\n",
      "1112\n",
      "step: 1112, train loss: 2.326, val loss 2.322\n",
      "1113\n",
      "step: 1113, train loss: 2.318, val loss 2.303\n",
      "1114\n",
      "step: 1114, train loss: 2.301, val loss 2.339\n",
      "1115\n",
      "step: 1115, train loss: 2.314, val loss 2.323\n",
      "1116\n",
      "step: 1116, train loss: 2.352, val loss 2.316\n",
      "1117\n",
      "step: 1117, train loss: 2.352, val loss 2.324\n",
      "1118\n",
      "step: 1118, train loss: 2.307, val loss 2.318\n",
      "1119\n",
      "step: 1119, train loss: 2.303, val loss 2.296\n",
      "1120\n",
      "step: 1120, train loss: 2.304, val loss 2.312\n",
      "1121\n",
      "step: 1121, train loss: 2.298, val loss 2.293\n",
      "1122\n",
      "step: 1122, train loss: 2.310, val loss 2.316\n",
      "1123\n",
      "step: 1123, train loss: 2.324, val loss 2.296\n",
      "1124\n",
      "step: 1124, train loss: 2.306, val loss 2.288\n",
      "1125\n",
      "step: 1125, train loss: 2.302, val loss 2.311\n",
      "1126\n",
      "step: 1126, train loss: 2.311, val loss 2.308\n",
      "1127\n",
      "step: 1127, train loss: 2.314, val loss 2.318\n",
      "1128\n",
      "step: 1128, train loss: 2.308, val loss 2.324\n",
      "1129\n",
      "step: 1129, train loss: 2.353, val loss 2.320\n",
      "1130\n",
      "step: 1130, train loss: 2.318, val loss 2.317\n",
      "1131\n",
      "step: 1131, train loss: 2.315, val loss 2.318\n",
      "1132\n",
      "step: 1132, train loss: 2.296, val loss 2.317\n",
      "1133\n",
      "step: 1133, train loss: 2.331, val loss 2.327\n",
      "1134\n",
      "step: 1134, train loss: 2.347, val loss 2.299\n",
      "1135\n",
      "step: 1135, train loss: 2.322, val loss 2.332\n",
      "1136\n",
      "step: 1136, train loss: 2.335, val loss 2.290\n",
      "1137\n",
      "step: 1137, train loss: 2.318, val loss 2.316\n",
      "1138\n",
      "step: 1138, train loss: 2.326, val loss 2.314\n",
      "1139\n",
      "step: 1139, train loss: 2.321, val loss 2.336\n",
      "1140\n",
      "step: 1140, train loss: 2.333, val loss 2.309\n",
      "1141\n",
      "step: 1141, train loss: 2.337, val loss 2.301\n",
      "1142\n",
      "step: 1142, train loss: 2.311, val loss 2.302\n",
      "1143\n",
      "step: 1143, train loss: 2.316, val loss 2.324\n",
      "1144\n",
      "step: 1144, train loss: 2.320, val loss 2.320\n",
      "1145\n",
      "step: 1145, train loss: 2.321, val loss 2.291\n",
      "1146\n",
      "step: 1146, train loss: 2.311, val loss 2.291\n",
      "1147\n",
      "step: 1147, train loss: 2.298, val loss 2.314\n",
      "1148\n",
      "step: 1148, train loss: 2.294, val loss 2.318\n",
      "1149\n",
      "step: 1149, train loss: 2.292, val loss 2.311\n",
      "1150\n",
      "step: 1150, train loss: 2.312, val loss 2.299\n",
      "1151\n",
      "step: 1151, train loss: 2.320, val loss 2.332\n",
      "1152\n",
      "step: 1152, train loss: 2.321, val loss 2.316\n",
      "1153\n",
      "step: 1153, train loss: 2.309, val loss 2.351\n",
      "1154\n",
      "step: 1154, train loss: 2.309, val loss 2.328\n",
      "1155\n",
      "step: 1155, train loss: 2.328, val loss 2.324\n",
      "1156\n",
      "step: 1156, train loss: 2.343, val loss 2.289\n",
      "1157\n",
      "step: 1157, train loss: 2.294, val loss 2.309\n",
      "1158\n",
      "step: 1158, train loss: 2.329, val loss 2.308\n",
      "1159\n",
      "step: 1159, train loss: 2.302, val loss 2.321\n",
      "1160\n",
      "step: 1160, train loss: 2.331, val loss 2.329\n",
      "1161\n",
      "step: 1161, train loss: 2.310, val loss 2.310\n",
      "1162\n",
      "step: 1162, train loss: 2.335, val loss 2.309\n",
      "1163\n",
      "step: 1163, train loss: 2.303, val loss 2.297\n",
      "1164\n",
      "step: 1164, train loss: 2.318, val loss 2.295\n",
      "1165\n",
      "step: 1165, train loss: 2.327, val loss 2.294\n",
      "1166\n",
      "step: 1166, train loss: 2.317, val loss 2.297\n",
      "1167\n",
      "step: 1167, train loss: 2.338, val loss 2.314\n",
      "1168\n",
      "step: 1168, train loss: 2.327, val loss 2.308\n",
      "1169\n",
      "step: 1169, train loss: 2.298, val loss 2.295\n",
      "1170\n",
      "step: 1170, train loss: 2.300, val loss 2.297\n",
      "1171\n",
      "step: 1171, train loss: 2.310, val loss 2.313\n",
      "1172\n",
      "step: 1172, train loss: 2.312, val loss 2.311\n",
      "1173\n",
      "step: 1173, train loss: 2.316, val loss 2.327\n",
      "1174\n",
      "step: 1174, train loss: 2.318, val loss 2.319\n",
      "1175\n",
      "step: 1175, train loss: 2.330, val loss 2.355\n",
      "1176\n",
      "step: 1176, train loss: 2.335, val loss 2.326\n",
      "1177\n",
      "step: 1177, train loss: 2.342, val loss 2.317\n",
      "1178\n",
      "step: 1178, train loss: 2.295, val loss 2.327\n",
      "1179\n",
      "step: 1179, train loss: 2.295, val loss 2.305\n",
      "1180\n",
      "step: 1180, train loss: 2.308, val loss 2.306\n",
      "1181\n",
      "step: 1181, train loss: 2.330, val loss 2.315\n",
      "1182\n",
      "step: 1182, train loss: 2.324, val loss 2.334\n",
      "1183\n",
      "step: 1183, train loss: 2.335, val loss 2.331\n",
      "1184\n",
      "step: 1184, train loss: 2.324, val loss 2.291\n",
      "1185\n",
      "step: 1185, train loss: 2.315, val loss 2.300\n",
      "1186\n",
      "step: 1186, train loss: 2.272, val loss 2.309\n",
      "1187\n",
      "step: 1187, train loss: 2.304, val loss 2.289\n",
      "1188\n",
      "step: 1188, train loss: 2.317, val loss 2.312\n",
      "1189\n",
      "step: 1189, train loss: 2.298, val loss 2.304\n",
      "1190\n",
      "step: 1190, train loss: 2.289, val loss 2.294\n",
      "1191\n",
      "step: 1191, train loss: 2.320, val loss 2.299\n",
      "1192\n",
      "step: 1192, train loss: 2.291, val loss 2.291\n",
      "1193\n",
      "step: 1193, train loss: 2.300, val loss 2.313\n",
      "1194\n",
      "step: 1194, train loss: 2.303, val loss 2.289\n",
      "1195\n",
      "step: 1195, train loss: 2.275, val loss 2.330\n",
      "1196\n",
      "step: 1196, train loss: 2.312, val loss 2.302\n",
      "1197\n",
      "step: 1197, train loss: 2.306, val loss 2.323\n",
      "1198\n",
      "step: 1198, train loss: 2.279, val loss 2.322\n",
      "1199\n",
      "step: 1199, train loss: 2.287, val loss 2.286\n",
      "1200\n",
      "step: 1200, train loss: 2.323, val loss 2.308\n",
      "1201\n",
      "step: 1201, train loss: 2.307, val loss 2.305\n",
      "1202\n",
      "step: 1202, train loss: 2.302, val loss 2.300\n",
      "1203\n",
      "step: 1203, train loss: 2.303, val loss 2.289\n",
      "1204\n",
      "step: 1204, train loss: 2.346, val loss 2.307\n",
      "1205\n",
      "step: 1205, train loss: 2.332, val loss 2.325\n",
      "1206\n",
      "step: 1206, train loss: 2.324, val loss 2.344\n",
      "1207\n",
      "step: 1207, train loss: 2.309, val loss 2.317\n",
      "1208\n",
      "step: 1208, train loss: 2.293, val loss 2.293\n",
      "1209\n",
      "step: 1209, train loss: 2.321, val loss 2.295\n",
      "1210\n",
      "step: 1210, train loss: 2.301, val loss 2.284\n",
      "1211\n",
      "step: 1211, train loss: 2.309, val loss 2.295\n",
      "1212\n",
      "step: 1212, train loss: 2.333, val loss 2.323\n",
      "1213\n",
      "step: 1213, train loss: 2.296, val loss 2.297\n",
      "1214\n",
      "step: 1214, train loss: 2.302, val loss 2.301\n",
      "1215\n",
      "step: 1215, train loss: 2.308, val loss 2.301\n",
      "1216\n",
      "step: 1216, train loss: 2.291, val loss 2.285\n",
      "1217\n",
      "step: 1217, train loss: 2.314, val loss 2.304\n",
      "1218\n",
      "step: 1218, train loss: 2.292, val loss 2.277\n",
      "1219\n",
      "step: 1219, train loss: 2.290, val loss 2.300\n",
      "1220\n",
      "step: 1220, train loss: 2.309, val loss 2.274\n",
      "1221\n",
      "step: 1221, train loss: 2.285, val loss 2.292\n",
      "1222\n",
      "step: 1222, train loss: 2.329, val loss 2.309\n",
      "1223\n",
      "step: 1223, train loss: 2.301, val loss 2.316\n",
      "1224\n",
      "step: 1224, train loss: 2.307, val loss 2.306\n",
      "1225\n",
      "step: 1225, train loss: 2.273, val loss 2.275\n",
      "1226\n",
      "step: 1226, train loss: 2.295, val loss 2.314\n",
      "1227\n",
      "step: 1227, train loss: 2.311, val loss 2.319\n",
      "1228\n",
      "step: 1228, train loss: 2.290, val loss 2.294\n",
      "1229\n",
      "step: 1229, train loss: 2.324, val loss 2.316\n",
      "1230\n",
      "step: 1230, train loss: 2.290, val loss 2.315\n",
      "1231\n",
      "step: 1231, train loss: 2.323, val loss 2.325\n",
      "1232\n",
      "step: 1232, train loss: 2.312, val loss 2.288\n",
      "1233\n",
      "step: 1233, train loss: 2.319, val loss 2.291\n",
      "1234\n",
      "step: 1234, train loss: 2.295, val loss 2.312\n",
      "1235\n",
      "step: 1235, train loss: 2.284, val loss 2.327\n",
      "1236\n",
      "step: 1236, train loss: 2.315, val loss 2.292\n",
      "1237\n",
      "step: 1237, train loss: 2.299, val loss 2.309\n",
      "1238\n",
      "step: 1238, train loss: 2.300, val loss 2.334\n",
      "1239\n",
      "step: 1239, train loss: 2.305, val loss 2.302\n",
      "1240\n",
      "step: 1240, train loss: 2.294, val loss 2.292\n",
      "1241\n",
      "step: 1241, train loss: 2.298, val loss 2.298\n",
      "1242\n",
      "step: 1242, train loss: 2.310, val loss 2.308\n",
      "1243\n",
      "step: 1243, train loss: 2.292, val loss 2.293\n",
      "1244\n",
      "step: 1244, train loss: 2.292, val loss 2.298\n",
      "1245\n",
      "step: 1245, train loss: 2.293, val loss 2.282\n",
      "1246\n",
      "step: 1246, train loss: 2.305, val loss 2.289\n",
      "1247\n",
      "step: 1247, train loss: 2.297, val loss 2.290\n",
      "1248\n",
      "step: 1248, train loss: 2.306, val loss 2.344\n",
      "1249\n",
      "step: 1249, train loss: 2.288, val loss 2.308\n",
      "1250\n",
      "step: 1250, train loss: 2.296, val loss 2.296\n",
      "1251\n",
      "step: 1251, train loss: 2.290, val loss 2.328\n",
      "1252\n",
      "step: 1252, train loss: 2.278, val loss 2.293\n",
      "1253\n",
      "step: 1253, train loss: 2.297, val loss 2.296\n",
      "1254\n",
      "step: 1254, train loss: 2.327, val loss 2.311\n",
      "1255\n",
      "step: 1255, train loss: 2.282, val loss 2.297\n",
      "1256\n",
      "step: 1256, train loss: 2.281, val loss 2.276\n",
      "1257\n",
      "step: 1257, train loss: 2.305, val loss 2.269\n",
      "1258\n",
      "step: 1258, train loss: 2.298, val loss 2.278\n",
      "1259\n",
      "step: 1259, train loss: 2.298, val loss 2.306\n",
      "1260\n",
      "step: 1260, train loss: 2.267, val loss 2.318\n",
      "1261\n",
      "step: 1261, train loss: 2.302, val loss 2.271\n",
      "1262\n",
      "step: 1262, train loss: 2.279, val loss 2.284\n",
      "1263\n",
      "step: 1263, train loss: 2.269, val loss 2.314\n",
      "1264\n",
      "step: 1264, train loss: 2.286, val loss 2.282\n",
      "1265\n",
      "step: 1265, train loss: 2.311, val loss 2.259\n",
      "1266\n",
      "step: 1266, train loss: 2.311, val loss 2.280\n",
      "1267\n",
      "step: 1267, train loss: 2.265, val loss 2.289\n",
      "1268\n",
      "step: 1268, train loss: 2.310, val loss 2.303\n",
      "1269\n",
      "step: 1269, train loss: 2.308, val loss 2.252\n",
      "1270\n",
      "step: 1270, train loss: 2.293, val loss 2.292\n",
      "1271\n",
      "step: 1271, train loss: 2.263, val loss 2.303\n",
      "1272\n",
      "step: 1272, train loss: 2.266, val loss 2.289\n",
      "1273\n",
      "step: 1273, train loss: 2.267, val loss 2.257\n",
      "1274\n",
      "step: 1274, train loss: 2.266, val loss 2.293\n",
      "1275\n",
      "step: 1275, train loss: 2.285, val loss 2.274\n",
      "1276\n",
      "step: 1276, train loss: 2.266, val loss 2.297\n",
      "1277\n",
      "step: 1277, train loss: 2.291, val loss 2.296\n",
      "1278\n",
      "step: 1278, train loss: 2.325, val loss 2.302\n",
      "1279\n",
      "step: 1279, train loss: 2.272, val loss 2.293\n",
      "1280\n",
      "step: 1280, train loss: 2.308, val loss 2.319\n",
      "1281\n",
      "step: 1281, train loss: 2.286, val loss 2.285\n",
      "1282\n",
      "step: 1282, train loss: 2.292, val loss 2.286\n",
      "1283\n",
      "step: 1283, train loss: 2.293, val loss 2.308\n",
      "1284\n",
      "step: 1284, train loss: 2.288, val loss 2.275\n",
      "1285\n",
      "step: 1285, train loss: 2.277, val loss 2.306\n",
      "1286\n",
      "step: 1286, train loss: 2.309, val loss 2.314\n",
      "1287\n",
      "step: 1287, train loss: 2.311, val loss 2.287\n",
      "1288\n",
      "step: 1288, train loss: 2.298, val loss 2.312\n",
      "1289\n",
      "step: 1289, train loss: 2.288, val loss 2.322\n",
      "1290\n",
      "step: 1290, train loss: 2.306, val loss 2.307\n",
      "1291\n",
      "step: 1291, train loss: 2.281, val loss 2.294\n",
      "1292\n",
      "step: 1292, train loss: 2.296, val loss 2.288\n",
      "1293\n",
      "step: 1293, train loss: 2.286, val loss 2.258\n",
      "1294\n",
      "step: 1294, train loss: 2.293, val loss 2.265\n",
      "1295\n",
      "step: 1295, train loss: 2.274, val loss 2.293\n",
      "1296\n",
      "step: 1296, train loss: 2.309, val loss 2.289\n",
      "1297\n",
      "step: 1297, train loss: 2.285, val loss 2.290\n",
      "1298\n",
      "step: 1298, train loss: 2.323, val loss 2.309\n",
      "1299\n",
      "step: 1299, train loss: 2.246, val loss 2.285\n",
      "1300\n",
      "step: 1300, train loss: 2.274, val loss 2.276\n",
      "1301\n",
      "step: 1301, train loss: 2.267, val loss 2.283\n",
      "1302\n",
      "step: 1302, train loss: 2.285, val loss 2.281\n",
      "1303\n",
      "step: 1303, train loss: 2.267, val loss 2.251\n",
      "1304\n",
      "step: 1304, train loss: 2.285, val loss 2.277\n",
      "1305\n",
      "step: 1305, train loss: 2.268, val loss 2.303\n",
      "1306\n",
      "step: 1306, train loss: 2.285, val loss 2.255\n",
      "1307\n",
      "step: 1307, train loss: 2.309, val loss 2.238\n",
      "1308\n",
      "step: 1308, train loss: 2.262, val loss 2.279\n",
      "1309\n",
      "step: 1309, train loss: 2.271, val loss 2.250\n",
      "1310\n",
      "step: 1310, train loss: 2.291, val loss 2.270\n",
      "1311\n",
      "step: 1311, train loss: 2.269, val loss 2.265\n",
      "1312\n",
      "step: 1312, train loss: 2.289, val loss 2.267\n",
      "1313\n",
      "step: 1313, train loss: 2.291, val loss 2.254\n",
      "1314\n",
      "step: 1314, train loss: 2.297, val loss 2.295\n",
      "1315\n",
      "step: 1315, train loss: 2.255, val loss 2.256\n",
      "1316\n",
      "step: 1316, train loss: 2.316, val loss 2.291\n",
      "1317\n",
      "step: 1317, train loss: 2.269, val loss 2.291\n",
      "1318\n",
      "step: 1318, train loss: 2.273, val loss 2.321\n",
      "1319\n",
      "step: 1319, train loss: 2.283, val loss 2.306\n",
      "1320\n",
      "step: 1320, train loss: 2.283, val loss 2.268\n",
      "1321\n",
      "step: 1321, train loss: 2.288, val loss 2.272\n",
      "1322\n",
      "step: 1322, train loss: 2.268, val loss 2.279\n",
      "1323\n",
      "step: 1323, train loss: 2.301, val loss 2.290\n",
      "1324\n",
      "step: 1324, train loss: 2.278, val loss 2.308\n",
      "1325\n",
      "step: 1325, train loss: 2.293, val loss 2.258\n",
      "1326\n",
      "step: 1326, train loss: 2.280, val loss 2.266\n",
      "1327\n",
      "step: 1327, train loss: 2.273, val loss 2.278\n",
      "1328\n",
      "step: 1328, train loss: 2.276, val loss 2.277\n",
      "1329\n",
      "step: 1329, train loss: 2.289, val loss 2.289\n",
      "1330\n",
      "step: 1330, train loss: 2.264, val loss 2.283\n",
      "1331\n",
      "step: 1331, train loss: 2.290, val loss 2.279\n",
      "1332\n",
      "step: 1332, train loss: 2.264, val loss 2.269\n",
      "1333\n",
      "step: 1333, train loss: 2.261, val loss 2.292\n",
      "1334\n",
      "step: 1334, train loss: 2.269, val loss 2.273\n",
      "1335\n",
      "step: 1335, train loss: 2.274, val loss 2.284\n",
      "1336\n",
      "step: 1336, train loss: 2.277, val loss 2.269\n",
      "1337\n",
      "step: 1337, train loss: 2.292, val loss 2.266\n",
      "1338\n",
      "step: 1338, train loss: 2.268, val loss 2.295\n",
      "1339\n",
      "step: 1339, train loss: 2.261, val loss 2.289\n",
      "1340\n",
      "step: 1340, train loss: 2.326, val loss 2.304\n",
      "1341\n",
      "step: 1341, train loss: 2.270, val loss 2.260\n",
      "1342\n",
      "step: 1342, train loss: 2.287, val loss 2.292\n",
      "1343\n",
      "step: 1343, train loss: 2.286, val loss 2.294\n",
      "1344\n",
      "step: 1344, train loss: 2.282, val loss 2.263\n",
      "1345\n",
      "step: 1345, train loss: 2.277, val loss 2.301\n",
      "1346\n",
      "step: 1346, train loss: 2.247, val loss 2.266\n",
      "1347\n",
      "step: 1347, train loss: 2.282, val loss 2.285\n",
      "1348\n",
      "step: 1348, train loss: 2.240, val loss 2.250\n",
      "1349\n",
      "step: 1349, train loss: 2.272, val loss 2.237\n",
      "1350\n",
      "step: 1350, train loss: 2.250, val loss 2.254\n",
      "1351\n",
      "step: 1351, train loss: 2.260, val loss 2.254\n",
      "1352\n",
      "step: 1352, train loss: 2.245, val loss 2.285\n",
      "1353\n",
      "step: 1353, train loss: 2.247, val loss 2.236\n",
      "1354\n",
      "step: 1354, train loss: 2.266, val loss 2.260\n",
      "1355\n",
      "step: 1355, train loss: 2.284, val loss 2.289\n",
      "1356\n",
      "step: 1356, train loss: 2.255, val loss 2.287\n",
      "1357\n",
      "step: 1357, train loss: 2.286, val loss 2.281\n",
      "1358\n",
      "step: 1358, train loss: 2.278, val loss 2.326\n",
      "1359\n",
      "step: 1359, train loss: 2.308, val loss 2.309\n",
      "1360\n",
      "step: 1360, train loss: 2.291, val loss 2.258\n",
      "1361\n",
      "step: 1361, train loss: 2.280, val loss 2.262\n",
      "1362\n",
      "step: 1362, train loss: 2.281, val loss 2.303\n",
      "1363\n",
      "step: 1363, train loss: 2.254, val loss 2.272\n",
      "1364\n",
      "step: 1364, train loss: 2.243, val loss 2.248\n",
      "1365\n",
      "step: 1365, train loss: 2.240, val loss 2.261\n",
      "1366\n",
      "step: 1366, train loss: 2.237, val loss 2.248\n",
      "1367\n",
      "step: 1367, train loss: 2.246, val loss 2.246\n",
      "1368\n",
      "step: 1368, train loss: 2.236, val loss 2.235\n",
      "1369\n",
      "step: 1369, train loss: 2.280, val loss 2.245\n",
      "1370\n",
      "step: 1370, train loss: 2.236, val loss 2.230\n",
      "1371\n",
      "step: 1371, train loss: 2.248, val loss 2.225\n",
      "1372\n",
      "step: 1372, train loss: 2.237, val loss 2.273\n",
      "1373\n",
      "step: 1373, train loss: 2.223, val loss 2.252\n",
      "1374\n",
      "step: 1374, train loss: 2.227, val loss 2.228\n",
      "1375\n",
      "step: 1375, train loss: 2.273, val loss 2.231\n",
      "1376\n",
      "step: 1376, train loss: 2.229, val loss 2.239\n",
      "1377\n",
      "step: 1377, train loss: 2.244, val loss 2.270\n",
      "1378\n",
      "step: 1378, train loss: 2.274, val loss 2.260\n",
      "1379\n",
      "step: 1379, train loss: 2.266, val loss 2.254\n",
      "1380\n",
      "step: 1380, train loss: 2.237, val loss 2.237\n",
      "1381\n",
      "step: 1381, train loss: 2.269, val loss 2.247\n",
      "1382\n",
      "step: 1382, train loss: 2.222, val loss 2.243\n",
      "1383\n",
      "step: 1383, train loss: 2.234, val loss 2.236\n",
      "1384\n",
      "step: 1384, train loss: 2.260, val loss 2.259\n",
      "1385\n",
      "step: 1385, train loss: 2.270, val loss 2.259\n",
      "1386\n",
      "step: 1386, train loss: 2.247, val loss 2.275\n",
      "1387\n",
      "step: 1387, train loss: 2.249, val loss 2.267\n",
      "1388\n",
      "step: 1388, train loss: 2.256, val loss 2.236\n",
      "1389\n",
      "step: 1389, train loss: 2.229, val loss 2.288\n",
      "1390\n",
      "step: 1390, train loss: 2.243, val loss 2.314\n",
      "1391\n",
      "step: 1391, train loss: 2.262, val loss 2.274\n",
      "1392\n",
      "step: 1392, train loss: 2.247, val loss 2.292\n",
      "1393\n",
      "step: 1393, train loss: 2.269, val loss 2.253\n",
      "1394\n",
      "step: 1394, train loss: 2.266, val loss 2.266\n",
      "1395\n",
      "step: 1395, train loss: 2.277, val loss 2.233\n",
      "1396\n",
      "step: 1396, train loss: 2.224, val loss 2.251\n",
      "1397\n",
      "step: 1397, train loss: 2.246, val loss 2.244\n",
      "1398\n",
      "step: 1398, train loss: 2.264, val loss 2.264\n",
      "1399\n",
      "step: 1399, train loss: 2.273, val loss 2.287\n",
      "1400\n",
      "step: 1400, train loss: 2.291, val loss 2.265\n",
      "1401\n",
      "step: 1401, train loss: 2.254, val loss 2.264\n",
      "1402\n",
      "step: 1402, train loss: 2.241, val loss 2.262\n",
      "1403\n",
      "step: 1403, train loss: 2.260, val loss 2.268\n",
      "1404\n",
      "step: 1404, train loss: 2.284, val loss 2.234\n",
      "1405\n",
      "step: 1405, train loss: 2.241, val loss 2.247\n",
      "1406\n",
      "step: 1406, train loss: 2.246, val loss 2.261\n",
      "1407\n",
      "step: 1407, train loss: 2.242, val loss 2.265\n",
      "1408\n",
      "step: 1408, train loss: 2.281, val loss 2.270\n",
      "1409\n",
      "step: 1409, train loss: 2.260, val loss 2.279\n",
      "1410\n",
      "step: 1410, train loss: 2.278, val loss 2.278\n",
      "1411\n",
      "step: 1411, train loss: 2.278, val loss 2.254\n",
      "1412\n",
      "step: 1412, train loss: 2.300, val loss 2.314\n",
      "1413\n",
      "step: 1413, train loss: 2.285, val loss 2.300\n",
      "1414\n",
      "step: 1414, train loss: 2.297, val loss 2.276\n",
      "1415\n",
      "step: 1415, train loss: 2.271, val loss 2.296\n",
      "1416\n",
      "step: 1416, train loss: 2.259, val loss 2.296\n",
      "1417\n",
      "step: 1417, train loss: 2.270, val loss 2.264\n",
      "1418\n",
      "step: 1418, train loss: 2.285, val loss 2.268\n",
      "1419\n",
      "step: 1419, train loss: 2.267, val loss 2.315\n",
      "1420\n",
      "step: 1420, train loss: 2.290, val loss 2.295\n",
      "1421\n",
      "step: 1421, train loss: 2.276, val loss 2.285\n",
      "1422\n",
      "step: 1422, train loss: 2.254, val loss 2.270\n",
      "1423\n",
      "step: 1423, train loss: 2.247, val loss 2.258\n",
      "1424\n",
      "step: 1424, train loss: 2.260, val loss 2.239\n",
      "1425\n",
      "step: 1425, train loss: 2.255, val loss 2.263\n",
      "1426\n",
      "step: 1426, train loss: 2.287, val loss 2.269\n",
      "1427\n",
      "step: 1427, train loss: 2.264, val loss 2.256\n",
      "1428\n",
      "step: 1428, train loss: 2.254, val loss 2.256\n",
      "1429\n",
      "step: 1429, train loss: 2.254, val loss 2.275\n",
      "1430\n",
      "step: 1430, train loss: 2.270, val loss 2.256\n",
      "1431\n",
      "step: 1431, train loss: 2.227, val loss 2.238\n",
      "1432\n",
      "step: 1432, train loss: 2.292, val loss 2.235\n",
      "1433\n",
      "step: 1433, train loss: 2.264, val loss 2.243\n",
      "1434\n",
      "step: 1434, train loss: 2.244, val loss 2.243\n",
      "1435\n",
      "step: 1435, train loss: 2.220, val loss 2.233\n",
      "1436\n",
      "step: 1436, train loss: 2.208, val loss 2.220\n",
      "1437\n",
      "step: 1437, train loss: 2.240, val loss 2.215\n",
      "1438\n",
      "step: 1438, train loss: 2.233, val loss 2.223\n",
      "1439\n",
      "step: 1439, train loss: 2.236, val loss 2.216\n",
      "1440\n",
      "step: 1440, train loss: 2.224, val loss 2.250\n",
      "1441\n",
      "step: 1441, train loss: 2.227, val loss 2.196\n",
      "1442\n",
      "step: 1442, train loss: 2.238, val loss 2.236\n",
      "1443\n",
      "step: 1443, train loss: 2.254, val loss 2.233\n",
      "1444\n",
      "step: 1444, train loss: 2.238, val loss 2.236\n",
      "1445\n",
      "step: 1445, train loss: 2.218, val loss 2.218\n",
      "1446\n",
      "step: 1446, train loss: 2.240, val loss 2.211\n",
      "1447\n",
      "step: 1447, train loss: 2.230, val loss 2.212\n",
      "1448\n",
      "step: 1448, train loss: 2.224, val loss 2.232\n",
      "1449\n",
      "step: 1449, train loss: 2.198, val loss 2.177\n",
      "1450\n",
      "step: 1450, train loss: 2.211, val loss 2.210\n",
      "1451\n",
      "step: 1451, train loss: 2.221, val loss 2.231\n",
      "1452\n",
      "step: 1452, train loss: 2.218, val loss 2.225\n",
      "1453\n",
      "step: 1453, train loss: 2.218, val loss 2.220\n",
      "1454\n",
      "step: 1454, train loss: 2.189, val loss 2.196\n",
      "1455\n",
      "step: 1455, train loss: 2.230, val loss 2.194\n",
      "1456\n",
      "step: 1456, train loss: 2.216, val loss 2.241\n",
      "1457\n",
      "step: 1457, train loss: 2.232, val loss 2.232\n",
      "1458\n",
      "step: 1458, train loss: 2.205, val loss 2.235\n",
      "1459\n",
      "step: 1459, train loss: 2.194, val loss 2.202\n",
      "1460\n",
      "step: 1460, train loss: 2.219, val loss 2.226\n",
      "1461\n",
      "step: 1461, train loss: 2.210, val loss 2.209\n",
      "1462\n",
      "step: 1462, train loss: 2.237, val loss 2.217\n",
      "1463\n",
      "step: 1463, train loss: 2.194, val loss 2.203\n",
      "1464\n",
      "step: 1464, train loss: 2.208, val loss 2.189\n",
      "1465\n",
      "step: 1465, train loss: 2.219, val loss 2.192\n",
      "1466\n",
      "step: 1466, train loss: 2.212, val loss 2.197\n",
      "1467\n",
      "step: 1467, train loss: 2.215, val loss 2.209\n",
      "1468\n",
      "step: 1468, train loss: 2.203, val loss 2.220\n",
      "1469\n",
      "step: 1469, train loss: 2.212, val loss 2.193\n",
      "1470\n",
      "step: 1470, train loss: 2.223, val loss 2.199\n",
      "1471\n",
      "step: 1471, train loss: 2.180, val loss 2.198\n",
      "1472\n",
      "step: 1472, train loss: 2.209, val loss 2.216\n",
      "1473\n",
      "step: 1473, train loss: 2.186, val loss 2.188\n",
      "1474\n",
      "step: 1474, train loss: 2.161, val loss 2.208\n",
      "1475\n",
      "step: 1475, train loss: 2.182, val loss 2.226\n",
      "1476\n",
      "step: 1476, train loss: 2.197, val loss 2.199\n",
      "1477\n",
      "step: 1477, train loss: 2.203, val loss 2.199\n",
      "1478\n",
      "step: 1478, train loss: 2.184, val loss 2.225\n",
      "1479\n",
      "step: 1479, train loss: 2.208, val loss 2.174\n",
      "1480\n",
      "step: 1480, train loss: 2.189, val loss 2.204\n",
      "1481\n",
      "step: 1481, train loss: 2.220, val loss 2.209\n",
      "1482\n",
      "step: 1482, train loss: 2.211, val loss 2.187\n",
      "1483\n",
      "step: 1483, train loss: 2.200, val loss 2.216\n",
      "1484\n",
      "step: 1484, train loss: 2.203, val loss 2.187\n",
      "1485\n",
      "step: 1485, train loss: 2.182, val loss 2.209\n",
      "1486\n",
      "step: 1486, train loss: 2.187, val loss 2.188\n",
      "1487\n",
      "step: 1487, train loss: 2.186, val loss 2.185\n",
      "1488\n",
      "step: 1488, train loss: 2.195, val loss 2.182\n",
      "1489\n",
      "step: 1489, train loss: 2.203, val loss 2.217\n",
      "1490\n",
      "step: 1490, train loss: 2.230, val loss 2.170\n",
      "1491\n",
      "step: 1491, train loss: 2.166, val loss 2.208\n",
      "1492\n",
      "step: 1492, train loss: 2.224, val loss 2.186\n",
      "1493\n",
      "step: 1493, train loss: 2.185, val loss 2.198\n",
      "1494\n",
      "step: 1494, train loss: 2.171, val loss 2.198\n",
      "1495\n",
      "step: 1495, train loss: 2.206, val loss 2.197\n",
      "1496\n",
      "step: 1496, train loss: 2.188, val loss 2.195\n",
      "1497\n",
      "step: 1497, train loss: 2.190, val loss 2.198\n",
      "1498\n",
      "step: 1498, train loss: 2.182, val loss 2.199\n",
      "1499\n",
      "step: 1499, train loss: 2.192, val loss 2.185\n",
      "1500\n",
      "step: 1500, train loss: 2.179, val loss 2.168\n",
      "1501\n",
      "step: 1501, train loss: 2.200, val loss 2.187\n",
      "1502\n",
      "step: 1502, train loss: 2.200, val loss 2.216\n",
      "1503\n",
      "step: 1503, train loss: 2.185, val loss 2.159\n",
      "1504\n",
      "step: 1504, train loss: 2.202, val loss 2.178\n",
      "1505\n",
      "step: 1505, train loss: 2.184, val loss 2.207\n",
      "1506\n",
      "step: 1506, train loss: 2.212, val loss 2.198\n",
      "1507\n",
      "step: 1507, train loss: 2.189, val loss 2.212\n",
      "1508\n",
      "step: 1508, train loss: 2.165, val loss 2.197\n",
      "1509\n",
      "step: 1509, train loss: 2.169, val loss 2.195\n",
      "1510\n",
      "step: 1510, train loss: 2.199, val loss 2.206\n",
      "1511\n",
      "step: 1511, train loss: 2.189, val loss 2.178\n",
      "1512\n",
      "step: 1512, train loss: 2.175, val loss 2.184\n",
      "1513\n",
      "step: 1513, train loss: 2.168, val loss 2.185\n",
      "1514\n",
      "step: 1514, train loss: 2.167, val loss 2.223\n",
      "1515\n",
      "step: 1515, train loss: 2.157, val loss 2.178\n",
      "1516\n",
      "step: 1516, train loss: 2.156, val loss 2.170\n",
      "1517\n",
      "step: 1517, train loss: 2.207, val loss 2.171\n",
      "1518\n",
      "step: 1518, train loss: 2.189, val loss 2.188\n",
      "1519\n",
      "step: 1519, train loss: 2.186, val loss 2.193\n",
      "1520\n",
      "step: 1520, train loss: 2.162, val loss 2.168\n",
      "1521\n",
      "step: 1521, train loss: 2.161, val loss 2.218\n",
      "1522\n",
      "step: 1522, train loss: 2.205, val loss 2.216\n",
      "1523\n",
      "step: 1523, train loss: 2.180, val loss 2.200\n",
      "1524\n",
      "step: 1524, train loss: 2.189, val loss 2.186\n",
      "1525\n",
      "step: 1525, train loss: 2.184, val loss 2.206\n",
      "1526\n",
      "step: 1526, train loss: 2.188, val loss 2.158\n",
      "1527\n",
      "step: 1527, train loss: 2.225, val loss 2.201\n",
      "1528\n",
      "step: 1528, train loss: 2.199, val loss 2.161\n",
      "1529\n",
      "step: 1529, train loss: 2.176, val loss 2.183\n",
      "1530\n",
      "step: 1530, train loss: 2.224, val loss 2.185\n",
      "1531\n",
      "step: 1531, train loss: 2.193, val loss 2.178\n",
      "1532\n",
      "step: 1532, train loss: 2.180, val loss 2.175\n",
      "1533\n",
      "step: 1533, train loss: 2.190, val loss 2.184\n",
      "1534\n",
      "step: 1534, train loss: 2.194, val loss 2.205\n",
      "1535\n",
      "step: 1535, train loss: 2.194, val loss 2.174\n",
      "1536\n",
      "step: 1536, train loss: 2.163, val loss 2.159\n",
      "1537\n",
      "step: 1537, train loss: 2.185, val loss 2.185\n",
      "1538\n",
      "step: 1538, train loss: 2.181, val loss 2.185\n",
      "1539\n",
      "step: 1539, train loss: 2.194, val loss 2.224\n",
      "1540\n",
      "step: 1540, train loss: 2.154, val loss 2.164\n",
      "1541\n",
      "step: 1541, train loss: 2.201, val loss 2.179\n",
      "1542\n",
      "step: 1542, train loss: 2.185, val loss 2.176\n",
      "1543\n",
      "step: 1543, train loss: 2.175, val loss 2.207\n",
      "1544\n",
      "step: 1544, train loss: 2.182, val loss 2.172\n",
      "1545\n",
      "step: 1545, train loss: 2.202, val loss 2.190\n",
      "1546\n",
      "step: 1546, train loss: 2.171, val loss 2.150\n",
      "1547\n",
      "step: 1547, train loss: 2.186, val loss 2.186\n",
      "1548\n",
      "step: 1548, train loss: 2.187, val loss 2.173\n",
      "1549\n",
      "step: 1549, train loss: 2.183, val loss 2.197\n",
      "1550\n",
      "step: 1550, train loss: 2.174, val loss 2.150\n",
      "1551\n",
      "step: 1551, train loss: 2.164, val loss 2.153\n",
      "1552\n",
      "step: 1552, train loss: 2.170, val loss 2.132\n",
      "1553\n",
      "step: 1553, train loss: 2.149, val loss 2.161\n",
      "1554\n",
      "step: 1554, train loss: 2.177, val loss 2.135\n",
      "1555\n",
      "step: 1555, train loss: 2.165, val loss 2.161\n",
      "1556\n",
      "step: 1556, train loss: 2.119, val loss 2.168\n",
      "1557\n",
      "step: 1557, train loss: 2.167, val loss 2.156\n",
      "1558\n",
      "step: 1558, train loss: 2.174, val loss 2.187\n",
      "1559\n",
      "step: 1559, train loss: 2.165, val loss 2.189\n",
      "1560\n",
      "step: 1560, train loss: 2.130, val loss 2.173\n",
      "1561\n",
      "step: 1561, train loss: 2.167, val loss 2.146\n",
      "1562\n",
      "step: 1562, train loss: 2.181, val loss 2.189\n",
      "1563\n",
      "step: 1563, train loss: 2.191, val loss 2.156\n",
      "1564\n",
      "step: 1564, train loss: 2.168, val loss 2.171\n",
      "1565\n",
      "step: 1565, train loss: 2.187, val loss 2.174\n",
      "1566\n",
      "step: 1566, train loss: 2.191, val loss 2.153\n",
      "1567\n",
      "step: 1567, train loss: 2.153, val loss 2.146\n",
      "1568\n",
      "step: 1568, train loss: 2.131, val loss 2.120\n",
      "1569\n",
      "step: 1569, train loss: 2.143, val loss 2.118\n",
      "1570\n",
      "step: 1570, train loss: 2.169, val loss 2.132\n",
      "1571\n",
      "step: 1571, train loss: 2.130, val loss 2.145\n",
      "1572\n",
      "step: 1572, train loss: 2.125, val loss 2.112\n",
      "1573\n",
      "step: 1573, train loss: 2.154, val loss 2.150\n",
      "1574\n",
      "step: 1574, train loss: 2.132, val loss 2.144\n",
      "1575\n",
      "step: 1575, train loss: 2.164, val loss 2.175\n",
      "1576\n",
      "step: 1576, train loss: 2.130, val loss 2.133\n",
      "1577\n",
      "step: 1577, train loss: 2.123, val loss 2.146\n",
      "1578\n",
      "step: 1578, train loss: 2.137, val loss 2.134\n",
      "1579\n",
      "step: 1579, train loss: 2.157, val loss 2.131\n",
      "1580\n",
      "step: 1580, train loss: 2.152, val loss 2.144\n",
      "1581\n",
      "step: 1581, train loss: 2.126, val loss 2.129\n",
      "1582\n",
      "step: 1582, train loss: 2.142, val loss 2.156\n",
      "1583\n",
      "step: 1583, train loss: 2.164, val loss 2.168\n",
      "1584\n",
      "step: 1584, train loss: 2.168, val loss 2.177\n",
      "1585\n",
      "step: 1585, train loss: 2.170, val loss 2.174\n",
      "1586\n",
      "step: 1586, train loss: 2.156, val loss 2.147\n",
      "1587\n",
      "step: 1587, train loss: 2.148, val loss 2.143\n",
      "1588\n",
      "step: 1588, train loss: 2.146, val loss 2.138\n",
      "1589\n",
      "step: 1589, train loss: 2.154, val loss 2.144\n",
      "1590\n",
      "step: 1590, train loss: 2.180, val loss 2.160\n",
      "1591\n",
      "step: 1591, train loss: 2.153, val loss 2.117\n",
      "1592\n",
      "step: 1592, train loss: 2.172, val loss 2.126\n",
      "1593\n",
      "step: 1593, train loss: 2.134, val loss 2.113\n",
      "1594\n",
      "step: 1594, train loss: 2.139, val loss 2.128\n",
      "1595\n",
      "step: 1595, train loss: 2.131, val loss 2.118\n",
      "1596\n",
      "step: 1596, train loss: 2.136, val loss 2.140\n",
      "1597\n",
      "step: 1597, train loss: 2.171, val loss 2.173\n",
      "1598\n",
      "step: 1598, train loss: 2.134, val loss 2.122\n",
      "1599\n",
      "step: 1599, train loss: 2.130, val loss 2.143\n",
      "1600\n",
      "step: 1600, train loss: 2.125, val loss 2.151\n",
      "1601\n",
      "step: 1601, train loss: 2.112, val loss 2.106\n",
      "1602\n",
      "step: 1602, train loss: 2.118, val loss 2.139\n",
      "1603\n",
      "step: 1603, train loss: 2.118, val loss 2.137\n",
      "1604\n",
      "step: 1604, train loss: 2.125, val loss 2.131\n",
      "1605\n",
      "step: 1605, train loss: 2.155, val loss 2.129\n",
      "1606\n",
      "step: 1606, train loss: 2.162, val loss 2.134\n",
      "1607\n",
      "step: 1607, train loss: 2.149, val loss 2.175\n",
      "1608\n",
      "step: 1608, train loss: 2.150, val loss 2.175\n",
      "1609\n",
      "step: 1609, train loss: 2.153, val loss 2.152\n",
      "1610\n",
      "step: 1610, train loss: 2.162, val loss 2.168\n",
      "1611\n",
      "step: 1611, train loss: 2.142, val loss 2.129\n",
      "1612\n",
      "step: 1612, train loss: 2.139, val loss 2.154\n",
      "1613\n",
      "step: 1613, train loss: 2.131, val loss 2.151\n",
      "1614\n",
      "step: 1614, train loss: 2.125, val loss 2.119\n",
      "1615\n",
      "step: 1615, train loss: 2.125, val loss 2.150\n",
      "1616\n",
      "step: 1616, train loss: 2.125, val loss 2.157\n",
      "1617\n",
      "step: 1617, train loss: 2.139, val loss 2.134\n",
      "1618\n",
      "step: 1618, train loss: 2.123, val loss 2.148\n",
      "1619\n",
      "step: 1619, train loss: 2.103, val loss 2.090\n",
      "1620\n",
      "step: 1620, train loss: 2.137, val loss 2.142\n",
      "1621\n",
      "step: 1621, train loss: 2.142, val loss 2.146\n",
      "1622\n",
      "step: 1622, train loss: 2.130, val loss 2.141\n",
      "1623\n",
      "step: 1623, train loss: 2.105, val loss 2.111\n",
      "1624\n",
      "step: 1624, train loss: 2.120, val loss 2.135\n",
      "1625\n",
      "step: 1625, train loss: 2.117, val loss 2.132\n",
      "1626\n",
      "step: 1626, train loss: 2.144, val loss 2.128\n",
      "1627\n",
      "step: 1627, train loss: 2.118, val loss 2.093\n",
      "1628\n",
      "step: 1628, train loss: 2.108, val loss 2.104\n",
      "1629\n",
      "step: 1629, train loss: 2.088, val loss 2.100\n",
      "1630\n",
      "step: 1630, train loss: 2.120, val loss 2.122\n",
      "1631\n",
      "step: 1631, train loss: 2.165, val loss 2.088\n",
      "1632\n",
      "step: 1632, train loss: 2.074, val loss 2.126\n",
      "1633\n",
      "step: 1633, train loss: 2.150, val loss 2.091\n",
      "1634\n",
      "step: 1634, train loss: 2.112, val loss 2.105\n",
      "1635\n",
      "step: 1635, train loss: 2.118, val loss 2.124\n",
      "1636\n",
      "step: 1636, train loss: 2.112, val loss 2.108\n",
      "1637\n",
      "step: 1637, train loss: 2.103, val loss 2.084\n",
      "1638\n",
      "step: 1638, train loss: 2.103, val loss 2.136\n",
      "1639\n",
      "step: 1639, train loss: 2.155, val loss 2.107\n",
      "1640\n",
      "step: 1640, train loss: 2.114, val loss 2.098\n",
      "1641\n",
      "step: 1641, train loss: 2.124, val loss 2.104\n",
      "1642\n",
      "step: 1642, train loss: 2.132, val loss 2.095\n",
      "1643\n",
      "step: 1643, train loss: 2.090, val loss 2.111\n",
      "1644\n",
      "step: 1644, train loss: 2.092, val loss 2.129\n",
      "1645\n",
      "step: 1645, train loss: 2.125, val loss 2.119\n",
      "1646\n",
      "step: 1646, train loss: 2.088, val loss 2.114\n",
      "1647\n",
      "step: 1647, train loss: 2.094, val loss 2.100\n",
      "1648\n",
      "step: 1648, train loss: 2.104, val loss 2.100\n",
      "1649\n",
      "step: 1649, train loss: 2.095, val loss 2.095\n",
      "1650\n",
      "step: 1650, train loss: 2.088, val loss 2.099\n",
      "1651\n",
      "step: 1651, train loss: 2.113, val loss 2.106\n",
      "1652\n",
      "step: 1652, train loss: 2.110, val loss 2.110\n",
      "1653\n",
      "step: 1653, train loss: 2.114, val loss 2.125\n",
      "1654\n",
      "step: 1654, train loss: 2.131, val loss 2.109\n",
      "1655\n",
      "step: 1655, train loss: 2.121, val loss 2.100\n",
      "1656\n",
      "step: 1656, train loss: 2.076, val loss 2.073\n",
      "1657\n",
      "step: 1657, train loss: 2.095, val loss 2.093\n",
      "1658\n",
      "step: 1658, train loss: 2.093, val loss 2.099\n",
      "1659\n",
      "step: 1659, train loss: 2.077, val loss 2.122\n",
      "1660\n",
      "step: 1660, train loss: 2.125, val loss 2.115\n",
      "1661\n",
      "step: 1661, train loss: 2.093, val loss 2.082\n",
      "1662\n",
      "step: 1662, train loss: 2.107, val loss 2.085\n",
      "1663\n",
      "step: 1663, train loss: 2.101, val loss 2.119\n",
      "1664\n",
      "step: 1664, train loss: 2.073, val loss 2.108\n",
      "1665\n",
      "step: 1665, train loss: 2.103, val loss 2.115\n",
      "1666\n",
      "step: 1666, train loss: 2.081, val loss 2.082\n",
      "1667\n",
      "step: 1667, train loss: 2.060, val loss 2.084\n",
      "1668\n",
      "step: 1668, train loss: 2.087, val loss 2.101\n",
      "1669\n",
      "step: 1669, train loss: 2.108, val loss 2.108\n",
      "1670\n",
      "step: 1670, train loss: 2.085, val loss 2.113\n",
      "1671\n",
      "step: 1671, train loss: 2.106, val loss 2.109\n",
      "1672\n",
      "step: 1672, train loss: 2.099, val loss 2.108\n",
      "1673\n",
      "step: 1673, train loss: 2.119, val loss 2.088\n",
      "1674\n",
      "step: 1674, train loss: 2.104, val loss 2.087\n",
      "1675\n",
      "step: 1675, train loss: 2.094, val loss 2.094\n",
      "1676\n",
      "step: 1676, train loss: 2.108, val loss 2.112\n",
      "1677\n",
      "step: 1677, train loss: 2.075, val loss 2.096\n",
      "1678\n",
      "step: 1678, train loss: 2.083, val loss 2.111\n",
      "1679\n",
      "step: 1679, train loss: 2.084, val loss 2.110\n",
      "1680\n",
      "step: 1680, train loss: 2.085, val loss 2.082\n",
      "1681\n",
      "step: 1681, train loss: 2.098, val loss 2.115\n",
      "1682\n",
      "step: 1682, train loss: 2.106, val loss 2.084\n",
      "1683\n",
      "step: 1683, train loss: 2.109, val loss 2.072\n",
      "1684\n",
      "step: 1684, train loss: 2.113, val loss 2.090\n",
      "1685\n",
      "step: 1685, train loss: 2.082, val loss 2.089\n",
      "1686\n",
      "step: 1686, train loss: 2.104, val loss 2.106\n",
      "1687\n",
      "step: 1687, train loss: 2.110, val loss 2.109\n",
      "1688\n",
      "step: 1688, train loss: 2.119, val loss 2.092\n",
      "1689\n",
      "step: 1689, train loss: 2.113, val loss 2.095\n",
      "1690\n",
      "step: 1690, train loss: 2.081, val loss 2.109\n",
      "1691\n",
      "step: 1691, train loss: 2.095, val loss 2.098\n",
      "1692\n",
      "step: 1692, train loss: 2.095, val loss 2.104\n",
      "1693\n",
      "step: 1693, train loss: 2.112, val loss 2.094\n",
      "1694\n",
      "step: 1694, train loss: 2.118, val loss 2.071\n",
      "1695\n",
      "step: 1695, train loss: 2.096, val loss 2.108\n",
      "1696\n",
      "step: 1696, train loss: 2.068, val loss 2.090\n",
      "1697\n",
      "step: 1697, train loss: 2.084, val loss 2.072\n",
      "1698\n",
      "step: 1698, train loss: 2.104, val loss 2.056\n",
      "1699\n",
      "step: 1699, train loss: 2.095, val loss 2.101\n",
      "1700\n",
      "step: 1700, train loss: 2.117, val loss 2.102\n",
      "1701\n",
      "step: 1701, train loss: 2.127, val loss 2.107\n",
      "1702\n",
      "step: 1702, train loss: 2.115, val loss 2.135\n",
      "1703\n",
      "step: 1703, train loss: 2.122, val loss 2.109\n",
      "1704\n",
      "step: 1704, train loss: 2.105, val loss 2.087\n",
      "1705\n",
      "step: 1705, train loss: 2.148, val loss 2.083\n",
      "1706\n",
      "step: 1706, train loss: 2.102, val loss 2.081\n",
      "1707\n",
      "step: 1707, train loss: 2.111, val loss 2.103\n",
      "1708\n",
      "step: 1708, train loss: 2.143, val loss 2.123\n",
      "1709\n",
      "step: 1709, train loss: 2.106, val loss 2.116\n",
      "1710\n",
      "step: 1710, train loss: 2.087, val loss 2.114\n",
      "1711\n",
      "step: 1711, train loss: 2.104, val loss 2.123\n",
      "1712\n",
      "step: 1712, train loss: 2.079, val loss 2.085\n",
      "1713\n",
      "step: 1713, train loss: 2.150, val loss 2.107\n",
      "1714\n",
      "step: 1714, train loss: 2.108, val loss 2.115\n",
      "1715\n",
      "step: 1715, train loss: 2.113, val loss 2.114\n",
      "1716\n",
      "step: 1716, train loss: 2.115, val loss 2.068\n",
      "1717\n",
      "step: 1717, train loss: 2.087, val loss 2.069\n",
      "1718\n",
      "step: 1718, train loss: 2.091, val loss 2.137\n",
      "1719\n",
      "step: 1719, train loss: 2.073, val loss 2.112\n",
      "1720\n",
      "step: 1720, train loss: 2.097, val loss 2.084\n",
      "1721\n",
      "step: 1721, train loss: 2.068, val loss 2.077\n",
      "1722\n",
      "step: 1722, train loss: 2.095, val loss 2.071\n",
      "1723\n",
      "step: 1723, train loss: 2.064, val loss 2.111\n",
      "1724\n",
      "step: 1724, train loss: 2.058, val loss 2.077\n",
      "1725\n",
      "step: 1725, train loss: 2.119, val loss 2.105\n",
      "1726\n",
      "step: 1726, train loss: 2.110, val loss 2.088\n",
      "1727\n",
      "step: 1727, train loss: 2.079, val loss 2.091\n",
      "1728\n",
      "step: 1728, train loss: 2.082, val loss 2.105\n",
      "1729\n",
      "step: 1729, train loss: 2.079, val loss 2.075\n",
      "1730\n",
      "step: 1730, train loss: 2.089, val loss 2.096\n",
      "1731\n",
      "step: 1731, train loss: 2.074, val loss 2.090\n",
      "1732\n",
      "step: 1732, train loss: 2.082, val loss 2.045\n",
      "1733\n",
      "step: 1733, train loss: 2.078, val loss 2.090\n",
      "1734\n",
      "step: 1734, train loss: 2.094, val loss 2.061\n",
      "1735\n",
      "step: 1735, train loss: 2.050, val loss 2.042\n",
      "1736\n",
      "step: 1736, train loss: 2.055, val loss 2.089\n",
      "1737\n",
      "step: 1737, train loss: 2.034, val loss 2.059\n",
      "1738\n",
      "step: 1738, train loss: 2.064, val loss 2.085\n",
      "1739\n",
      "step: 1739, train loss: 2.051, val loss 2.089\n",
      "1740\n",
      "step: 1740, train loss: 2.047, val loss 2.063\n",
      "1741\n",
      "step: 1741, train loss: 2.067, val loss 2.067\n",
      "1742\n",
      "step: 1742, train loss: 2.071, val loss 2.123\n",
      "1743\n",
      "step: 1743, train loss: 2.058, val loss 2.090\n",
      "1744\n",
      "step: 1744, train loss: 2.081, val loss 2.081\n",
      "1745\n",
      "step: 1745, train loss: 2.098, val loss 2.080\n",
      "1746\n",
      "step: 1746, train loss: 2.067, val loss 2.030\n",
      "1747\n",
      "step: 1747, train loss: 2.046, val loss 2.073\n",
      "1748\n",
      "step: 1748, train loss: 2.058, val loss 2.069\n",
      "1749\n",
      "step: 1749, train loss: 2.072, val loss 2.056\n",
      "1750\n",
      "step: 1750, train loss: 2.050, val loss 2.066\n",
      "1751\n",
      "step: 1751, train loss: 2.100, val loss 2.079\n",
      "1752\n",
      "step: 1752, train loss: 2.064, val loss 2.112\n",
      "1753\n",
      "step: 1753, train loss: 2.071, val loss 2.051\n",
      "1754\n",
      "step: 1754, train loss: 2.078, val loss 2.060\n",
      "1755\n",
      "step: 1755, train loss: 2.073, val loss 2.092\n",
      "1756\n",
      "step: 1756, train loss: 2.045, val loss 2.020\n",
      "1757\n",
      "step: 1757, train loss: 2.051, val loss 2.116\n",
      "1758\n",
      "step: 1758, train loss: 2.067, val loss 2.044\n",
      "1759\n",
      "step: 1759, train loss: 2.086, val loss 2.063\n",
      "1760\n",
      "step: 1760, train loss: 2.067, val loss 2.043\n",
      "1761\n",
      "step: 1761, train loss: 2.061, val loss 2.072\n",
      "1762\n",
      "step: 1762, train loss: 2.071, val loss 2.068\n",
      "1763\n",
      "step: 1763, train loss: 2.091, val loss 2.070\n",
      "1764\n",
      "step: 1764, train loss: 2.077, val loss 2.051\n",
      "1765\n",
      "step: 1765, train loss: 2.065, val loss 2.109\n",
      "1766\n",
      "step: 1766, train loss: 2.079, val loss 2.051\n",
      "1767\n",
      "step: 1767, train loss: 2.070, val loss 2.090\n",
      "1768\n",
      "step: 1768, train loss: 2.067, val loss 2.073\n",
      "1769\n",
      "step: 1769, train loss: 2.056, val loss 2.064\n",
      "1770\n",
      "step: 1770, train loss: 2.087, val loss 2.074\n",
      "1771\n",
      "step: 1771, train loss: 2.093, val loss 2.109\n",
      "1772\n",
      "step: 1772, train loss: 2.099, val loss 2.084\n",
      "1773\n",
      "step: 1773, train loss: 2.055, val loss 2.060\n",
      "1774\n",
      "step: 1774, train loss: 2.079, val loss 2.064\n",
      "1775\n",
      "step: 1775, train loss: 2.146, val loss 2.062\n",
      "1776\n",
      "step: 1776, train loss: 2.080, val loss 2.108\n",
      "1777\n",
      "step: 1777, train loss: 2.114, val loss 2.075\n",
      "1778\n",
      "step: 1778, train loss: 2.081, val loss 2.074\n",
      "1779\n",
      "step: 1779, train loss: 2.071, val loss 2.097\n",
      "1780\n",
      "step: 1780, train loss: 2.060, val loss 2.093\n",
      "1781\n",
      "step: 1781, train loss: 2.090, val loss 2.086\n",
      "1782\n",
      "step: 1782, train loss: 2.067, val loss 2.090\n",
      "1783\n",
      "step: 1783, train loss: 2.101, val loss 2.068\n",
      "1784\n",
      "step: 1784, train loss: 2.045, val loss 2.067\n",
      "1785\n",
      "step: 1785, train loss: 2.094, val loss 2.061\n",
      "1786\n",
      "step: 1786, train loss: 2.110, val loss 2.103\n",
      "1787\n",
      "step: 1787, train loss: 2.070, val loss 2.108\n",
      "1788\n",
      "step: 1788, train loss: 2.069, val loss 2.083\n",
      "1789\n",
      "step: 1789, train loss: 2.068, val loss 2.102\n",
      "1790\n",
      "step: 1790, train loss: 2.076, val loss 2.098\n",
      "1791\n",
      "step: 1791, train loss: 2.107, val loss 2.082\n",
      "1792\n",
      "step: 1792, train loss: 2.102, val loss 2.159\n",
      "1793\n",
      "step: 1793, train loss: 2.100, val loss 2.098\n",
      "1794\n",
      "step: 1794, train loss: 2.112, val loss 2.077\n",
      "1795\n",
      "step: 1795, train loss: 2.040, val loss 2.111\n",
      "1796\n",
      "step: 1796, train loss: 2.088, val loss 2.075\n",
      "1797\n",
      "step: 1797, train loss: 2.102, val loss 2.066\n",
      "1798\n",
      "step: 1798, train loss: 2.042, val loss 2.117\n",
      "1799\n",
      "step: 1799, train loss: 2.083, val loss 2.066\n",
      "1800\n",
      "step: 1800, train loss: 2.090, val loss 2.083\n",
      "1801\n",
      "step: 1801, train loss: 2.060, val loss 2.081\n",
      "1802\n",
      "step: 1802, train loss: 2.062, val loss 2.041\n",
      "1803\n",
      "step: 1803, train loss: 2.049, val loss 2.072\n",
      "1804\n",
      "step: 1804, train loss: 2.079, val loss 2.070\n",
      "1805\n",
      "step: 1805, train loss: 2.049, val loss 2.056\n",
      "1806\n",
      "step: 1806, train loss: 2.085, val loss 2.062\n",
      "1807\n",
      "step: 1807, train loss: 2.066, val loss 2.042\n",
      "1808\n",
      "step: 1808, train loss: 2.045, val loss 2.048\n",
      "1809\n",
      "step: 1809, train loss: 2.083, val loss 2.067\n",
      "1810\n",
      "step: 1810, train loss: 2.073, val loss 2.095\n",
      "1811\n",
      "step: 1811, train loss: 2.087, val loss 2.074\n",
      "1812\n",
      "step: 1812, train loss: 2.087, val loss 2.071\n",
      "1813\n",
      "step: 1813, train loss: 2.063, val loss 2.072\n",
      "1814\n",
      "step: 1814, train loss: 2.063, val loss 2.059\n",
      "1815\n",
      "step: 1815, train loss: 2.096, val loss 2.109\n",
      "1816\n",
      "step: 1816, train loss: 2.048, val loss 2.072\n",
      "1817\n",
      "step: 1817, train loss: 2.043, val loss 2.080\n",
      "1818\n",
      "step: 1818, train loss: 2.049, val loss 2.086\n",
      "1819\n",
      "step: 1819, train loss: 2.067, val loss 2.062\n",
      "1820\n",
      "step: 1820, train loss: 2.066, val loss 2.056\n",
      "1821\n",
      "step: 1821, train loss: 2.043, val loss 2.073\n",
      "1822\n",
      "step: 1822, train loss: 2.075, val loss 2.074\n",
      "1823\n",
      "step: 1823, train loss: 2.076, val loss 2.081\n",
      "1824\n",
      "step: 1824, train loss: 2.055, val loss 2.078\n",
      "1825\n",
      "step: 1825, train loss: 2.036, val loss 2.063\n",
      "1826\n",
      "step: 1826, train loss: 2.077, val loss 2.062\n",
      "1827\n",
      "step: 1827, train loss: 2.069, val loss 2.070\n",
      "1828\n",
      "step: 1828, train loss: 2.050, val loss 2.078\n",
      "1829\n",
      "step: 1829, train loss: 2.074, val loss 2.103\n",
      "1830\n",
      "step: 1830, train loss: 2.051, val loss 2.042\n",
      "1831\n",
      "step: 1831, train loss: 2.068, val loss 2.073\n",
      "1832\n",
      "step: 1832, train loss: 2.081, val loss 2.028\n",
      "1833\n",
      "step: 1833, train loss: 2.053, val loss 2.054\n",
      "1834\n",
      "step: 1834, train loss: 2.060, val loss 2.036\n",
      "1835\n",
      "step: 1835, train loss: 2.044, val loss 2.084\n",
      "1836\n",
      "step: 1836, train loss: 2.051, val loss 2.066\n",
      "1837\n",
      "step: 1837, train loss: 2.055, val loss 2.062\n",
      "1838\n",
      "step: 1838, train loss: 2.045, val loss 2.075\n",
      "1839\n",
      "step: 1839, train loss: 2.093, val loss 2.069\n",
      "1840\n",
      "step: 1840, train loss: 2.079, val loss 2.075\n",
      "1841\n",
      "step: 1841, train loss: 2.080, val loss 2.088\n",
      "1842\n",
      "step: 1842, train loss: 2.053, val loss 2.078\n",
      "1843\n",
      "step: 1843, train loss: 2.073, val loss 2.071\n",
      "1844\n",
      "step: 1844, train loss: 2.068, val loss 2.069\n",
      "1845\n",
      "step: 1845, train loss: 2.130, val loss 2.067\n",
      "1846\n",
      "step: 1846, train loss: 2.072, val loss 2.054\n",
      "1847\n",
      "step: 1847, train loss: 2.045, val loss 2.069\n",
      "1848\n",
      "step: 1848, train loss: 2.061, val loss 2.088\n",
      "1849\n",
      "step: 1849, train loss: 2.046, val loss 2.049\n",
      "1850\n",
      "step: 1850, train loss: 2.014, val loss 2.041\n",
      "1851\n",
      "step: 1851, train loss: 2.058, val loss 2.038\n",
      "1852\n",
      "step: 1852, train loss: 2.006, val loss 2.041\n",
      "1853\n",
      "step: 1853, train loss: 2.038, val loss 2.041\n",
      "1854\n",
      "step: 1854, train loss: 2.017, val loss 2.043\n",
      "1855\n",
      "step: 1855, train loss: 2.055, val loss 2.040\n",
      "1856\n",
      "step: 1856, train loss: 2.086, val loss 2.061\n",
      "1857\n",
      "step: 1857, train loss: 2.106, val loss 2.032\n",
      "1858\n",
      "step: 1858, train loss: 2.059, val loss 2.060\n",
      "1859\n",
      "step: 1859, train loss: 2.066, val loss 2.082\n",
      "1860\n",
      "step: 1860, train loss: 2.082, val loss 2.039\n",
      "1861\n",
      "step: 1861, train loss: 2.032, val loss 2.040\n",
      "1862\n",
      "step: 1862, train loss: 2.042, val loss 2.031\n",
      "1863\n",
      "step: 1863, train loss: 2.069, val loss 2.053\n",
      "1864\n",
      "step: 1864, train loss: 2.075, val loss 2.075\n",
      "1865\n",
      "step: 1865, train loss: 2.047, val loss 2.014\n",
      "1866\n",
      "step: 1866, train loss: 2.081, val loss 2.072\n",
      "1867\n",
      "step: 1867, train loss: 2.048, val loss 2.042\n",
      "1868\n",
      "step: 1868, train loss: 2.044, val loss 2.049\n",
      "1869\n",
      "step: 1869, train loss: 2.064, val loss 2.088\n",
      "1870\n",
      "step: 1870, train loss: 2.066, val loss 2.072\n",
      "1871\n",
      "step: 1871, train loss: 2.079, val loss 2.152\n",
      "1872\n",
      "step: 1872, train loss: 2.096, val loss 2.028\n",
      "1873\n",
      "step: 1873, train loss: 2.031, val loss 2.044\n",
      "1874\n",
      "step: 1874, train loss: 2.043, val loss 2.053\n",
      "1875\n",
      "step: 1875, train loss: 2.064, val loss 2.020\n",
      "1876\n",
      "step: 1876, train loss: 2.045, val loss 2.021\n",
      "1877\n",
      "step: 1877, train loss: 2.054, val loss 2.030\n",
      "1878\n",
      "step: 1878, train loss: 2.080, val loss 2.089\n",
      "1879\n",
      "step: 1879, train loss: 2.083, val loss 2.056\n",
      "1880\n",
      "step: 1880, train loss: 2.065, val loss 2.057\n",
      "1881\n",
      "step: 1881, train loss: 2.048, val loss 2.049\n",
      "1882\n",
      "step: 1882, train loss: 2.063, val loss 2.093\n",
      "1883\n",
      "step: 1883, train loss: 2.029, val loss 2.037\n",
      "1884\n",
      "step: 1884, train loss: 2.074, val loss 2.026\n",
      "1885\n",
      "step: 1885, train loss: 2.050, val loss 2.056\n",
      "1886\n",
      "step: 1886, train loss: 2.063, val loss 2.045\n",
      "1887\n",
      "step: 1887, train loss: 2.049, val loss 2.055\n",
      "1888\n",
      "step: 1888, train loss: 2.066, val loss 2.044\n",
      "1889\n",
      "step: 1889, train loss: 2.068, val loss 2.057\n",
      "1890\n",
      "step: 1890, train loss: 2.063, val loss 2.061\n",
      "1891\n",
      "step: 1891, train loss: 2.066, val loss 2.030\n",
      "1892\n",
      "step: 1892, train loss: 2.025, val loss 2.033\n",
      "1893\n",
      "step: 1893, train loss: 2.043, val loss 2.050\n",
      "1894\n",
      "step: 1894, train loss: 2.096, val loss 2.037\n",
      "1895\n",
      "step: 1895, train loss: 2.037, val loss 2.052\n",
      "1896\n",
      "step: 1896, train loss: 2.059, val loss 2.051\n",
      "1897\n",
      "step: 1897, train loss: 2.063, val loss 2.099\n",
      "1898\n",
      "step: 1898, train loss: 2.066, val loss 2.052\n",
      "1899\n",
      "step: 1899, train loss: 2.062, val loss 2.064\n",
      "1900\n",
      "step: 1900, train loss: 2.028, val loss 2.055\n",
      "1901\n",
      "step: 1901, train loss: 2.059, val loss 2.075\n",
      "1902\n",
      "step: 1902, train loss: 2.037, val loss 2.051\n",
      "1903\n",
      "step: 1903, train loss: 2.063, val loss 2.066\n",
      "1904\n",
      "step: 1904, train loss: 2.017, val loss 2.103\n",
      "1905\n",
      "step: 1905, train loss: 2.041, val loss 2.002\n",
      "1906\n",
      "step: 1906, train loss: 2.058, val loss 2.062\n",
      "1907\n",
      "step: 1907, train loss: 2.024, val loss 2.012\n",
      "1908\n",
      "step: 1908, train loss: 2.068, val loss 2.019\n",
      "1909\n",
      "step: 1909, train loss: 2.095, val loss 2.118\n",
      "1910\n",
      "step: 1910, train loss: 2.078, val loss 2.050\n",
      "1911\n",
      "step: 1911, train loss: 2.057, val loss 2.042\n",
      "1912\n",
      "step: 1912, train loss: 2.086, val loss 2.059\n",
      "1913\n",
      "step: 1913, train loss: 2.023, val loss 2.057\n",
      "1914\n",
      "step: 1914, train loss: 2.045, val loss 2.036\n",
      "1915\n",
      "step: 1915, train loss: 2.052, val loss 2.040\n",
      "1916\n",
      "step: 1916, train loss: 2.045, val loss 2.024\n",
      "1917\n",
      "step: 1917, train loss: 2.037, val loss 2.009\n",
      "1918\n",
      "step: 1918, train loss: 2.049, val loss 2.024\n",
      "1919\n",
      "step: 1919, train loss: 2.010, val loss 2.043\n",
      "1920\n",
      "step: 1920, train loss: 2.045, val loss 2.037\n",
      "1921\n",
      "step: 1921, train loss: 2.022, val loss 2.018\n",
      "1922\n",
      "step: 1922, train loss: 2.027, val loss 2.029\n",
      "1923\n",
      "step: 1923, train loss: 2.053, val loss 2.063\n",
      "1924\n",
      "step: 1924, train loss: 2.047, val loss 2.040\n",
      "1925\n",
      "step: 1925, train loss: 2.052, val loss 2.052\n",
      "1926\n",
      "step: 1926, train loss: 2.046, val loss 2.031\n",
      "1927\n",
      "step: 1927, train loss: 2.051, val loss 2.025\n",
      "1928\n",
      "step: 1928, train loss: 2.026, val loss 2.024\n",
      "1929\n",
      "step: 1929, train loss: 2.069, val loss 2.036\n",
      "1930\n",
      "step: 1930, train loss: 2.028, val loss 2.015\n",
      "1931\n",
      "step: 1931, train loss: 2.082, val loss 2.041\n",
      "1932\n",
      "step: 1932, train loss: 1.998, val loss 2.038\n",
      "1933\n",
      "step: 1933, train loss: 2.065, val loss 2.045\n",
      "1934\n",
      "step: 1934, train loss: 2.029, val loss 2.052\n",
      "1935\n",
      "step: 1935, train loss: 2.052, val loss 2.061\n",
      "1936\n",
      "step: 1936, train loss: 2.036, val loss 2.033\n",
      "1937\n",
      "step: 1937, train loss: 2.013, val loss 2.040\n",
      "1938\n",
      "step: 1938, train loss: 2.049, val loss 2.020\n",
      "1939\n",
      "step: 1939, train loss: 2.045, val loss 2.015\n",
      "1940\n",
      "step: 1940, train loss: 2.028, val loss 2.011\n",
      "1941\n",
      "step: 1941, train loss: 2.051, val loss 2.024\n",
      "1942\n",
      "step: 1942, train loss: 2.042, val loss 2.033\n",
      "1943\n",
      "step: 1943, train loss: 2.024, val loss 2.064\n",
      "1944\n",
      "step: 1944, train loss: 2.006, val loss 2.033\n",
      "1945\n",
      "step: 1945, train loss: 2.032, val loss 2.015\n",
      "1946\n",
      "step: 1946, train loss: 2.081, val loss 2.035\n",
      "1947\n",
      "step: 1947, train loss: 2.017, val loss 2.043\n",
      "1948\n",
      "step: 1948, train loss: 2.042, val loss 2.040\n",
      "1949\n",
      "step: 1949, train loss: 2.047, val loss 2.021\n",
      "1950\n",
      "step: 1950, train loss: 2.002, val loss 2.015\n",
      "1951\n",
      "step: 1951, train loss: 2.067, val loss 2.014\n",
      "1952\n",
      "step: 1952, train loss: 2.010, val loss 2.017\n",
      "1953\n",
      "step: 1953, train loss: 2.033, val loss 2.023\n",
      "1954\n",
      "step: 1954, train loss: 2.080, val loss 2.063\n",
      "1955\n",
      "step: 1955, train loss: 2.020, val loss 2.065\n",
      "1956\n",
      "step: 1956, train loss: 2.036, val loss 2.016\n",
      "1957\n",
      "step: 1957, train loss: 2.012, val loss 2.012\n",
      "1958\n",
      "step: 1958, train loss: 2.017, val loss 2.026\n",
      "1959\n",
      "step: 1959, train loss: 2.021, val loss 2.022\n",
      "1960\n",
      "step: 1960, train loss: 2.054, val loss 2.044\n",
      "1961\n",
      "step: 1961, train loss: 1.994, val loss 2.035\n",
      "1962\n",
      "step: 1962, train loss: 2.013, val loss 2.019\n",
      "1963\n",
      "step: 1963, train loss: 1.999, val loss 2.021\n",
      "1964\n",
      "step: 1964, train loss: 2.039, val loss 2.063\n",
      "1965\n",
      "step: 1965, train loss: 2.046, val loss 2.058\n",
      "1966\n",
      "step: 1966, train loss: 2.011, val loss 2.038\n",
      "1967\n",
      "step: 1967, train loss: 2.021, val loss 2.020\n",
      "1968\n",
      "step: 1968, train loss: 2.004, val loss 2.037\n",
      "1969\n",
      "step: 1969, train loss: 2.050, val loss 2.085\n",
      "1970\n",
      "step: 1970, train loss: 2.034, val loss 1.998\n",
      "1971\n",
      "step: 1971, train loss: 2.040, val loss 2.060\n",
      "1972\n",
      "step: 1972, train loss: 2.072, val loss 2.048\n",
      "1973\n",
      "step: 1973, train loss: 2.017, val loss 2.030\n",
      "1974\n",
      "step: 1974, train loss: 2.064, val loss 2.028\n",
      "1975\n",
      "step: 1975, train loss: 2.037, val loss 2.026\n",
      "1976\n",
      "step: 1976, train loss: 1.987, val loss 2.024\n",
      "1977\n",
      "step: 1977, train loss: 2.007, val loss 2.040\n",
      "1978\n",
      "step: 1978, train loss: 1.992, val loss 2.019\n",
      "1979\n",
      "step: 1979, train loss: 2.004, val loss 2.014\n",
      "1980\n",
      "step: 1980, train loss: 1.997, val loss 1.977\n",
      "1981\n",
      "step: 1981, train loss: 2.001, val loss 2.012\n",
      "1982\n",
      "step: 1982, train loss: 2.036, val loss 2.023\n",
      "1983\n",
      "step: 1983, train loss: 1.991, val loss 1.995\n",
      "1984\n",
      "step: 1984, train loss: 2.009, val loss 2.023\n",
      "1985\n",
      "step: 1985, train loss: 2.030, val loss 2.031\n",
      "1986\n",
      "step: 1986, train loss: 2.033, val loss 2.067\n",
      "1987\n",
      "step: 1987, train loss: 2.031, val loss 2.012\n",
      "1988\n",
      "step: 1988, train loss: 2.005, val loss 2.056\n",
      "1989\n",
      "step: 1989, train loss: 2.007, val loss 2.002\n",
      "1990\n",
      "step: 1990, train loss: 2.022, val loss 2.007\n",
      "1991\n",
      "step: 1991, train loss: 2.026, val loss 2.024\n",
      "1992\n",
      "step: 1992, train loss: 1.991, val loss 2.047\n",
      "1993\n",
      "step: 1993, train loss: 2.028, val loss 1.992\n",
      "1994\n",
      "step: 1994, train loss: 1.994, val loss 1.993\n",
      "1995\n",
      "step: 1995, train loss: 2.024, val loss 2.007\n",
      "1996\n",
      "step: 1996, train loss: 2.051, val loss 2.060\n",
      "1997\n",
      "step: 1997, train loss: 2.019, val loss 2.012\n",
      "1998\n",
      "step: 1998, train loss: 2.031, val loss 2.025\n",
      "1999\n",
      "step: 1999, train loss: 2.025, val loss 2.007\n",
      "2.1123437881469727\n",
      "model saved\n",
      "Code block executed in 998.2087 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    print(iter)\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits,loss = model.forward(xb,yb) #don't use forward\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-03.pkl', 'wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "print('model saved')\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Code block executed in {elapsed_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96cbb04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Can you see me?\" kene satttanel furs yof isersuch\n",
      "onf or force was he cophasodler eastemeed tists fit foors, was to\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello! Can you see me?\"\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(model.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0dfeeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Linear in module torch.nn.modules.linear:\n",
      "\n",
      "class Linear(torch.nn.modules.module.Module)\n",
      " |  Linear(in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None\n",
      " |  \n",
      " |  Applies an affine linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
      " |  \n",
      " |  This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      " |  \n",
      " |  On certain ROCm devices, when using float16 inputs this module will use :ref:`different precision<fp16_on_mi200>` for backward.\n",
      " |  \n",
      " |  Args:\n",
      " |      in_features: size of each input sample\n",
      " |      out_features: size of each output sample\n",
      " |      bias: If set to ``False``, the layer will not learn an additive bias.\n",
      " |          Default: ``True``\n",
      " |  \n",
      " |  Shape:\n",
      " |      - Input: :math:`(*, H_\\text{in})` where :math:`*` means any number of\n",
      " |        dimensions including none and :math:`H_\\text{in} = \\text{in\\_features}`.\n",
      " |      - Output: :math:`(*, H_\\text{out})` where all but the last dimension\n",
      " |        are the same shape as the input and :math:`H_\\text{out} = \\text{out\\_features}`.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      weight: the learnable weights of the module of shape\n",
      " |          :math:`(\\text{out\\_features}, \\text{in\\_features})`. The values are\n",
      " |          initialized from :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})`, where\n",
      " |          :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      " |      bias:   the learnable bias of the module of shape :math:`(\\text{out\\_features})`.\n",
      " |              If :attr:`bias` is ``True``, the values are initialized from\n",
      " |              :math:`\\mathcal{U}(-\\sqrt{k}, \\sqrt{k})` where\n",
      " |              :math:`k = \\frac{1}{\\text{in\\_features}}`\n",
      " |  \n",
      " |  Examples::\n",
      " |  \n",
      " |      >>> m = nn.Linear(20, 30)\n",
      " |      >>> input = torch.randn(128, 20)\n",
      " |      >>> output = m(input)\n",
      " |      >>> print(output.size())\n",
      " |      torch.Size([128, 30])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Linear\n",
      " |      torch.nn.modules.module.Module\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, in_features: int, out_features: int, bias: bool = True, device=None, dtype=None) -> None\n",
      " |      Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      " |  \n",
      " |  extra_repr(self) -> str\n",
      " |      Return the extra representation of the module.\n",
      " |  \n",
      " |  forward(self, input: torch.Tensor) -> torch.Tensor\n",
      " |      Runs the forward pass.\n",
      " |  \n",
      " |  reset_parameters(self) -> None\n",
      " |      Resets parameters based on their initialization used in ``__init__``.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __annotations__ = {'in_features': <class 'int'>, 'out_features': <clas...\n",
      " |  \n",
      " |  __constants__ = ['in_features', 'out_features']\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __call__ = _wrapped_call_impl(self, *args, **kwargs)\n",
      " |  \n",
      " |  __delattr__(self, name) -> None\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __dir__(self)\n",
      " |      Default dir() implementation.\n",
      " |  \n",
      " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
      " |      # It is crucial that the return type is not annotated as `Any`, otherwise type checking\n",
      " |      # on `torch.nn.Module` and all its subclasses is largely disabled as a result. See:\n",
      " |      # https://github.com/pytorch/pytorch/pull/115074\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |  \n",
      " |  __repr__(self) -> str\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
      " |      Implement setattr(self, name, value).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Add a child module to the current module.\n",
      " |      \n",
      " |      The module can be accessed as an attribute using the given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the child module. The child module can be\n",
      " |              accessed from this module using the given name\n",
      " |          module (Module): child module to be added to the module.\n",
      " |  \n",
      " |  apply(self, fn: Callable[[ForwardRef('Module')], NoneType]) -> Self\n",
      " |      Apply ``fn`` recursively to every submodule (as returned by ``.children()``) as well as self.\n",
      " |      \n",
      " |      Typical use includes initializing the parameters of a model\n",
      " |      (see also :ref:`nn-init-doc`).\n",
      " |      \n",
      " |      Args:\n",
      " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> @torch.no_grad()\n",
      " |          >>> def init_weights(m):\n",
      " |          >>>     print(m)\n",
      " |          >>>     if type(m) == nn.Linear:\n",
      " |          >>>         m.weight.fill_(1.0)\n",
      " |          >>>         print(m.weight)\n",
      " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      " |          >>> net.apply(init_weights)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          Parameter containing:\n",
      " |          tensor([[1., 1.],\n",
      " |                  [1., 1.]], requires_grad=True)\n",
      " |          Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |  \n",
      " |  bfloat16(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  buffers(self, recurse: bool = True) -> collections.abc.Iterator[torch.Tensor]\n",
      " |      Return an iterator over module buffers.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          torch.Tensor: module buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for buf in model.buffers():\n",
      " |          >>>     print(type(buf), buf.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  children(self) -> collections.abc.Iterator['Module']\n",
      " |      Return an iterator over immediate children modules.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a child module\n",
      " |  \n",
      " |  compile(self, *args, **kwargs)\n",
      " |      Compile this Module's forward using :func:`torch.compile`.\n",
      " |      \n",
      " |      This Module's `__call__` method is compiled and all arguments are passed as-is\n",
      " |      to :func:`torch.compile`.\n",
      " |      \n",
      " |      See :func:`torch.compile` for details on the arguments for this function.\n",
      " |  \n",
      " |  cpu(self) -> Self\n",
      " |      Move all model parameters and buffers to the CPU.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  cuda(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the GPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing the optimizer if the module will\n",
      " |      live on GPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  double(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  eval(self) -> Self\n",
      " |      Set the module in evaluation mode.\n",
      " |      \n",
      " |      This has an effect only on certain modules. See the documentation of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, i.e. whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  float(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  get_buffer(self, target: str) -> 'Tensor'\n",
      " |      Return the buffer given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the buffer\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.Tensor: The buffer referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not a\n",
      " |              buffer\n",
      " |  \n",
      " |  get_extra_state(self) -> Any\n",
      " |      Return any extra state to include in the module's state_dict.\n",
      " |      \n",
      " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      " |      if you need to store extra state. This function is called when building the\n",
      " |      module's `state_dict()`.\n",
      " |      \n",
      " |      Note that extra state should be picklable to ensure working serialization\n",
      " |      of the state_dict. We only provide backwards compatibility guarantees\n",
      " |      for serializing Tensors; other objects may break backwards compatibility if\n",
      " |      their serialized pickled form changes.\n",
      " |      \n",
      " |      Returns:\n",
      " |          object: Any extra state to store in the module's state_dict\n",
      " |  \n",
      " |  get_parameter(self, target: str) -> 'Parameter'\n",
      " |      Return the parameter given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      See the docstring for ``get_submodule`` for a more detailed\n",
      " |      explanation of this method's functionality as well as how to\n",
      " |      correctly specify ``target``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the Parameter\n",
      " |              to look for. (See ``get_submodule`` for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If the target string references an invalid\n",
      " |              path or resolves to something that is not an\n",
      " |              ``nn.Parameter``\n",
      " |  \n",
      " |  get_submodule(self, target: str) -> 'Module'\n",
      " |      Return the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      " |                  )\n",
      " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` which has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To check whether or not we have the ``linear`` submodule, we\n",
      " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      " |      we have the ``conv`` submodule, we would call\n",
      " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      " |      \n",
      " |      The runtime of ``get_submodule`` is bounded by the degree\n",
      " |      of module nesting in ``target``. A query against\n",
      " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      " |      the number of transitive modules. So, for a simple check to see\n",
      " |      if some submodule exists, ``get_submodule`` should always be\n",
      " |      used.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |      \n",
      " |      Returns:\n",
      " |          torch.nn.Module: The submodule referenced by ``target``\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: If at any point along the path resulting from\n",
      " |              the target string the (sub)path resolves to a non-existent\n",
      " |              attribute name or an object that is not an instance of ``nn.Module``.\n",
      " |  \n",
      " |  half(self) -> Self\n",
      " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  ipu(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the IPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing the optimizer if the module will\n",
      " |      live on IPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  load_state_dict(self, state_dict: collections.abc.Mapping[str, typing.Any], strict: bool = True, assign: bool = False)\n",
      " |      Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\n",
      " |      \n",
      " |      If :attr:`strict` is ``True``, then\n",
      " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          If :attr:`assign` is ``True`` the optimizer must be created after\n",
      " |          the call to :attr:`load_state_dict` unless\n",
      " |          :func:`~torch.__future__.get_swap_module_params_on_conversion` is ``True``.\n",
      " |      \n",
      " |      Args:\n",
      " |          state_dict (dict): a dict containing parameters and\n",
      " |              persistent buffers.\n",
      " |          strict (bool, optional): whether to strictly enforce that the keys\n",
      " |              in :attr:`state_dict` match the keys returned by this module's\n",
      " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      " |          assign (bool, optional): When set to ``False``, the properties of the tensors\n",
      " |              in the current module are preserved whereas setting it to ``True`` preserves\n",
      " |              properties of the Tensors in the state dict. The only\n",
      " |              exception is the ``requires_grad`` field of :class:`~torch.nn.Parameter`\n",
      " |              for which the value from the module is preserved. Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      " |              * ``missing_keys`` is a list of str containing any keys that are expected\n",
      " |                  by this module but missing from the provided ``state_dict``.\n",
      " |              * ``unexpected_keys`` is a list of str containing the keys that are not\n",
      " |                  expected by this module but present in the provided ``state_dict``.\n",
      " |      \n",
      " |      Note:\n",
      " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      " |          ``RuntimeError``.\n",
      " |  \n",
      " |  modules(self) -> collections.abc.Iterator['Module']\n",
      " |      Return an iterator over all modules in the network.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Module: a module in the network\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          )\n",
      " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      " |  \n",
      " |  mtia(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the MTIA.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing the optimizer if the module will\n",
      " |      live on MTIA while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  named_buffers(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.Tensor]]\n",
      " |      Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all buffer names.\n",
      " |          recurse (bool, optional): if True, then yields buffers of this module\n",
      " |              and all submodules. Otherwise, yields only buffers that\n",
      " |              are direct members of this module. Defaults to True.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated buffers in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, torch.Tensor): Tuple containing the name and buffer\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, buf in self.named_buffers():\n",
      " |          >>>     if name in ['running_var']:\n",
      " |          >>>         print(buf.size())\n",
      " |  \n",
      " |  named_children(self) -> collections.abc.Iterator[tuple[str, 'Module']]\n",
      " |      Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple containing a name and child module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, module in model.named_children():\n",
      " |          >>>     if name in ['conv4', 'conv5']:\n",
      " |          >>>         print(module)\n",
      " |  \n",
      " |  named_modules(self, memo: Optional[set['Module']] = None, prefix: str = '', remove_duplicate: bool = True)\n",
      " |      Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          memo: a memo to store the set of modules already added to the result\n",
      " |          prefix: a prefix that will be added to the name of the module\n",
      " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      " |              or not\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Module): Tuple of name and module\n",
      " |      \n",
      " |      Note:\n",
      " |          Duplicate modules are returned only once. In the following\n",
      " |          example, ``l`` will be returned only once.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> l = nn.Linear(2, 2)\n",
      " |          >>> net = nn.Sequential(l, l)\n",
      " |          >>> for idx, m in enumerate(net.named_modules()):\n",
      " |          ...     print(idx, '->', m)\n",
      " |      \n",
      " |          0 -> ('', Sequential(\n",
      " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      " |          ))\n",
      " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      " |  \n",
      " |  named_parameters(self, prefix: str = '', recurse: bool = True, remove_duplicate: bool = True) -> collections.abc.Iterator[tuple[str, torch.nn.parameter.Parameter]]\n",
      " |      Return an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.\n",
      " |      \n",
      " |      Args:\n",
      " |          prefix (str): prefix to prepend to all parameter names.\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |          remove_duplicate (bool, optional): whether to remove the duplicated\n",
      " |              parameters in the result. Defaults to True.\n",
      " |      \n",
      " |      Yields:\n",
      " |          (str, Parameter): Tuple containing the name and parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for name, param in self.named_parameters():\n",
      " |          >>>     if name in ['bias']:\n",
      " |          >>>         print(param.size())\n",
      " |  \n",
      " |  parameters(self, recurse: bool = True) -> collections.abc.Iterator[torch.nn.parameter.Parameter]\n",
      " |      Return an iterator over module parameters.\n",
      " |      \n",
      " |      This is typically passed to an optimizer.\n",
      " |      \n",
      " |      Args:\n",
      " |          recurse (bool): if True, then yields parameters of this module\n",
      " |              and all submodules. Otherwise, yields only parameters that\n",
      " |              are direct members of this module.\n",
      " |      \n",
      " |      Yields:\n",
      " |          Parameter: module parameter\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> for param in model.parameters():\n",
      " |          >>>     print(type(param), param.size())\n",
      " |          <class 'torch.Tensor'> (20L,)\n",
      " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      " |  \n",
      " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      " |      the behavior of this function will change in future versions.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None\n",
      " |      Add a buffer to the module.\n",
      " |      \n",
      " |      This is typically used to register a buffer that should not be\n",
      " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      " |      is not a parameter, but is part of the module's state. Buffers, by\n",
      " |      default, are persistent and will be saved alongside parameters. This\n",
      " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      " |      only difference between a persistent buffer and a non-persistent buffer\n",
      " |      is that the latter will not be a part of this module's\n",
      " |      :attr:`state_dict`.\n",
      " |      \n",
      " |      Buffers can be accessed as attributes using given names.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the buffer. The buffer can be accessed\n",
      " |              from this module using the given name\n",
      " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      " |          persistent (bool): whether the buffer is part of this module's\n",
      " |              :attr:`state_dict`.\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      " |  \n",
      " |  register_forward_hook(self, hook: Union[Callable[[~T, tuple[Any, ...], Any], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any], Any], Optional[Any]]], *, prepend: bool = False, with_kwargs: bool = False, always_call: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time after :func:`forward` has computed an output.\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``False`` or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      output. It can modify the input inplace but it will not have effect on\n",
      " |      forward since this is called after :func:`forward` is called. The hook\n",
      " |      should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, output) -> None or modified output\n",
      " |      \n",
      " |      If ``with_kwargs`` is ``True``, the forward hook will be passed the\n",
      " |      ``kwargs`` given to the forward function and be expected to return the\n",
      " |      output possibly modified. The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs, output) -> None or modified output\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If ``True``, the provided ``hook`` will be fired\n",
      " |              before all existing ``forward`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward`` hooks on\n",
      " |              this :class:`torch.nn.Module`. Note that global\n",
      " |              ``forward`` hooks registered with\n",
      " |              :func:`register_module_forward_hook` will fire before all hooks\n",
      " |              registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If ``True``, the ``hook`` will be passed the\n",
      " |              kwargs given to the forward function.\n",
      " |              Default: ``False``\n",
      " |          always_call (bool): If ``True`` the ``hook`` will be run regardless of\n",
      " |              whether an exception is raised while calling the Module.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_forward_pre_hook(self, hook: Union[Callable[[~T, tuple[Any, ...]], Optional[Any]], Callable[[~T, tuple[Any, ...], dict[str, Any]], Optional[tuple[Any, dict[str, Any]]]]], *, prepend: bool = False, with_kwargs: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a forward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time before :func:`forward` is invoked.\n",
      " |      \n",
      " |      \n",
      " |      If ``with_kwargs`` is false or not specified, the input contains only\n",
      " |      the positional arguments given to the module. Keyword arguments won't be\n",
      " |      passed to the hooks and only to the ``forward``. The hook can modify the\n",
      " |      input. User can either return a tuple or a single modified value in the\n",
      " |      hook. We will wrap the value into a tuple if a single value is returned\n",
      " |      (unless that value is already a tuple). The hook should have the\n",
      " |      following signature::\n",
      " |      \n",
      " |          hook(module, args) -> None or modified input\n",
      " |      \n",
      " |      If ``with_kwargs`` is true, the forward pre-hook will be passed the\n",
      " |      kwargs given to the forward function. And if the hook modifies the\n",
      " |      input, both the args and kwargs should be returned. The hook should have\n",
      " |      the following signature::\n",
      " |      \n",
      " |          hook(module, args, kwargs) -> None or a tuple of modified input and kwargs\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``forward_pre`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``forward_pre`` hooks\n",
      " |              on this :class:`torch.nn.Module`. Note that global\n",
      " |              ``forward_pre`` hooks registered with\n",
      " |              :func:`register_module_forward_pre_hook` will fire before all\n",
      " |              hooks registered by this method.\n",
      " |              Default: ``False``\n",
      " |          with_kwargs (bool): If true, the ``hook`` will be passed the kwargs\n",
      " |              given to the forward function.\n",
      " |              Default: ``False``\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor], Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\n",
      " |      \n",
      " |          1. Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.\n",
      " |          2. If none of the module inputs require gradients, the hook will fire when the gradients are computed\n",
      " |             with respect to module outputs.\n",
      " |          3. If none of the module outputs require gradients, then the hooks will not fire.\n",
      " |      \n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      " |      \n",
      " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      " |      with respect to the inputs and outputs respectively. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      " |      arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward`` hooks on\n",
      " |              this :class:`torch.nn.Module`. Note that global\n",
      " |              ``backward`` hooks registered with\n",
      " |              :func:`register_module_full_backward_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_full_backward_pre_hook(self, hook: Callable[[ForwardRef('Module'), Union[tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, tuple[torch.Tensor, ...], torch.Tensor]], prepend: bool = False) -> torch.utils.hooks.RemovableHandle\n",
      " |      Register a backward pre-hook on the module.\n",
      " |      \n",
      " |      The hook will be called every time the gradients for the module are computed.\n",
      " |      The hook should have the following signature::\n",
      " |      \n",
      " |          hook(module, grad_output) -> tuple[Tensor] or None\n",
      " |      \n",
      " |      The :attr:`grad_output` is a tuple. The hook should\n",
      " |      not modify its arguments, but it can optionally return a new gradient with\n",
      " |      respect to the output that will be used in place of :attr:`grad_output` in\n",
      " |      subsequent computations. Entries in :attr:`grad_output` will be ``None`` for\n",
      " |      all non-Tensor arguments.\n",
      " |      \n",
      " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      " |      of each Tensor returned by the Module's forward function.\n",
      " |      \n",
      " |      .. warning ::\n",
      " |          Modifying inputs inplace is not allowed when using backward hooks and\n",
      " |          will raise an error.\n",
      " |      \n",
      " |      Args:\n",
      " |          hook (Callable): The user-defined hook to be registered.\n",
      " |          prepend (bool): If true, the provided ``hook`` will be fired before\n",
      " |              all existing ``backward_pre`` hooks on this\n",
      " |              :class:`torch.nn.Module`. Otherwise, the provided\n",
      " |              ``hook`` will be fired after all existing ``backward_pre`` hooks\n",
      " |              on this :class:`torch.nn.Module`. Note that global\n",
      " |              ``backward_pre`` hooks registered with\n",
      " |              :func:`register_module_full_backward_pre_hook` will fire before\n",
      " |              all hooks registered by this method.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook to be run after module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, incompatible_keys) -> None\n",
      " |      \n",
      " |      The ``module`` argument is the current module that this hook is registered\n",
      " |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting\n",
      " |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``\n",
      " |      is a ``list`` of ``str`` containing the missing keys and\n",
      " |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.\n",
      " |      \n",
      " |      The given incompatible_keys can be modified inplace if needed.\n",
      " |      \n",
      " |      Note that the checks performed when calling :func:`load_state_dict` with\n",
      " |      ``strict=True`` are affected by modifications the hook makes to\n",
      " |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either\n",
      " |      set of keys will result in an error being thrown when ``strict=True``, and\n",
      " |      clearing out both missing and unexpected keys will avoid an error.\n",
      " |      \n",
      " |      Returns:\n",
      " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      " |              a handle that can be used to remove the added hook by calling\n",
      " |              ``handle.remove()``\n",
      " |  \n",
      " |  register_load_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook to be run before module's :meth:`~nn.Module.load_state_dict` is called.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None  # noqa: B950\n",
      " |      \n",
      " |      Arguments:\n",
      " |          hook (Callable): Callable hook that will be invoked before\n",
      " |              loading the state dict.\n",
      " |  \n",
      " |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None\n",
      " |      Alias for :func:`add_module`.\n",
      " |  \n",
      " |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None\n",
      " |      Add a parameter to the module.\n",
      " |      \n",
      " |      The parameter can be accessed as an attribute using given name.\n",
      " |      \n",
      " |      Args:\n",
      " |          name (str): name of the parameter. The parameter can be accessed\n",
      " |              from this module using the given name\n",
      " |          param (Parameter or None): parameter to be added to the module. If\n",
      " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      " |              are ignored. If ``None``, the parameter is **not** included in the\n",
      " |              module's :attr:`state_dict`.\n",
      " |  \n",
      " |  register_state_dict_post_hook(self, hook)\n",
      " |      Register a post-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, state_dict, prefix, local_metadata) -> None\n",
      " |      \n",
      " |      The registered hooks can modify the ``state_dict`` inplace.\n",
      " |  \n",
      " |  register_state_dict_pre_hook(self, hook)\n",
      " |      Register a pre-hook for the :meth:`~torch.nn.Module.state_dict` method.\n",
      " |      \n",
      " |      It should have the following signature::\n",
      " |          hook(module, prefix, keep_vars) -> None\n",
      " |      \n",
      " |      The registered hooks can be used to perform pre-processing before the ``state_dict``\n",
      " |      call is made.\n",
      " |  \n",
      " |  requires_grad_(self, requires_grad: bool = True) -> Self\n",
      " |      Change if autograd should record operations on parameters in this module.\n",
      " |      \n",
      " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      " |      in-place.\n",
      " |      \n",
      " |      This method is helpful for freezing part of the module for finetuning\n",
      " |      or training parts of a model individually (e.g., GAN training).\n",
      " |      \n",
      " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      " |      \n",
      " |      Args:\n",
      " |          requires_grad (bool): whether autograd should record operations on\n",
      " |                                parameters in this module. Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  set_extra_state(self, state: Any) -> None\n",
      " |      Set extra state contained in the loaded `state_dict`.\n",
      " |      \n",
      " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      " |      found within the `state_dict`. Implement this function and a corresponding\n",
      " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      " |      `state_dict`.\n",
      " |      \n",
      " |      Args:\n",
      " |          state (dict): Extra state from the `state_dict`\n",
      " |  \n",
      " |  set_submodule(self, target: str, module: 'Module', strict: bool = False) -> None\n",
      " |      Set the submodule given by ``target`` if it exists, otherwise throw an error.\n",
      " |      \n",
      " |      .. note::\n",
      " |          If ``strict`` is set to ``False`` (default), the method will replace an existing submodule\n",
      " |          or create a new submodule if the parent module exists. If ``strict`` is set to ``True``,\n",
      " |          the method will only attempt to replace an existing submodule and throw an error if\n",
      " |          the submodule does not exist.\n",
      " |      \n",
      " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      " |      looks like this:\n",
      " |      \n",
      " |      .. code-block:: text\n",
      " |      \n",
      " |          A(\n",
      " |              (net_b): Module(\n",
      " |                  (net_c): Module(\n",
      " |                      (conv): Conv2d(3, 3, 3)\n",
      " |                  )\n",
      " |                  (linear): Linear(3, 3)\n",
      " |              )\n",
      " |          )\n",
      " |      \n",
      " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      " |      \n",
      " |      To override the ``Conv2d`` with a new submodule ``Linear``, you\n",
      " |      could call ``set_submodule(\"net_b.net_c.conv\", nn.Linear(1, 1))``\n",
      " |      where ``strict`` could be ``True`` or ``False``\n",
      " |      \n",
      " |      To add a new submodule ``Conv2d`` to the existing ``net_b`` module,\n",
      " |      you would call ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1))``.\n",
      " |      \n",
      " |      In the above if you set ``strict=True`` and call\n",
      " |      ``set_submodule(\"net_b.conv\", nn.Conv2d(1, 1, 1), strict=True)``, an AttributeError\n",
      " |      will be raised because ``net_b`` does not have a submodule named ``conv``.\n",
      " |      \n",
      " |      Args:\n",
      " |          target: The fully-qualified string name of the submodule\n",
      " |              to look for. (See above example for how to specify a\n",
      " |              fully-qualified string.)\n",
      " |          module: The module to set the submodule to.\n",
      " |          strict: If ``False``, the method will replace an existing submodule\n",
      " |              or create a new submodule if the parent module exists. If ``True``,\n",
      " |              the method will only attempt to replace an existing submodule and throw an error\n",
      " |              if the submodule doesn't already exist.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the ``target`` string is empty or if ``module`` is not an instance of ``nn.Module``.\n",
      " |          AttributeError: If at any point along the path resulting from\n",
      " |              the ``target`` string the (sub)path resolves to a non-existent\n",
      " |              attribute name or an object that is not an instance of ``nn.Module``.\n",
      " |  \n",
      " |  share_memory(self) -> Self\n",
      " |      See :meth:`torch.Tensor.share_memory_`.\n",
      " |  \n",
      " |  state_dict(self, *args, destination=None, prefix='', keep_vars=False)\n",
      " |      Return a dictionary containing references to the whole state of the module.\n",
      " |      \n",
      " |      Both parameters and persistent buffers (e.g. running averages) are\n",
      " |      included. Keys are corresponding parameter and buffer names.\n",
      " |      Parameters and buffers set to ``None`` are not included.\n",
      " |      \n",
      " |      .. note::\n",
      " |          The returned object is a shallow copy. It contains references\n",
      " |          to the module's parameters and buffers.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Currently ``state_dict()`` also accepts positional arguments for\n",
      " |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,\n",
      " |          this is being deprecated and keyword arguments will be enforced in\n",
      " |          future releases.\n",
      " |      \n",
      " |      .. warning::\n",
      " |          Please avoid the use of argument ``destination`` as it is not\n",
      " |          designed for end-users.\n",
      " |      \n",
      " |      Args:\n",
      " |          destination (dict, optional): If provided, the state of module will\n",
      " |              be updated into the dict and the same object is returned.\n",
      " |              Otherwise, an ``OrderedDict`` will be created and returned.\n",
      " |              Default: ``None``.\n",
      " |          prefix (str, optional): a prefix added to parameter and buffer\n",
      " |              names to compose the keys in state_dict. Default: ``''``.\n",
      " |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s\n",
      " |              returned in the state dict are detached from autograd. If it's\n",
      " |              set to ``True``, detaching will not be performed.\n",
      " |              Default: ``False``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict:\n",
      " |              a dictionary containing a whole state of the module\n",
      " |      \n",
      " |      Example::\n",
      " |      \n",
      " |          >>> # xdoctest: +SKIP(\"undefined vars\")\n",
      " |          >>> module.state_dict().keys()\n",
      " |          ['bias', 'weight']\n",
      " |  \n",
      " |  to(self, *args, **kwargs)\n",
      " |      Move and/or cast the parameters and buffers.\n",
      " |      \n",
      " |      This can be called as\n",
      " |      \n",
      " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(dtype, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(tensor, non_blocking=False)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      .. function:: to(memory_format=torch.channels_last)\n",
      " |         :noindex:\n",
      " |      \n",
      " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      " |      (if given). The integral parameters and buffers will be moved\n",
      " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      " |      pinned memory to CUDA devices.\n",
      " |      \n",
      " |      See below for examples.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): the desired device of the parameters\n",
      " |              and buffers in this module\n",
      " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      " |              the parameters and buffers in this module\n",
      " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      " |              dtype and device for all parameters and buffers in this module\n",
      " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      " |              format for 4D parameters and buffers in this module (keyword\n",
      " |              only argument)\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |      \n",
      " |      Examples::\n",
      " |      \n",
      " |          >>> # xdoctest: +IGNORE_WANT(\"non-deterministic\")\n",
      " |          >>> linear = nn.Linear(2, 2)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]])\n",
      " |          >>> linear.to(torch.double)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1913, -0.3420],\n",
      " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      " |          >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA1)\n",
      " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      " |          >>> cpu = torch.device(\"cpu\")\n",
      " |          >>> linear.to(cpu)\n",
      " |          Linear(in_features=2, out_features=2, bias=True)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.1914, -0.3420],\n",
      " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      " |      \n",
      " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      " |          >>> linear.weight\n",
      " |          Parameter containing:\n",
      " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j],\n",
      " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      " |  \n",
      " |  to_empty(self, *, device: Union[int, str, torch.device, NoneType], recurse: bool = True) -> Self\n",
      " |      Move the parameters and buffers to the specified device without copying storage.\n",
      " |      \n",
      " |      Args:\n",
      " |          device (:class:`torch.device`): The desired device of the parameters\n",
      " |              and buffers in this module.\n",
      " |          recurse (bool): Whether parameters and buffers of submodules should\n",
      " |              be recursively moved to the specified device.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  train(self, mode: bool = True) -> Self\n",
      " |      Set the module in training mode.\n",
      " |      \n",
      " |      This has an effect only on certain modules. See the documentation of\n",
      " |      particular modules for details of their behaviors in training/evaluation\n",
      " |      mode, i.e., whether they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      " |      etc.\n",
      " |      \n",
      " |      Args:\n",
      " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      " |                       mode (``False``). Default: ``True``.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  type(self, dst_type: Union[torch.dtype, str]) -> Self\n",
      " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Args:\n",
      " |          dst_type (type or string): the desired type\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  xpu(self, device: Union[torch.device, int, NoneType] = None) -> Self\n",
      " |      Move all model parameters and buffers to the XPU.\n",
      " |      \n",
      " |      This also makes associated parameters and buffers different objects. So\n",
      " |      it should be called before constructing optimizer if the module will\n",
      " |      live on XPU while being optimized.\n",
      " |      \n",
      " |      .. note::\n",
      " |          This method modifies the module in-place.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          device (int, optional): if specified, all parameters will be\n",
      " |              copied to that device\n",
      " |      \n",
      " |      Returns:\n",
      " |          Module: self\n",
      " |  \n",
      " |  zero_grad(self, set_to_none: bool = True) -> None\n",
      " |      Reset gradients of all model parameters.\n",
      " |      \n",
      " |      See similar function under :class:`torch.optim.Optimizer` for more context.\n",
      " |      \n",
      " |      Args:\n",
      " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      " |  \n",
      " |  T_destination = ~T_destination\n",
      " |  \n",
      " |  call_super_init = False\n",
      " |  \n",
      " |  dump_patches = False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "help(nn.Linear)\n",
    "#print(inspect.signature(nn.Module.__init__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21c2e6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fakename(nn.Linear):\n",
    "    def __init__(self, name, in_features, out_features):\n",
    "        super().__init__(in_features, out_features)\n",
    "        self.name = name\n",
    "\n",
    "me = fakename(\"Carlos\", 4, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# ğŸ“˜ **LogosTransformer â€” A GPT-Style Language Model Trained on Philosophy Texts**

## ğŸ§  Overview
**LogosTransformer** is a miniature GPT-style transformer model trained **from scratch** in PyTorch using a **character-level tokenizer** and a corpus of philosophical writings (Aristotle, Socrates, Kant, Descartes, etc.).

This project is built entirely inside a Jupyter Notebook for transparency and educational value â€” no HuggingFace, no shortcuts.  
By implementing attention, feed-forward layers, positional embeddings, and sampling logic from the ground up, this project demonstrates a working understanding of how GPT-style large language models operate internally.

The most compelling outcome is how **coherent, philosophy-inspired language emerges** from purely **character-level** training.

---

# ğŸš€ Features

### ğŸ”¡ Character-Level Tokenization  
- Vocabulary consists of unique characters from the corpus  
- Demonstrates how structure and meaning arise from minimal tokenization  
- Enables training on arbitrarily large text files

### ğŸ”¥ Full Transformer Implementation (No External Libraries)
Implemented manually:
- Scaled dot-product attention  
- Multi-head attention  
- Feed-forward layers  
- Layer normalization  
- Residual connections  
- Learned positional embeddings  
- Training loop w/ optimizer, scheduler, and loss estimation  
- Checkpoint saving + loading  

### âœï¸ Text Generation
- Temperature-controlled sampling  
- Produces increasingly coherent output as training progresses  
- Captures philosophical tone despite char-level encoding

---

# ğŸ“‰ Training Progress

The model shows strong learning behaviour, rapidly improving in the first few hundred steps and continuing to converge smoothly.

<p align="center">
  <img src="./assets/loss_curve.png" width="600">
</p>

---

# âœï¸ Emergence of Coherent Philosophical Language

### **ğŸ§ª Output at Start of Training**

```
Hello! Can you see me?Mf5EjÃ¦â€™Pb-S8wx 4!9DKâ€”wiPdjfouJ,L"p7WxVx7fBHtU 7i]HRaEx,ï»¿TIHJGXÃ¦â€”eIï»¿k5p7â€”;N*â€qT=expR!YWBqEvt"]UM?:]ï»¿!-;IqlWr5e?RelxlgNr?f9'â€™eseeq]I0*AClX??qWw;)7;lJâ€™l'Ygp':f?;JlgU=BQYZ'U?JHuZ?5RaLâ€œFcï»¿R7Zl?3xJHÃ¦b2;rgI
```

---

### **ğŸ§ª Output at Step 2000**

```
Hello! Can you see me? I ought to be absolute to one, and when he
can say that I can see to be considered him, to a concerned would as
the money, then, say, although they do not allow those who examine
the same are objects
```

---

# ğŸ§© Architecture Summary

```
Token Embeddings  
+ Positional Embeddings
â†’ Multi-Head Self-Attention
â†’ Feedforward Network
â†’ LayerNorm + Residuals
â†’ Linear Projection
â†’ Softmax over next-token distribution
```

---

# ğŸ“ Repository Structure

```
â”œâ”€â”€ assets/
â”‚   â””â”€â”€ loss_curve.png
â”œâ”€â”€ data/
â”‚   â””â”€â”€ philosophers.txt
â”œâ”€â”€ gpt-mk03-updated3.ipynb        # Main training notebook
â”œâ”€â”€ checkpoints/                   # Optional saved weights
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

# âš™ï¸ Hyperparameters Used

```python
block_size    = 64
batch_size    = 64
learning_rate = 3e-4

n_embd  = 384
n_head  = 8
n_layer = 4
dropout = 0.2

max_iters = 2000
```

---

# ğŸ”® Next Steps

### **1ï¸âƒ£ Fine-Tuning Framework**
### **2ï¸âƒ£ Upgrade Tokenization**
### **3ï¸âƒ£ Expand Model Depth**
### **4ï¸âƒ£ Build an Interactive Web Demo**
### **5ï¸âƒ£ Add Evaluation Tools**

---

# ğŸ“œ License
MIT License

---

# ğŸ™Œ Acknowledgements
Model designed and implemented entirely by **Carlos Santa**.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351b7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import mmap\n",
    "import random\n",
    "import pickle\n",
    "#import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a83fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = (\n",
    "    'cuda' if torch.cuda.is_available()\n",
    "    else 'mps' if torch.backends.mps.is_available()\n",
    "    else 'cpu'\n",
    ")\n",
    "\n",
    "batch_size = 8\n",
    "block_size = 32\n",
    "max_iters = 1000\n",
    "learning_rate = 1e-4\n",
    "eval_iters = 100\n",
    "n_embd = 128\n",
    "n_head = 8\n",
    "n_layer = 1\n",
    "dropout = 0.2\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "516b1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = \"\"\n",
    "with open(\"vocab.txt\", 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        chars = sorted(list(set(text)))\n",
    "        \n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "506ad003",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = { ch:i for i,ch in enumerate(chars) }\n",
    "int_to_string = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [string_to_int[c] for c in s]\n",
    "decode = lambda l: ''.join([int_to_string[i] for i in l])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c5efd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory map for using small snippets of text from a single file of any size\n",
    "def get_random_chunk(split):\n",
    "    filename = \"wizard_of_oz.txt\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # Determine the file size and a random position to start reading\n",
    "            file_size = len(mm)\n",
    "            start_pos = random.randint(0, (file_size) - block_size*batch_size)\n",
    "\n",
    "            # Seek to the random position and read the block of text\n",
    "            mm.seek(start_pos)\n",
    "            block = mm.read(block_size*batch_size-1)\n",
    "\n",
    "            # Decode the block to a string, ignoring any invalid byte sequences\n",
    "            decoded_block = block.decode('utf-8', errors='ignore').replace('\\r', '')\n",
    "            \n",
    "            # Train and test splits\n",
    "            data = torch.tensor(encode(decoded_block), dtype=torch.long)\n",
    "            \n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(split):\n",
    "    data = get_random_chunk(split)\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3838bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, F) -> (B, T, [h1, h1, h1, h1, h2, h2, h2, h2, h3, h3, h3, h3])\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.sa(x)\n",
    "        x = self.ln1(x + y)\n",
    "        y = self.ffwd(x)\n",
    "        x = self.ln2(x + y)\n",
    "        return x\n",
    "    \n",
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        B, T = index.shape\n",
    "        \n",
    "        \n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(index) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # index is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            index_cond = index[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(index_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            index = torch.cat((index, index_next), dim=1) # (B, T+1)\n",
    "        return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96f1d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTLanguageModel(vocab_size)\n",
    "\n",
    "m = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d4a0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a1c38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "step: 0, train loss: 8.657, val loss: 8.655\n",
      "1\n",
      "step: 1, train loss: 8.647, val loss: 8.646\n",
      "2\n",
      "step: 2, train loss: 8.633, val loss: 8.632\n",
      "3\n",
      "step: 3, train loss: 8.620, val loss: 8.620\n",
      "4\n",
      "step: 4, train loss: 8.607, val loss: 8.610\n",
      "5\n",
      "step: 5, train loss: 8.597, val loss: 8.599\n",
      "6\n",
      "step: 6, train loss: 8.586, val loss: 8.585\n",
      "7\n",
      "step: 7, train loss: 8.576, val loss: 8.580\n",
      "8\n",
      "step: 8, train loss: 8.560, val loss: 8.562\n",
      "9\n",
      "step: 9, train loss: 8.550, val loss: 8.553\n",
      "10\n",
      "step: 10, train loss: 8.537, val loss: 8.537\n",
      "11\n",
      "step: 11, train loss: 8.525, val loss: 8.525\n",
      "12\n",
      "step: 12, train loss: 8.512, val loss: 8.512\n",
      "13\n",
      "step: 13, train loss: 8.499, val loss: 8.499\n",
      "14\n",
      "step: 14, train loss: 8.488, val loss: 8.489\n",
      "15\n",
      "step: 15, train loss: 8.473, val loss: 8.476\n",
      "16\n",
      "step: 16, train loss: 8.462, val loss: 8.459\n",
      "17\n",
      "step: 17, train loss: 8.448, val loss: 8.446\n",
      "18\n",
      "step: 18, train loss: 8.434, val loss: 8.434\n",
      "19\n",
      "step: 19, train loss: 8.418, val loss: 8.416\n",
      "20\n",
      "step: 20, train loss: 8.404, val loss: 8.404\n",
      "21\n",
      "step: 21, train loss: 8.390, val loss: 8.390\n",
      "22\n",
      "step: 22, train loss: 8.375, val loss: 8.377\n",
      "23\n",
      "step: 23, train loss: 8.358, val loss: 8.357\n",
      "24\n",
      "step: 24, train loss: 8.342, val loss: 8.339\n",
      "25\n",
      "step: 25, train loss: 8.324, val loss: 8.325\n",
      "26\n",
      "step: 26, train loss: 8.306, val loss: 8.306\n",
      "27\n",
      "step: 27, train loss: 8.289, val loss: 8.289\n",
      "28\n",
      "step: 28, train loss: 8.270, val loss: 8.264\n",
      "29\n",
      "step: 29, train loss: 8.245, val loss: 8.249\n",
      "30\n",
      "step: 30, train loss: 8.233, val loss: 8.231\n",
      "31\n",
      "step: 31, train loss: 8.211, val loss: 8.212\n",
      "32\n",
      "step: 32, train loss: 8.189, val loss: 8.188\n",
      "33\n",
      "step: 33, train loss: 8.168, val loss: 8.168\n",
      "34\n",
      "step: 34, train loss: 8.138, val loss: 8.143\n",
      "35\n",
      "step: 35, train loss: 8.119, val loss: 8.122\n",
      "36\n",
      "step: 36, train loss: 8.099, val loss: 8.095\n",
      "37\n",
      "step: 37, train loss: 8.076, val loss: 8.072\n",
      "38\n",
      "step: 38, train loss: 8.043, val loss: 8.049\n",
      "39\n",
      "step: 39, train loss: 8.023, val loss: 8.020\n",
      "40\n",
      "step: 40, train loss: 7.998, val loss: 7.998\n",
      "41\n",
      "step: 41, train loss: 7.968, val loss: 7.970\n",
      "42\n",
      "step: 42, train loss: 7.940, val loss: 7.943\n",
      "43\n",
      "step: 43, train loss: 7.903, val loss: 7.911\n",
      "44\n",
      "step: 44, train loss: 7.882, val loss: 7.878\n",
      "45\n",
      "step: 45, train loss: 7.848, val loss: 7.848\n",
      "46\n",
      "step: 46, train loss: 7.815, val loss: 7.816\n",
      "47\n",
      "step: 47, train loss: 7.785, val loss: 7.781\n",
      "48\n",
      "step: 48, train loss: 7.746, val loss: 7.745\n",
      "49\n",
      "step: 49, train loss: 7.706, val loss: 7.711\n",
      "50\n",
      "step: 50, train loss: 7.667, val loss: 7.672\n",
      "51\n",
      "step: 51, train loss: 7.633, val loss: 7.644\n",
      "52\n",
      "step: 52, train loss: 7.596, val loss: 7.606\n",
      "53\n",
      "step: 53, train loss: 7.565, val loss: 7.564\n",
      "54\n",
      "step: 54, train loss: 7.521, val loss: 7.526\n",
      "55\n",
      "step: 55, train loss: 7.478, val loss: 7.482\n",
      "56\n",
      "step: 56, train loss: 7.438, val loss: 7.446\n",
      "57\n",
      "step: 57, train loss: 7.405, val loss: 7.406\n",
      "58\n",
      "step: 58, train loss: 7.355, val loss: 7.371\n",
      "59\n",
      "step: 59, train loss: 7.326, val loss: 7.316\n",
      "60\n",
      "step: 60, train loss: 7.288, val loss: 7.277\n",
      "61\n",
      "step: 61, train loss: 7.242, val loss: 7.240\n",
      "62\n",
      "step: 62, train loss: 7.190, val loss: 7.196\n",
      "63\n",
      "step: 63, train loss: 7.156, val loss: 7.157\n",
      "64\n",
      "step: 64, train loss: 7.115, val loss: 7.124\n",
      "65\n",
      "step: 65, train loss: 7.076, val loss: 7.077\n",
      "66\n",
      "step: 66, train loss: 7.032, val loss: 7.025\n",
      "67\n",
      "step: 67, train loss: 6.994, val loss: 6.988\n",
      "68\n",
      "step: 68, train loss: 6.954, val loss: 6.950\n",
      "69\n",
      "step: 69, train loss: 6.913, val loss: 6.919\n",
      "70\n",
      "step: 70, train loss: 6.876, val loss: 6.866\n",
      "71\n",
      "step: 71, train loss: 6.832, val loss: 6.832\n",
      "72\n",
      "step: 72, train loss: 6.797, val loss: 6.801\n",
      "73\n",
      "step: 73, train loss: 6.771, val loss: 6.753\n",
      "74\n",
      "step: 74, train loss: 6.727, val loss: 6.719\n",
      "75\n",
      "step: 75, train loss: 6.694, val loss: 6.695\n",
      "76\n",
      "step: 76, train loss: 6.660, val loss: 6.652\n",
      "77\n",
      "step: 77, train loss: 6.607, val loss: 6.621\n",
      "78\n",
      "step: 78, train loss: 6.581, val loss: 6.589\n",
      "79\n",
      "step: 79, train loss: 6.540, val loss: 6.551\n",
      "80\n",
      "step: 80, train loss: 6.510, val loss: 6.500\n",
      "81\n",
      "step: 81, train loss: 6.481, val loss: 6.488\n",
      "82\n",
      "step: 82, train loss: 6.440, val loss: 6.448\n",
      "83\n",
      "step: 83, train loss: 6.411, val loss: 6.405\n",
      "84\n",
      "step: 84, train loss: 6.376, val loss: 6.390\n",
      "85\n",
      "step: 85, train loss: 6.326, val loss: 6.335\n",
      "86\n",
      "step: 86, train loss: 6.296, val loss: 6.307\n",
      "87\n",
      "step: 87, train loss: 6.281, val loss: 6.285\n",
      "88\n",
      "step: 88, train loss: 6.241, val loss: 6.240\n",
      "89\n",
      "step: 89, train loss: 6.217, val loss: 6.227\n",
      "90\n",
      "step: 90, train loss: 6.181, val loss: 6.192\n",
      "91\n",
      "step: 91, train loss: 6.145, val loss: 6.162\n",
      "92\n",
      "step: 92, train loss: 6.113, val loss: 6.103\n",
      "93\n",
      "step: 93, train loss: 6.090, val loss: 6.082\n",
      "94\n",
      "step: 94, train loss: 6.053, val loss: 6.059\n",
      "95\n",
      "step: 95, train loss: 6.019, val loss: 6.028\n",
      "96\n",
      "step: 96, train loss: 5.985, val loss: 5.989\n",
      "97\n",
      "step: 97, train loss: 5.962, val loss: 5.961\n",
      "98\n",
      "step: 98, train loss: 5.924, val loss: 5.942\n",
      "99\n",
      "step: 99, train loss: 5.908, val loss: 5.899\n",
      "100\n",
      "step: 100, train loss: 5.858, val loss: 5.860\n",
      "101\n",
      "step: 101, train loss: 5.830, val loss: 5.847\n",
      "102\n",
      "step: 102, train loss: 5.817, val loss: 5.812\n",
      "103\n",
      "step: 103, train loss: 5.782, val loss: 5.785\n",
      "104\n",
      "step: 104, train loss: 5.757, val loss: 5.759\n",
      "105\n",
      "step: 105, train loss: 5.704, val loss: 5.724\n",
      "106\n",
      "step: 106, train loss: 5.689, val loss: 5.699\n",
      "107\n",
      "step: 107, train loss: 5.655, val loss: 5.650\n",
      "108\n",
      "step: 108, train loss: 5.625, val loss: 5.614\n",
      "109\n",
      "step: 109, train loss: 5.604, val loss: 5.612\n",
      "110\n",
      "step: 110, train loss: 5.570, val loss: 5.557\n",
      "111\n",
      "step: 111, train loss: 5.541, val loss: 5.547\n",
      "112\n",
      "step: 112, train loss: 5.533, val loss: 5.510\n",
      "113\n",
      "step: 113, train loss: 5.487, val loss: 5.491\n",
      "114\n",
      "step: 114, train loss: 5.457, val loss: 5.461\n",
      "115\n",
      "step: 115, train loss: 5.436, val loss: 5.433\n",
      "116\n",
      "step: 116, train loss: 5.415, val loss: 5.415\n",
      "117\n",
      "step: 117, train loss: 5.373, val loss: 5.374\n",
      "118\n",
      "step: 118, train loss: 5.354, val loss: 5.353\n",
      "119\n",
      "step: 119, train loss: 5.324, val loss: 5.308\n",
      "120\n",
      "step: 120, train loss: 5.292, val loss: 5.279\n",
      "121\n",
      "step: 121, train loss: 5.276, val loss: 5.278\n",
      "122\n",
      "step: 122, train loss: 5.235, val loss: 5.239\n",
      "123\n",
      "step: 123, train loss: 5.216, val loss: 5.219\n",
      "124\n",
      "step: 124, train loss: 5.180, val loss: 5.176\n",
      "125\n",
      "step: 125, train loss: 5.162, val loss: 5.151\n",
      "126\n",
      "step: 126, train loss: 5.145, val loss: 5.135\n",
      "127\n",
      "step: 127, train loss: 5.114, val loss: 5.094\n",
      "128\n",
      "step: 128, train loss: 5.096, val loss: 5.081\n",
      "129\n",
      "step: 129, train loss: 5.066, val loss: 5.064\n",
      "130\n",
      "step: 130, train loss: 5.016, val loss: 5.014\n",
      "131\n",
      "step: 131, train loss: 5.018, val loss: 4.993\n",
      "132\n",
      "step: 132, train loss: 4.988, val loss: 4.976\n",
      "133\n",
      "step: 133, train loss: 4.962, val loss: 4.970\n",
      "134\n",
      "step: 134, train loss: 4.935, val loss: 4.940\n",
      "135\n",
      "step: 135, train loss: 4.899, val loss: 4.908\n",
      "136\n",
      "step: 136, train loss: 4.877, val loss: 4.875\n",
      "137\n",
      "step: 137, train loss: 4.856, val loss: 4.855\n",
      "138\n",
      "step: 138, train loss: 4.839, val loss: 4.839\n",
      "139\n",
      "step: 139, train loss: 4.823, val loss: 4.817\n",
      "140\n",
      "step: 140, train loss: 4.785, val loss: 4.816\n",
      "141\n",
      "step: 141, train loss: 4.756, val loss: 4.761\n",
      "142\n",
      "step: 142, train loss: 4.733, val loss: 4.740\n",
      "143\n",
      "step: 143, train loss: 4.714, val loss: 4.750\n",
      "144\n",
      "step: 144, train loss: 4.692, val loss: 4.708\n",
      "145\n",
      "step: 145, train loss: 4.686, val loss: 4.673\n",
      "146\n",
      "step: 146, train loss: 4.638, val loss: 4.653\n",
      "147\n",
      "step: 147, train loss: 4.612, val loss: 4.632\n",
      "148\n",
      "step: 148, train loss: 4.610, val loss: 4.616\n",
      "149\n",
      "step: 149, train loss: 4.579, val loss: 4.591\n",
      "150\n",
      "step: 150, train loss: 4.557, val loss: 4.569\n",
      "151\n",
      "step: 151, train loss: 4.540, val loss: 4.544\n",
      "152\n",
      "step: 152, train loss: 4.508, val loss: 4.517\n",
      "153\n",
      "step: 153, train loss: 4.504, val loss: 4.489\n",
      "154\n",
      "step: 154, train loss: 4.486, val loss: 4.491\n",
      "155\n",
      "step: 155, train loss: 4.474, val loss: 4.454\n",
      "156\n",
      "step: 156, train loss: 4.430, val loss: 4.429\n",
      "157\n",
      "step: 157, train loss: 4.399, val loss: 4.403\n",
      "158\n",
      "step: 158, train loss: 4.403, val loss: 4.392\n",
      "159\n",
      "step: 159, train loss: 4.387, val loss: 4.385\n",
      "160\n",
      "step: 160, train loss: 4.346, val loss: 4.350\n",
      "161\n",
      "step: 161, train loss: 4.332, val loss: 4.331\n",
      "162\n",
      "step: 162, train loss: 4.312, val loss: 4.319\n",
      "163\n",
      "step: 163, train loss: 4.311, val loss: 4.325\n",
      "164\n",
      "step: 164, train loss: 4.286, val loss: 4.282\n",
      "165\n",
      "step: 165, train loss: 4.267, val loss: 4.251\n",
      "166\n",
      "step: 166, train loss: 4.241, val loss: 4.270\n",
      "167\n",
      "step: 167, train loss: 4.232, val loss: 4.259\n",
      "168\n",
      "step: 168, train loss: 4.201, val loss: 4.216\n",
      "169\n",
      "step: 169, train loss: 4.200, val loss: 4.200\n",
      "170\n",
      "step: 170, train loss: 4.149, val loss: 4.180\n",
      "171\n",
      "step: 171, train loss: 4.154, val loss: 4.162\n",
      "172\n",
      "step: 172, train loss: 4.124, val loss: 4.135\n",
      "173\n",
      "step: 173, train loss: 4.132, val loss: 4.131\n",
      "174\n",
      "step: 174, train loss: 4.117, val loss: 4.083\n",
      "175\n",
      "step: 175, train loss: 4.104, val loss: 4.088\n",
      "176\n",
      "step: 176, train loss: 4.080, val loss: 4.078\n",
      "177\n",
      "step: 177, train loss: 4.054, val loss: 4.057\n",
      "178\n",
      "step: 178, train loss: 4.060, val loss: 4.050\n",
      "179\n",
      "step: 179, train loss: 4.037, val loss: 4.026\n",
      "180\n",
      "step: 180, train loss: 4.000, val loss: 3.992\n",
      "181\n",
      "step: 181, train loss: 4.004, val loss: 3.996\n",
      "182\n",
      "step: 182, train loss: 3.973, val loss: 4.007\n",
      "183\n",
      "step: 183, train loss: 3.979, val loss: 3.962\n",
      "184\n",
      "step: 184, train loss: 3.948, val loss: 3.948\n",
      "185\n",
      "step: 185, train loss: 3.933, val loss: 3.945\n",
      "186\n",
      "step: 186, train loss: 3.930, val loss: 3.931\n",
      "187\n",
      "step: 187, train loss: 3.912, val loss: 3.908\n",
      "188\n",
      "step: 188, train loss: 3.904, val loss: 3.905\n",
      "189\n",
      "step: 189, train loss: 3.870, val loss: 3.892\n",
      "190\n",
      "step: 190, train loss: 3.867, val loss: 3.874\n",
      "191\n",
      "step: 191, train loss: 3.828, val loss: 3.853\n",
      "192\n",
      "step: 192, train loss: 3.833, val loss: 3.865\n",
      "193\n",
      "step: 193, train loss: 3.842, val loss: 3.845\n",
      "194\n",
      "step: 194, train loss: 3.823, val loss: 3.824\n",
      "195\n",
      "step: 195, train loss: 3.816, val loss: 3.815\n",
      "196\n",
      "step: 196, train loss: 3.805, val loss: 3.784\n",
      "197\n",
      "step: 197, train loss: 3.807, val loss: 3.788\n",
      "198\n",
      "step: 198, train loss: 3.761, val loss: 3.800\n",
      "199\n",
      "step: 199, train loss: 3.759, val loss: 3.753\n",
      "200\n",
      "step: 200, train loss: 3.758, val loss: 3.755\n",
      "201\n",
      "step: 201, train loss: 3.759, val loss: 3.726\n",
      "202\n",
      "step: 202, train loss: 3.739, val loss: 3.730\n",
      "203\n",
      "step: 203, train loss: 3.730, val loss: 3.715\n",
      "204\n",
      "step: 204, train loss: 3.734, val loss: 3.711\n",
      "205\n",
      "step: 205, train loss: 3.695, val loss: 3.714\n",
      "206\n",
      "step: 206, train loss: 3.686, val loss: 3.685\n",
      "207\n",
      "step: 207, train loss: 3.669, val loss: 3.680\n",
      "208\n",
      "step: 208, train loss: 3.697, val loss: 3.666\n",
      "209\n",
      "step: 209, train loss: 3.667, val loss: 3.642\n",
      "210\n",
      "step: 210, train loss: 3.651, val loss: 3.647\n",
      "211\n",
      "step: 211, train loss: 3.662, val loss: 3.656\n",
      "212\n",
      "step: 212, train loss: 3.640, val loss: 3.617\n",
      "213\n",
      "step: 213, train loss: 3.601, val loss: 3.589\n",
      "214\n",
      "step: 214, train loss: 3.604, val loss: 3.609\n",
      "215\n",
      "step: 215, train loss: 3.618, val loss: 3.608\n",
      "216\n",
      "step: 216, train loss: 3.606, val loss: 3.603\n",
      "217\n",
      "step: 217, train loss: 3.614, val loss: 3.599\n",
      "218\n",
      "step: 218, train loss: 3.596, val loss: 3.599\n",
      "219\n",
      "step: 219, train loss: 3.595, val loss: 3.601\n",
      "220\n",
      "step: 220, train loss: 3.601, val loss: 3.596\n",
      "221\n",
      "step: 221, train loss: 3.567, val loss: 3.559\n",
      "222\n",
      "step: 222, train loss: 3.561, val loss: 3.568\n",
      "223\n",
      "step: 223, train loss: 3.525, val loss: 3.556\n",
      "224\n",
      "step: 224, train loss: 3.539, val loss: 3.541\n",
      "225\n",
      "step: 225, train loss: 3.516, val loss: 3.517\n",
      "226\n",
      "step: 226, train loss: 3.527, val loss: 3.538\n",
      "227\n",
      "step: 227, train loss: 3.516, val loss: 3.519\n",
      "228\n",
      "step: 228, train loss: 3.500, val loss: 3.533\n",
      "229\n",
      "step: 229, train loss: 3.542, val loss: 3.523\n",
      "230\n",
      "step: 230, train loss: 3.504, val loss: 3.501\n",
      "231\n",
      "step: 231, train loss: 3.482, val loss: 3.476\n",
      "232\n",
      "step: 232, train loss: 3.482, val loss: 3.505\n",
      "233\n",
      "step: 233, train loss: 3.471, val loss: 3.479\n",
      "234\n",
      "step: 234, train loss: 3.441, val loss: 3.462\n",
      "235\n",
      "step: 235, train loss: 3.487, val loss: 3.452\n",
      "236\n",
      "step: 236, train loss: 3.485, val loss: 3.500\n",
      "237\n",
      "step: 237, train loss: 3.458, val loss: 3.450\n",
      "238\n",
      "step: 238, train loss: 3.441, val loss: 3.445\n",
      "239\n",
      "step: 239, train loss: 3.465, val loss: 3.434\n",
      "240\n",
      "step: 240, train loss: 3.468, val loss: 3.425\n",
      "241\n",
      "step: 241, train loss: 3.493, val loss: 3.450\n",
      "242\n",
      "step: 242, train loss: 3.444, val loss: 3.428\n",
      "243\n",
      "step: 243, train loss: 3.441, val loss: 3.429\n",
      "244\n",
      "step: 244, train loss: 3.417, val loss: 3.432\n",
      "245\n",
      "step: 245, train loss: 3.459, val loss: 3.397\n",
      "246\n",
      "step: 246, train loss: 3.414, val loss: 3.414\n",
      "247\n",
      "step: 247, train loss: 3.388, val loss: 3.420\n",
      "248\n",
      "step: 248, train loss: 3.420, val loss: 3.393\n",
      "249\n",
      "step: 249, train loss: 3.394, val loss: 3.381\n",
      "250\n",
      "step: 250, train loss: 3.389, val loss: 3.438\n",
      "251\n",
      "step: 251, train loss: 3.365, val loss: 3.390\n",
      "252\n",
      "step: 252, train loss: 3.394, val loss: 3.375\n",
      "253\n",
      "step: 253, train loss: 3.370, val loss: 3.368\n",
      "254\n",
      "step: 254, train loss: 3.388, val loss: 3.344\n",
      "255\n",
      "step: 255, train loss: 3.353, val loss: 3.357\n",
      "256\n",
      "step: 256, train loss: 3.414, val loss: 3.379\n",
      "257\n",
      "step: 257, train loss: 3.369, val loss: 3.368\n",
      "258\n",
      "step: 258, train loss: 3.379, val loss: 3.353\n",
      "259\n",
      "step: 259, train loss: 3.353, val loss: 3.364\n",
      "260\n",
      "step: 260, train loss: 3.378, val loss: 3.336\n",
      "261\n",
      "step: 261, train loss: 3.406, val loss: 3.364\n",
      "262\n",
      "step: 262, train loss: 3.318, val loss: 3.342\n",
      "263\n",
      "step: 263, train loss: 3.309, val loss: 3.343\n",
      "264\n",
      "step: 264, train loss: 3.319, val loss: 3.312\n",
      "265\n",
      "step: 265, train loss: 3.344, val loss: 3.351\n",
      "266\n",
      "step: 266, train loss: 3.322, val loss: 3.325\n",
      "267\n",
      "step: 267, train loss: 3.303, val loss: 3.299\n",
      "268\n",
      "step: 268, train loss: 3.316, val loss: 3.340\n",
      "269\n",
      "step: 269, train loss: 3.330, val loss: 3.341\n",
      "270\n",
      "step: 270, train loss: 3.331, val loss: 3.333\n",
      "271\n",
      "step: 271, train loss: 3.310, val loss: 3.325\n",
      "272\n",
      "step: 272, train loss: 3.327, val loss: 3.306\n",
      "273\n",
      "step: 273, train loss: 3.312, val loss: 3.288\n",
      "274\n",
      "step: 274, train loss: 3.296, val loss: 3.288\n",
      "275\n",
      "step: 275, train loss: 3.313, val loss: 3.313\n",
      "276\n",
      "step: 276, train loss: 3.293, val loss: 3.295\n",
      "277\n",
      "step: 277, train loss: 3.297, val loss: 3.307\n",
      "278\n",
      "step: 278, train loss: 3.272, val loss: 3.324\n",
      "279\n",
      "step: 279, train loss: 3.303, val loss: 3.289\n",
      "280\n",
      "step: 280, train loss: 3.291, val loss: 3.279\n",
      "281\n",
      "step: 281, train loss: 3.272, val loss: 3.312\n",
      "282\n",
      "step: 282, train loss: 3.239, val loss: 3.275\n",
      "283\n",
      "step: 283, train loss: 3.273, val loss: 3.306\n",
      "284\n",
      "step: 284, train loss: 3.276, val loss: 3.287\n",
      "285\n",
      "step: 285, train loss: 3.266, val loss: 3.265\n",
      "286\n",
      "step: 286, train loss: 3.267, val loss: 3.271\n",
      "287\n",
      "step: 287, train loss: 3.263, val loss: 3.244\n",
      "288\n",
      "step: 288, train loss: 3.255, val loss: 3.246\n",
      "289\n",
      "step: 289, train loss: 3.270, val loss: 3.265\n",
      "290\n",
      "step: 290, train loss: 3.249, val loss: 3.247\n",
      "291\n",
      "step: 291, train loss: 3.274, val loss: 3.228\n",
      "292\n",
      "step: 292, train loss: 3.243, val loss: 3.228\n",
      "293\n",
      "step: 293, train loss: 3.242, val loss: 3.231\n",
      "294\n",
      "step: 294, train loss: 3.222, val loss: 3.248\n",
      "295\n",
      "step: 295, train loss: 3.270, val loss: 3.229\n",
      "296\n",
      "step: 296, train loss: 3.226, val loss: 3.273\n",
      "297\n",
      "step: 297, train loss: 3.254, val loss: 3.242\n",
      "298\n",
      "step: 298, train loss: 3.228, val loss: 3.275\n",
      "299\n",
      "step: 299, train loss: 3.266, val loss: 3.272\n",
      "300\n",
      "step: 300, train loss: 3.250, val loss: 3.226\n",
      "301\n",
      "step: 301, train loss: 3.243, val loss: 3.217\n",
      "302\n",
      "step: 302, train loss: 3.259, val loss: 3.243\n",
      "303\n",
      "step: 303, train loss: 3.223, val loss: 3.184\n",
      "304\n",
      "step: 304, train loss: 3.219, val loss: 3.187\n",
      "305\n",
      "step: 305, train loss: 3.216, val loss: 3.228\n",
      "306\n",
      "step: 306, train loss: 3.224, val loss: 3.250\n",
      "307\n",
      "step: 307, train loss: 3.202, val loss: 3.235\n",
      "308\n",
      "step: 308, train loss: 3.211, val loss: 3.205\n",
      "309\n",
      "step: 309, train loss: 3.238, val loss: 3.236\n",
      "310\n",
      "step: 310, train loss: 3.229, val loss: 3.221\n",
      "311\n",
      "step: 311, train loss: 3.199, val loss: 3.218\n",
      "312\n",
      "step: 312, train loss: 3.176, val loss: 3.195\n",
      "313\n",
      "step: 313, train loss: 3.201, val loss: 3.187\n",
      "314\n",
      "step: 314, train loss: 3.188, val loss: 3.186\n",
      "315\n",
      "step: 315, train loss: 3.197, val loss: 3.186\n",
      "316\n",
      "step: 316, train loss: 3.223, val loss: 3.174\n",
      "317\n",
      "step: 317, train loss: 3.182, val loss: 3.200\n",
      "318\n",
      "step: 318, train loss: 3.204, val loss: 3.201\n",
      "319\n",
      "step: 319, train loss: 3.215, val loss: 3.237\n",
      "320\n",
      "step: 320, train loss: 3.162, val loss: 3.212\n",
      "321\n",
      "step: 321, train loss: 3.165, val loss: 3.193\n",
      "322\n",
      "step: 322, train loss: 3.206, val loss: 3.186\n",
      "323\n",
      "step: 323, train loss: 3.174, val loss: 3.212\n",
      "324\n",
      "step: 324, train loss: 3.161, val loss: 3.197\n",
      "325\n",
      "step: 325, train loss: 3.215, val loss: 3.191\n",
      "326\n",
      "step: 326, train loss: 3.199, val loss: 3.179\n",
      "327\n",
      "step: 327, train loss: 3.180, val loss: 3.178\n",
      "328\n",
      "step: 328, train loss: 3.177, val loss: 3.173\n",
      "329\n",
      "step: 329, train loss: 3.166, val loss: 3.165\n",
      "330\n",
      "step: 330, train loss: 3.177, val loss: 3.176\n",
      "331\n",
      "step: 331, train loss: 3.142, val loss: 3.164\n",
      "332\n",
      "step: 332, train loss: 3.208, val loss: 3.164\n",
      "333\n",
      "step: 333, train loss: 3.163, val loss: 3.180\n",
      "334\n",
      "step: 334, train loss: 3.170, val loss: 3.136\n",
      "335\n",
      "step: 335, train loss: 3.179, val loss: 3.174\n",
      "336\n",
      "step: 336, train loss: 3.171, val loss: 3.155\n",
      "337\n",
      "step: 337, train loss: 3.152, val loss: 3.186\n",
      "338\n",
      "step: 338, train loss: 3.137, val loss: 3.148\n",
      "339\n",
      "step: 339, train loss: 3.127, val loss: 3.133\n",
      "340\n",
      "step: 340, train loss: 3.128, val loss: 3.146\n",
      "341\n",
      "step: 341, train loss: 3.170, val loss: 3.166\n",
      "342\n",
      "step: 342, train loss: 3.161, val loss: 3.167\n",
      "343\n",
      "step: 343, train loss: 3.160, val loss: 3.162\n",
      "344\n",
      "step: 344, train loss: 3.124, val loss: 3.151\n",
      "345\n",
      "step: 345, train loss: 3.158, val loss: 3.103\n",
      "346\n",
      "step: 346, train loss: 3.119, val loss: 3.141\n",
      "347\n",
      "step: 347, train loss: 3.139, val loss: 3.128\n",
      "348\n",
      "step: 348, train loss: 3.088, val loss: 3.133\n",
      "349\n",
      "step: 349, train loss: 3.142, val loss: 3.108\n",
      "350\n",
      "step: 350, train loss: 3.108, val loss: 3.158\n",
      "351\n",
      "step: 351, train loss: 3.091, val loss: 3.114\n",
      "352\n",
      "step: 352, train loss: 3.152, val loss: 3.110\n",
      "353\n",
      "step: 353, train loss: 3.110, val loss: 3.128\n",
      "354\n",
      "step: 354, train loss: 3.116, val loss: 3.111\n",
      "355\n",
      "step: 355, train loss: 3.109, val loss: 3.121\n",
      "356\n",
      "step: 356, train loss: 3.112, val loss: 3.140\n",
      "357\n",
      "step: 357, train loss: 3.110, val loss: 3.109\n",
      "358\n",
      "step: 358, train loss: 3.134, val loss: 3.138\n",
      "359\n",
      "step: 359, train loss: 3.091, val loss: 3.117\n",
      "360\n",
      "step: 360, train loss: 3.132, val loss: 3.145\n",
      "361\n",
      "step: 361, train loss: 3.074, val loss: 3.139\n",
      "362\n",
      "step: 362, train loss: 3.139, val loss: 3.108\n",
      "363\n",
      "step: 363, train loss: 3.109, val loss: 3.111\n",
      "364\n",
      "step: 364, train loss: 3.128, val loss: 3.102\n",
      "365\n",
      "step: 365, train loss: 3.069, val loss: 3.085\n",
      "366\n",
      "step: 366, train loss: 3.102, val loss: 3.099\n",
      "367\n",
      "step: 367, train loss: 3.124, val loss: 3.146\n",
      "368\n",
      "step: 368, train loss: 3.126, val loss: 3.103\n",
      "369\n",
      "step: 369, train loss: 3.119, val loss: 3.113\n",
      "370\n",
      "step: 370, train loss: 3.098, val loss: 3.120\n",
      "371\n",
      "step: 371, train loss: 3.088, val loss: 3.070\n",
      "372\n",
      "step: 372, train loss: 3.086, val loss: 3.115\n",
      "373\n",
      "step: 373, train loss: 3.072, val loss: 3.126\n",
      "374\n",
      "step: 374, train loss: 3.100, val loss: 3.115\n",
      "375\n",
      "step: 375, train loss: 3.147, val loss: 3.114\n",
      "376\n",
      "step: 376, train loss: 3.116, val loss: 3.132\n",
      "377\n",
      "step: 377, train loss: 3.092, val loss: 3.093\n",
      "378\n",
      "step: 378, train loss: 3.076, val loss: 3.116\n",
      "379\n",
      "step: 379, train loss: 3.100, val loss: 3.064\n",
      "380\n",
      "step: 380, train loss: 3.047, val loss: 3.069\n",
      "381\n",
      "step: 381, train loss: 3.081, val loss: 3.081\n",
      "382\n",
      "step: 382, train loss: 3.058, val loss: 3.062\n",
      "383\n",
      "step: 383, train loss: 3.070, val loss: 3.066\n",
      "384\n",
      "step: 384, train loss: 3.061, val loss: 3.105\n",
      "385\n",
      "step: 385, train loss: 3.083, val loss: 3.019\n",
      "386\n",
      "step: 386, train loss: 3.081, val loss: 3.062\n",
      "387\n",
      "step: 387, train loss: 3.054, val loss: 3.057\n",
      "388\n",
      "step: 388, train loss: 3.080, val loss: 3.070\n",
      "389\n",
      "step: 389, train loss: 3.046, val loss: 3.088\n",
      "390\n",
      "step: 390, train loss: 3.066, val loss: 3.103\n",
      "391\n",
      "step: 391, train loss: 3.046, val loss: 3.044\n",
      "392\n",
      "step: 392, train loss: 3.081, val loss: 3.082\n",
      "393\n",
      "step: 393, train loss: 3.071, val loss: 3.037\n",
      "394\n",
      "step: 394, train loss: 3.072, val loss: 3.037\n",
      "395\n",
      "step: 395, train loss: 3.035, val loss: 3.066\n",
      "396\n",
      "step: 396, train loss: 3.067, val loss: 3.080\n",
      "397\n",
      "step: 397, train loss: 3.057, val loss: 3.036\n",
      "398\n",
      "step: 398, train loss: 3.064, val loss: 3.037\n",
      "399\n",
      "step: 399, train loss: 3.051, val loss: 3.084\n",
      "400\n",
      "step: 400, train loss: 3.068, val loss: 3.030\n",
      "401\n",
      "step: 401, train loss: 3.058, val loss: 3.056\n",
      "402\n",
      "step: 402, train loss: 3.094, val loss: 3.053\n",
      "403\n",
      "step: 403, train loss: 3.040, val loss: 3.055\n",
      "404\n",
      "step: 404, train loss: 3.066, val loss: 3.057\n",
      "405\n",
      "step: 405, train loss: 3.090, val loss: 3.038\n",
      "406\n",
      "step: 406, train loss: 3.038, val loss: 3.057\n",
      "407\n",
      "step: 407, train loss: 3.040, val loss: 3.082\n",
      "408\n",
      "step: 408, train loss: 3.043, val loss: 3.036\n",
      "409\n",
      "step: 409, train loss: 3.030, val loss: 3.110\n",
      "410\n",
      "step: 410, train loss: 3.052, val loss: 3.056\n",
      "411\n",
      "step: 411, train loss: 3.052, val loss: 3.030\n",
      "412\n",
      "step: 412, train loss: 3.047, val loss: 2.985\n",
      "413\n",
      "step: 413, train loss: 3.034, val loss: 3.032\n",
      "414\n",
      "step: 414, train loss: 3.057, val loss: 3.048\n",
      "415\n",
      "step: 415, train loss: 2.989, val loss: 3.031\n",
      "416\n",
      "step: 416, train loss: 3.005, val loss: 3.055\n",
      "417\n",
      "step: 417, train loss: 3.043, val loss: 3.006\n",
      "418\n",
      "step: 418, train loss: 3.032, val loss: 3.052\n",
      "419\n",
      "step: 419, train loss: 3.024, val loss: 2.995\n",
      "420\n",
      "step: 420, train loss: 3.018, val loss: 3.024\n",
      "421\n",
      "step: 421, train loss: 3.018, val loss: 3.006\n",
      "422\n",
      "step: 422, train loss: 2.985, val loss: 3.011\n",
      "423\n",
      "step: 423, train loss: 2.991, val loss: 3.021\n",
      "424\n",
      "step: 424, train loss: 3.008, val loss: 3.041\n",
      "425\n",
      "step: 425, train loss: 3.031, val loss: 3.025\n",
      "426\n",
      "step: 426, train loss: 3.029, val loss: 3.020\n",
      "427\n",
      "step: 427, train loss: 2.998, val loss: 2.984\n",
      "428\n",
      "step: 428, train loss: 3.028, val loss: 3.046\n",
      "429\n",
      "step: 429, train loss: 3.008, val loss: 3.019\n",
      "430\n",
      "step: 430, train loss: 2.986, val loss: 3.011\n",
      "431\n",
      "step: 431, train loss: 3.002, val loss: 3.026\n",
      "432\n",
      "step: 432, train loss: 3.062, val loss: 3.020\n",
      "433\n",
      "step: 433, train loss: 3.032, val loss: 2.996\n",
      "434\n",
      "step: 434, train loss: 3.034, val loss: 2.986\n",
      "435\n",
      "step: 435, train loss: 2.995, val loss: 3.059\n",
      "436\n",
      "step: 436, train loss: 3.023, val loss: 3.002\n",
      "437\n",
      "step: 437, train loss: 3.000, val loss: 3.008\n",
      "438\n",
      "step: 438, train loss: 3.006, val loss: 3.001\n",
      "439\n",
      "step: 439, train loss: 2.988, val loss: 3.008\n",
      "440\n",
      "step: 440, train loss: 2.975, val loss: 2.987\n",
      "441\n",
      "step: 441, train loss: 2.976, val loss: 3.010\n",
      "442\n",
      "step: 442, train loss: 2.974, val loss: 2.971\n",
      "443\n",
      "step: 443, train loss: 2.966, val loss: 2.948\n",
      "444\n",
      "step: 444, train loss: 2.965, val loss: 2.982\n",
      "445\n",
      "step: 445, train loss: 2.971, val loss: 2.999\n",
      "446\n",
      "step: 446, train loss: 2.987, val loss: 3.001\n",
      "447\n",
      "step: 447, train loss: 3.017, val loss: 2.982\n",
      "448\n",
      "step: 448, train loss: 2.977, val loss: 2.998\n",
      "449\n",
      "step: 449, train loss: 2.966, val loss: 3.005\n",
      "450\n",
      "step: 450, train loss: 2.963, val loss: 2.982\n",
      "451\n",
      "step: 451, train loss: 2.958, val loss: 3.009\n",
      "452\n",
      "step: 452, train loss: 2.955, val loss: 2.950\n",
      "453\n",
      "step: 453, train loss: 2.956, val loss: 2.988\n",
      "454\n",
      "step: 454, train loss: 2.991, val loss: 2.999\n",
      "455\n",
      "step: 455, train loss: 2.974, val loss: 3.006\n",
      "456\n",
      "step: 456, train loss: 3.029, val loss: 2.956\n",
      "457\n",
      "step: 457, train loss: 2.972, val loss: 2.996\n",
      "458\n",
      "step: 458, train loss: 2.956, val loss: 3.008\n",
      "459\n",
      "step: 459, train loss: 2.947, val loss: 2.973\n",
      "460\n",
      "step: 460, train loss: 2.975, val loss: 2.992\n",
      "461\n",
      "step: 461, train loss: 2.973, val loss: 2.966\n",
      "462\n",
      "step: 462, train loss: 2.955, val loss: 2.999\n",
      "463\n",
      "step: 463, train loss: 2.927, val loss: 2.984\n",
      "464\n",
      "step: 464, train loss: 2.959, val loss: 3.014\n",
      "465\n",
      "step: 465, train loss: 2.959, val loss: 2.929\n",
      "466\n",
      "step: 466, train loss: 2.970, val loss: 2.952\n",
      "467\n",
      "step: 467, train loss: 2.941, val loss: 3.013\n",
      "468\n",
      "step: 468, train loss: 2.919, val loss: 2.968\n",
      "469\n",
      "step: 469, train loss: 2.961, val loss: 2.968\n",
      "470\n",
      "step: 470, train loss: 2.978, val loss: 2.948\n",
      "471\n",
      "step: 471, train loss: 2.988, val loss: 2.973\n",
      "472\n",
      "step: 472, train loss: 2.933, val loss: 2.940\n",
      "473\n",
      "step: 473, train loss: 2.935, val loss: 2.946\n",
      "474\n",
      "step: 474, train loss: 2.948, val loss: 2.920\n",
      "475\n",
      "step: 475, train loss: 2.962, val loss: 2.990\n",
      "476\n",
      "step: 476, train loss: 2.921, val loss: 2.961\n",
      "477\n",
      "step: 477, train loss: 2.995, val loss: 2.950\n",
      "478\n",
      "step: 478, train loss: 2.939, val loss: 2.991\n",
      "479\n",
      "step: 479, train loss: 2.947, val loss: 2.944\n",
      "480\n",
      "step: 480, train loss: 2.959, val loss: 2.955\n",
      "481\n",
      "step: 481, train loss: 2.922, val loss: 2.947\n",
      "482\n",
      "step: 482, train loss: 2.962, val loss: 2.926\n",
      "483\n",
      "step: 483, train loss: 2.934, val loss: 2.906\n",
      "484\n",
      "step: 484, train loss: 2.934, val loss: 2.980\n",
      "485\n",
      "step: 485, train loss: 2.938, val loss: 2.965\n",
      "486\n",
      "step: 486, train loss: 2.953, val loss: 2.971\n",
      "487\n",
      "step: 487, train loss: 2.922, val loss: 2.940\n",
      "488\n",
      "step: 488, train loss: 2.908, val loss: 2.907\n",
      "489\n",
      "step: 489, train loss: 2.956, val loss: 2.923\n",
      "490\n",
      "step: 490, train loss: 2.952, val loss: 2.944\n",
      "491\n",
      "step: 491, train loss: 2.899, val loss: 2.928\n",
      "492\n",
      "step: 492, train loss: 2.964, val loss: 2.948\n",
      "493\n",
      "step: 493, train loss: 2.952, val loss: 2.930\n",
      "494\n",
      "step: 494, train loss: 2.915, val loss: 2.887\n",
      "495\n",
      "step: 495, train loss: 2.930, val loss: 2.917\n",
      "496\n",
      "step: 496, train loss: 2.931, val loss: 2.931\n",
      "497\n",
      "step: 497, train loss: 2.973, val loss: 2.889\n",
      "498\n",
      "step: 498, train loss: 2.936, val loss: 2.947\n",
      "499\n",
      "step: 499, train loss: 2.956, val loss: 2.936\n",
      "500\n",
      "step: 500, train loss: 2.949, val loss: 2.932\n",
      "501\n",
      "step: 501, train loss: 2.877, val loss: 2.926\n",
      "502\n",
      "step: 502, train loss: 2.929, val loss: 2.885\n",
      "503\n",
      "step: 503, train loss: 2.899, val loss: 2.908\n",
      "504\n",
      "step: 504, train loss: 2.951, val loss: 2.901\n",
      "505\n",
      "step: 505, train loss: 2.949, val loss: 2.911\n",
      "506\n",
      "step: 506, train loss: 2.904, val loss: 2.919\n",
      "507\n",
      "step: 507, train loss: 2.945, val loss: 2.951\n",
      "508\n",
      "step: 508, train loss: 2.927, val loss: 2.935\n",
      "509\n",
      "step: 509, train loss: 2.953, val loss: 2.917\n",
      "510\n",
      "step: 510, train loss: 2.890, val loss: 2.892\n",
      "511\n",
      "step: 511, train loss: 2.893, val loss: 2.901\n",
      "512\n",
      "step: 512, train loss: 2.929, val loss: 2.873\n",
      "513\n",
      "step: 513, train loss: 2.885, val loss: 2.937\n",
      "514\n",
      "step: 514, train loss: 2.907, val loss: 2.872\n",
      "515\n",
      "step: 515, train loss: 2.889, val loss: 2.893\n",
      "516\n",
      "step: 516, train loss: 2.880, val loss: 2.889\n",
      "517\n",
      "step: 517, train loss: 2.907, val loss: 2.902\n",
      "518\n",
      "step: 518, train loss: 2.912, val loss: 2.926\n",
      "519\n",
      "step: 519, train loss: 2.903, val loss: 2.917\n",
      "520\n",
      "step: 520, train loss: 2.904, val loss: 2.877\n",
      "521\n",
      "step: 521, train loss: 2.859, val loss: 2.893\n",
      "522\n",
      "step: 522, train loss: 2.920, val loss: 2.916\n",
      "523\n",
      "step: 523, train loss: 2.904, val loss: 2.851\n",
      "524\n",
      "step: 524, train loss: 2.926, val loss: 2.934\n",
      "525\n",
      "step: 525, train loss: 2.860, val loss: 2.949\n",
      "526\n",
      "step: 526, train loss: 2.884, val loss: 2.902\n",
      "527\n",
      "step: 527, train loss: 2.872, val loss: 2.896\n",
      "528\n",
      "step: 528, train loss: 2.900, val loss: 2.914\n",
      "529\n",
      "step: 529, train loss: 2.882, val loss: 2.898\n",
      "530\n",
      "step: 530, train loss: 2.902, val loss: 2.941\n",
      "531\n",
      "step: 531, train loss: 2.919, val loss: 2.957\n",
      "532\n",
      "step: 532, train loss: 2.866, val loss: 2.852\n",
      "533\n",
      "step: 533, train loss: 2.873, val loss: 2.902\n",
      "534\n",
      "step: 534, train loss: 2.870, val loss: 2.921\n",
      "535\n",
      "step: 535, train loss: 2.873, val loss: 2.851\n",
      "536\n",
      "step: 536, train loss: 2.903, val loss: 2.880\n",
      "537\n",
      "step: 537, train loss: 2.875, val loss: 2.890\n",
      "538\n",
      "step: 538, train loss: 2.908, val loss: 2.886\n",
      "539\n",
      "step: 539, train loss: 2.895, val loss: 2.920\n",
      "540\n",
      "step: 540, train loss: 2.919, val loss: 2.894\n",
      "541\n",
      "step: 541, train loss: 2.888, val loss: 2.887\n",
      "542\n",
      "step: 542, train loss: 2.903, val loss: 2.884\n",
      "543\n",
      "step: 543, train loss: 2.926, val loss: 2.892\n",
      "544\n",
      "step: 544, train loss: 2.875, val loss: 2.873\n",
      "545\n",
      "step: 545, train loss: 2.905, val loss: 2.874\n",
      "546\n",
      "step: 546, train loss: 2.875, val loss: 2.852\n",
      "547\n",
      "step: 547, train loss: 2.827, val loss: 2.866\n",
      "548\n",
      "step: 548, train loss: 2.842, val loss: 2.858\n",
      "549\n",
      "step: 549, train loss: 2.885, val loss: 2.884\n",
      "550\n",
      "step: 550, train loss: 2.885, val loss: 2.877\n",
      "551\n",
      "step: 551, train loss: 2.846, val loss: 2.883\n",
      "552\n",
      "step: 552, train loss: 2.875, val loss: 2.863\n",
      "553\n",
      "step: 553, train loss: 2.835, val loss: 2.878\n",
      "554\n",
      "step: 554, train loss: 2.882, val loss: 2.876\n",
      "555\n",
      "step: 555, train loss: 2.867, val loss: 2.903\n",
      "556\n",
      "step: 556, train loss: 2.857, val loss: 2.870\n",
      "557\n",
      "step: 557, train loss: 2.892, val loss: 2.931\n",
      "558\n",
      "step: 558, train loss: 2.848, val loss: 2.861\n",
      "559\n",
      "step: 559, train loss: 2.894, val loss: 2.900\n",
      "560\n",
      "step: 560, train loss: 2.863, val loss: 2.914\n",
      "561\n",
      "step: 561, train loss: 2.885, val loss: 2.870\n",
      "562\n",
      "step: 562, train loss: 2.832, val loss: 2.861\n",
      "563\n",
      "step: 563, train loss: 2.841, val loss: 2.847\n",
      "564\n",
      "step: 564, train loss: 2.889, val loss: 2.887\n",
      "565\n",
      "step: 565, train loss: 2.825, val loss: 2.914\n",
      "566\n",
      "step: 566, train loss: 2.859, val loss: 2.880\n",
      "567\n",
      "step: 567, train loss: 2.843, val loss: 2.847\n",
      "568\n",
      "step: 568, train loss: 2.868, val loss: 2.861\n",
      "569\n",
      "step: 569, train loss: 2.886, val loss: 2.863\n",
      "570\n",
      "step: 570, train loss: 2.864, val loss: 2.849\n",
      "571\n",
      "step: 571, train loss: 2.823, val loss: 2.839\n",
      "572\n",
      "step: 572, train loss: 2.883, val loss: 2.895\n",
      "573\n",
      "step: 573, train loss: 2.827, val loss: 2.893\n",
      "574\n",
      "step: 574, train loss: 2.911, val loss: 2.889\n",
      "575\n",
      "step: 575, train loss: 2.845, val loss: 2.847\n",
      "576\n",
      "step: 576, train loss: 2.875, val loss: 2.839\n",
      "577\n",
      "step: 577, train loss: 2.829, val loss: 2.841\n",
      "578\n",
      "step: 578, train loss: 2.835, val loss: 2.860\n",
      "579\n",
      "step: 579, train loss: 2.816, val loss: 2.861\n",
      "580\n",
      "step: 580, train loss: 2.867, val loss: 2.816\n",
      "581\n",
      "step: 581, train loss: 2.893, val loss: 2.816\n",
      "582\n",
      "step: 582, train loss: 2.796, val loss: 2.865\n",
      "583\n",
      "step: 583, train loss: 2.825, val loss: 2.836\n",
      "584\n",
      "step: 584, train loss: 2.881, val loss: 2.815\n",
      "585\n",
      "step: 585, train loss: 2.824, val loss: 2.849\n",
      "586\n",
      "step: 586, train loss: 2.830, val loss: 2.826\n",
      "587\n",
      "step: 587, train loss: 2.867, val loss: 2.816\n",
      "588\n",
      "step: 588, train loss: 2.840, val loss: 2.867\n",
      "589\n",
      "step: 589, train loss: 2.830, val loss: 2.861\n",
      "590\n",
      "step: 590, train loss: 2.804, val loss: 2.806\n",
      "591\n",
      "step: 591, train loss: 2.855, val loss: 2.845\n",
      "592\n",
      "step: 592, train loss: 2.806, val loss: 2.820\n",
      "593\n",
      "step: 593, train loss: 2.856, val loss: 2.812\n",
      "594\n",
      "step: 594, train loss: 2.822, val loss: 2.853\n",
      "595\n",
      "step: 595, train loss: 2.824, val loss: 2.852\n",
      "596\n",
      "step: 596, train loss: 2.795, val loss: 2.842\n",
      "597\n",
      "step: 597, train loss: 2.808, val loss: 2.830\n",
      "598\n",
      "step: 598, train loss: 2.825, val loss: 2.823\n",
      "599\n",
      "step: 599, train loss: 2.813, val loss: 2.804\n",
      "600\n",
      "step: 600, train loss: 2.843, val loss: 2.857\n",
      "601\n",
      "step: 601, train loss: 2.840, val loss: 2.795\n",
      "602\n",
      "step: 602, train loss: 2.862, val loss: 2.833\n",
      "603\n",
      "step: 603, train loss: 2.819, val loss: 2.830\n",
      "604\n",
      "step: 604, train loss: 2.810, val loss: 2.804\n",
      "605\n",
      "step: 605, train loss: 2.860, val loss: 2.845\n",
      "606\n",
      "step: 606, train loss: 2.813, val loss: 2.808\n",
      "607\n",
      "step: 607, train loss: 2.841, val loss: 2.845\n",
      "608\n",
      "step: 608, train loss: 2.783, val loss: 2.831\n",
      "609\n",
      "step: 609, train loss: 2.808, val loss: 2.830\n",
      "610\n",
      "step: 610, train loss: 2.833, val loss: 2.800\n",
      "611\n",
      "step: 611, train loss: 2.823, val loss: 2.793\n",
      "612\n",
      "step: 612, train loss: 2.816, val loss: 2.791\n",
      "613\n",
      "step: 613, train loss: 2.813, val loss: 2.773\n",
      "614\n",
      "step: 614, train loss: 2.784, val loss: 2.815\n",
      "615\n",
      "step: 615, train loss: 2.799, val loss: 2.791\n",
      "616\n",
      "step: 616, train loss: 2.822, val loss: 2.831\n",
      "617\n",
      "step: 617, train loss: 2.801, val loss: 2.819\n",
      "618\n",
      "step: 618, train loss: 2.801, val loss: 2.808\n",
      "619\n",
      "step: 619, train loss: 2.785, val loss: 2.799\n",
      "620\n",
      "step: 620, train loss: 2.829, val loss: 2.833\n",
      "621\n",
      "step: 621, train loss: 2.805, val loss: 2.808\n",
      "622\n",
      "step: 622, train loss: 2.835, val loss: 2.814\n",
      "623\n",
      "step: 623, train loss: 2.811, val loss: 2.773\n",
      "624\n",
      "step: 624, train loss: 2.804, val loss: 2.789\n",
      "625\n",
      "step: 625, train loss: 2.857, val loss: 2.833\n",
      "626\n",
      "step: 626, train loss: 2.827, val loss: 2.812\n",
      "627\n",
      "step: 627, train loss: 2.825, val loss: 2.792\n",
      "628\n",
      "step: 628, train loss: 2.807, val loss: 2.796\n",
      "629\n",
      "step: 629, train loss: 2.812, val loss: 2.794\n",
      "630\n",
      "step: 630, train loss: 2.765, val loss: 2.783\n",
      "631\n",
      "step: 631, train loss: 2.779, val loss: 2.820\n",
      "632\n",
      "step: 632, train loss: 2.836, val loss: 2.794\n",
      "633\n",
      "step: 633, train loss: 2.794, val loss: 2.790\n",
      "634\n",
      "step: 634, train loss: 2.764, val loss: 2.767\n",
      "635\n",
      "step: 635, train loss: 2.804, val loss: 2.867\n",
      "636\n",
      "step: 636, train loss: 2.797, val loss: 2.806\n",
      "637\n",
      "step: 637, train loss: 2.816, val loss: 2.806\n",
      "638\n",
      "step: 638, train loss: 2.800, val loss: 2.859\n",
      "639\n",
      "step: 639, train loss: 2.797, val loss: 2.806\n",
      "640\n",
      "step: 640, train loss: 2.840, val loss: 2.756\n",
      "641\n",
      "step: 641, train loss: 2.768, val loss: 2.746\n",
      "642\n",
      "step: 642, train loss: 2.769, val loss: 2.849\n",
      "643\n",
      "step: 643, train loss: 2.776, val loss: 2.796\n",
      "644\n",
      "step: 644, train loss: 2.784, val loss: 2.808\n",
      "645\n",
      "step: 645, train loss: 2.772, val loss: 2.770\n",
      "646\n",
      "step: 646, train loss: 2.782, val loss: 2.777\n",
      "647\n",
      "step: 647, train loss: 2.774, val loss: 2.822\n",
      "648\n",
      "step: 648, train loss: 2.776, val loss: 2.781\n",
      "649\n",
      "step: 649, train loss: 2.792, val loss: 2.774\n",
      "650\n",
      "step: 650, train loss: 2.810, val loss: 2.882\n",
      "651\n",
      "step: 651, train loss: 2.786, val loss: 2.759\n",
      "652\n",
      "step: 652, train loss: 2.767, val loss: 2.820\n",
      "653\n",
      "step: 653, train loss: 2.815, val loss: 2.827\n",
      "654\n",
      "step: 654, train loss: 2.790, val loss: 2.766\n",
      "655\n",
      "step: 655, train loss: 2.768, val loss: 2.769\n",
      "656\n",
      "step: 656, train loss: 2.762, val loss: 2.796\n",
      "657\n",
      "step: 657, train loss: 2.786, val loss: 2.782\n",
      "658\n",
      "step: 658, train loss: 2.806, val loss: 2.805\n",
      "659\n",
      "step: 659, train loss: 2.840, val loss: 2.782\n",
      "660\n",
      "step: 660, train loss: 2.780, val loss: 2.828\n",
      "661\n",
      "step: 661, train loss: 2.811, val loss: 2.809\n",
      "662\n",
      "step: 662, train loss: 2.722, val loss: 2.793\n",
      "663\n",
      "step: 663, train loss: 2.781, val loss: 2.780\n",
      "664\n",
      "step: 664, train loss: 2.786, val loss: 2.755\n",
      "665\n",
      "step: 665, train loss: 2.762, val loss: 2.809\n",
      "666\n",
      "step: 666, train loss: 2.814, val loss: 2.803\n",
      "667\n",
      "step: 667, train loss: 2.779, val loss: 2.767\n",
      "668\n",
      "step: 668, train loss: 2.799, val loss: 2.769\n",
      "669\n",
      "step: 669, train loss: 2.753, val loss: 2.841\n",
      "670\n",
      "step: 670, train loss: 2.745, val loss: 2.758\n",
      "671\n",
      "step: 671, train loss: 2.803, val loss: 2.749\n",
      "672\n",
      "step: 672, train loss: 2.768, val loss: 2.736\n",
      "673\n",
      "step: 673, train loss: 2.740, val loss: 2.766\n",
      "674\n",
      "step: 674, train loss: 2.804, val loss: 2.773\n",
      "675\n",
      "step: 675, train loss: 2.794, val loss: 2.781\n",
      "676\n",
      "step: 676, train loss: 2.785, val loss: 2.770\n",
      "677\n",
      "step: 677, train loss: 2.735, val loss: 2.778\n",
      "678\n",
      "step: 678, train loss: 2.751, val loss: 2.777\n",
      "679\n",
      "step: 679, train loss: 2.829, val loss: 2.754\n",
      "680\n",
      "step: 680, train loss: 2.764, val loss: 2.814\n",
      "681\n",
      "step: 681, train loss: 2.729, val loss: 2.751\n",
      "682\n",
      "step: 682, train loss: 2.788, val loss: 2.780\n",
      "683\n",
      "step: 683, train loss: 2.767, val loss: 2.774\n",
      "684\n",
      "step: 684, train loss: 2.726, val loss: 2.746\n",
      "685\n",
      "step: 685, train loss: 2.754, val loss: 2.816\n",
      "686\n",
      "step: 686, train loss: 2.799, val loss: 2.767\n",
      "687\n",
      "step: 687, train loss: 2.725, val loss: 2.743\n",
      "688\n",
      "step: 688, train loss: 2.757, val loss: 2.747\n",
      "689\n",
      "step: 689, train loss: 2.735, val loss: 2.738\n",
      "690\n",
      "step: 690, train loss: 2.733, val loss: 2.778\n",
      "691\n",
      "step: 691, train loss: 2.753, val loss: 2.725\n",
      "692\n",
      "step: 692, train loss: 2.808, val loss: 2.750\n",
      "693\n",
      "step: 693, train loss: 2.756, val loss: 2.781\n",
      "694\n",
      "step: 694, train loss: 2.757, val loss: 2.762\n",
      "695\n",
      "step: 695, train loss: 2.765, val loss: 2.726\n",
      "696\n",
      "step: 696, train loss: 2.743, val loss: 2.766\n",
      "697\n",
      "step: 697, train loss: 2.756, val loss: 2.797\n",
      "698\n",
      "step: 698, train loss: 2.738, val loss: 2.764\n",
      "699\n",
      "step: 699, train loss: 2.774, val loss: 2.838\n",
      "700\n",
      "step: 700, train loss: 2.750, val loss: 2.780\n",
      "701\n",
      "step: 701, train loss: 2.751, val loss: 2.762\n",
      "702\n",
      "step: 702, train loss: 2.720, val loss: 2.763\n",
      "703\n",
      "step: 703, train loss: 2.752, val loss: 2.787\n",
      "704\n",
      "step: 704, train loss: 2.709, val loss: 2.740\n",
      "705\n",
      "step: 705, train loss: 2.747, val loss: 2.714\n",
      "706\n",
      "step: 706, train loss: 2.771, val loss: 2.789\n",
      "707\n",
      "step: 707, train loss: 2.737, val loss: 2.769\n",
      "708\n",
      "step: 708, train loss: 2.760, val loss: 2.811\n",
      "709\n",
      "step: 709, train loss: 2.765, val loss: 2.802\n",
      "710\n",
      "step: 710, train loss: 2.730, val loss: 2.763\n",
      "711\n",
      "step: 711, train loss: 2.741, val loss: 2.747\n",
      "712\n",
      "step: 712, train loss: 2.761, val loss: 2.745\n",
      "713\n",
      "step: 713, train loss: 2.745, val loss: 2.796\n",
      "714\n",
      "step: 714, train loss: 2.773, val loss: 2.760\n",
      "715\n",
      "step: 715, train loss: 2.753, val loss: 2.769\n",
      "716\n",
      "step: 716, train loss: 2.771, val loss: 2.765\n",
      "717\n",
      "step: 717, train loss: 2.753, val loss: 2.753\n",
      "718\n",
      "step: 718, train loss: 2.728, val loss: 2.761\n",
      "719\n",
      "step: 719, train loss: 2.738, val loss: 2.736\n",
      "720\n",
      "step: 720, train loss: 2.712, val loss: 2.778\n",
      "721\n",
      "step: 721, train loss: 2.797, val loss: 2.735\n",
      "722\n",
      "step: 722, train loss: 2.750, val loss: 2.790\n",
      "723\n",
      "step: 723, train loss: 2.738, val loss: 2.722\n",
      "724\n",
      "step: 724, train loss: 2.712, val loss: 2.712\n",
      "725\n",
      "step: 725, train loss: 2.769, val loss: 2.769\n",
      "726\n",
      "step: 726, train loss: 2.732, val loss: 2.748\n",
      "727\n",
      "step: 727, train loss: 2.737, val loss: 2.757\n",
      "728\n",
      "step: 728, train loss: 2.787, val loss: 2.711\n",
      "729\n",
      "step: 729, train loss: 2.734, val loss: 2.728\n",
      "730\n",
      "step: 730, train loss: 2.707, val loss: 2.726\n",
      "731\n",
      "step: 731, train loss: 2.736, val loss: 2.709\n",
      "732\n",
      "step: 732, train loss: 2.713, val loss: 2.761\n",
      "733\n",
      "step: 733, train loss: 2.696, val loss: 2.757\n",
      "734\n",
      "step: 734, train loss: 2.714, val loss: 2.761\n",
      "735\n",
      "step: 735, train loss: 2.706, val loss: 2.738\n",
      "736\n",
      "step: 736, train loss: 2.728, val loss: 2.729\n",
      "737\n",
      "step: 737, train loss: 2.754, val loss: 2.699\n",
      "738\n",
      "step: 738, train loss: 2.744, val loss: 2.707\n",
      "739\n",
      "step: 739, train loss: 2.737, val loss: 2.737\n",
      "740\n",
      "step: 740, train loss: 2.777, val loss: 2.716\n",
      "741\n",
      "step: 741, train loss: 2.740, val loss: 2.768\n",
      "742\n",
      "step: 742, train loss: 2.759, val loss: 2.703\n",
      "743\n",
      "step: 743, train loss: 2.757, val loss: 2.745\n",
      "744\n",
      "step: 744, train loss: 2.748, val loss: 2.726\n",
      "745\n",
      "step: 745, train loss: 2.760, val loss: 2.738\n",
      "746\n",
      "step: 746, train loss: 2.691, val loss: 2.751\n",
      "747\n",
      "step: 747, train loss: 2.718, val loss: 2.770\n",
      "748\n",
      "step: 748, train loss: 2.715, val loss: 2.690\n",
      "749\n",
      "step: 749, train loss: 2.720, val loss: 2.756\n",
      "750\n",
      "step: 750, train loss: 2.723, val loss: 2.728\n",
      "751\n",
      "step: 751, train loss: 2.747, val loss: 2.698\n",
      "752\n",
      "step: 752, train loss: 2.718, val loss: 2.691\n",
      "753\n",
      "step: 753, train loss: 2.720, val loss: 2.742\n",
      "754\n",
      "step: 754, train loss: 2.726, val loss: 2.762\n",
      "755\n",
      "step: 755, train loss: 2.739, val loss: 2.773\n",
      "756\n",
      "step: 756, train loss: 2.720, val loss: 2.712\n",
      "757\n",
      "step: 757, train loss: 2.723, val loss: 2.729\n",
      "758\n",
      "step: 758, train loss: 2.713, val loss: 2.737\n",
      "759\n",
      "step: 759, train loss: 2.720, val loss: 2.732\n",
      "760\n",
      "step: 760, train loss: 2.714, val loss: 2.740\n",
      "761\n",
      "step: 761, train loss: 2.702, val loss: 2.707\n",
      "762\n",
      "step: 762, train loss: 2.732, val loss: 2.719\n",
      "763\n",
      "step: 763, train loss: 2.704, val loss: 2.714\n",
      "764\n",
      "step: 764, train loss: 2.707, val loss: 2.695\n",
      "765\n",
      "step: 765, train loss: 2.704, val loss: 2.691\n",
      "766\n",
      "step: 766, train loss: 2.733, val loss: 2.689\n",
      "767\n",
      "step: 767, train loss: 2.712, val loss: 2.700\n",
      "768\n",
      "step: 768, train loss: 2.728, val loss: 2.748\n",
      "769\n",
      "step: 769, train loss: 2.681, val loss: 2.735\n",
      "770\n",
      "step: 770, train loss: 2.691, val loss: 2.716\n",
      "771\n",
      "step: 771, train loss: 2.738, val loss: 2.729\n",
      "772\n",
      "step: 772, train loss: 2.706, val loss: 2.704\n",
      "773\n",
      "step: 773, train loss: 2.759, val loss: 2.756\n",
      "774\n",
      "step: 774, train loss: 2.734, val loss: 2.723\n",
      "775\n",
      "step: 775, train loss: 2.679, val loss: 2.685\n",
      "776\n",
      "step: 776, train loss: 2.737, val loss: 2.703\n",
      "777\n",
      "step: 777, train loss: 2.686, val loss: 2.692\n",
      "778\n",
      "step: 778, train loss: 2.708, val loss: 2.710\n",
      "779\n",
      "step: 779, train loss: 2.692, val loss: 2.735\n",
      "780\n",
      "step: 780, train loss: 2.702, val loss: 2.737\n",
      "781\n",
      "step: 781, train loss: 2.693, val loss: 2.662\n",
      "782\n",
      "step: 782, train loss: 2.664, val loss: 2.698\n",
      "783\n",
      "step: 783, train loss: 2.683, val loss: 2.738\n",
      "784\n",
      "step: 784, train loss: 2.718, val loss: 2.733\n",
      "785\n",
      "step: 785, train loss: 2.670, val loss: 2.682\n",
      "786\n",
      "step: 786, train loss: 2.732, val loss: 2.730\n",
      "787\n",
      "step: 787, train loss: 2.722, val loss: 2.720\n",
      "788\n",
      "step: 788, train loss: 2.696, val loss: 2.738\n",
      "789\n",
      "step: 789, train loss: 2.732, val loss: 2.671\n",
      "790\n",
      "step: 790, train loss: 2.701, val loss: 2.680\n",
      "791\n",
      "step: 791, train loss: 2.698, val loss: 2.736\n",
      "792\n",
      "step: 792, train loss: 2.730, val loss: 2.729\n",
      "793\n",
      "step: 793, train loss: 2.687, val loss: 2.661\n",
      "794\n",
      "step: 794, train loss: 2.717, val loss: 2.690\n",
      "795\n",
      "step: 795, train loss: 2.689, val loss: 2.683\n",
      "796\n",
      "step: 796, train loss: 2.749, val loss: 2.714\n",
      "797\n",
      "step: 797, train loss: 2.669, val loss: 2.701\n",
      "798\n",
      "step: 798, train loss: 2.713, val loss: 2.709\n",
      "799\n",
      "step: 799, train loss: 2.711, val loss: 2.672\n",
      "800\n",
      "step: 800, train loss: 2.740, val loss: 2.739\n",
      "801\n",
      "step: 801, train loss: 2.669, val loss: 2.680\n",
      "802\n",
      "step: 802, train loss: 2.694, val loss: 2.668\n",
      "803\n",
      "step: 803, train loss: 2.720, val loss: 2.723\n",
      "804\n",
      "step: 804, train loss: 2.753, val loss: 2.719\n",
      "805\n",
      "step: 805, train loss: 2.696, val loss: 2.716\n",
      "806\n",
      "step: 806, train loss: 2.688, val loss: 2.707\n",
      "807\n",
      "step: 807, train loss: 2.705, val loss: 2.693\n",
      "808\n",
      "step: 808, train loss: 2.679, val loss: 2.718\n",
      "809\n",
      "step: 809, train loss: 2.713, val loss: 2.723\n",
      "810\n",
      "step: 810, train loss: 2.739, val loss: 2.703\n",
      "811\n",
      "step: 811, train loss: 2.701, val loss: 2.694\n",
      "812\n",
      "step: 812, train loss: 2.688, val loss: 2.698\n",
      "813\n",
      "step: 813, train loss: 2.740, val loss: 2.674\n",
      "814\n",
      "step: 814, train loss: 2.688, val loss: 2.700\n",
      "815\n",
      "step: 815, train loss: 2.751, val loss: 2.673\n",
      "816\n",
      "step: 816, train loss: 2.711, val loss: 2.702\n",
      "817\n",
      "step: 817, train loss: 2.694, val loss: 2.688\n",
      "818\n",
      "step: 818, train loss: 2.711, val loss: 2.678\n",
      "819\n",
      "step: 819, train loss: 2.675, val loss: 2.710\n",
      "820\n",
      "step: 820, train loss: 2.656, val loss: 2.682\n",
      "821\n",
      "step: 821, train loss: 2.656, val loss: 2.674\n",
      "822\n",
      "step: 822, train loss: 2.712, val loss: 2.714\n",
      "823\n",
      "step: 823, train loss: 2.687, val loss: 2.682\n",
      "824\n",
      "step: 824, train loss: 2.711, val loss: 2.704\n",
      "825\n",
      "step: 825, train loss: 2.698, val loss: 2.698\n",
      "826\n",
      "step: 826, train loss: 2.684, val loss: 2.706\n",
      "827\n",
      "step: 827, train loss: 2.682, val loss: 2.688\n",
      "828\n",
      "step: 828, train loss: 2.691, val loss: 2.667\n",
      "829\n",
      "step: 829, train loss: 2.687, val loss: 2.692\n",
      "830\n",
      "step: 830, train loss: 2.679, val loss: 2.659\n",
      "831\n",
      "step: 831, train loss: 2.678, val loss: 2.743\n",
      "832\n",
      "step: 832, train loss: 2.689, val loss: 2.668\n",
      "833\n",
      "step: 833, train loss: 2.669, val loss: 2.686\n",
      "834\n",
      "step: 834, train loss: 2.639, val loss: 2.710\n",
      "835\n",
      "step: 835, train loss: 2.689, val loss: 2.661\n",
      "836\n",
      "step: 836, train loss: 2.662, val loss: 2.658\n",
      "837\n",
      "step: 837, train loss: 2.649, val loss: 2.664\n",
      "838\n",
      "step: 838, train loss: 2.687, val loss: 2.726\n",
      "839\n",
      "step: 839, train loss: 2.654, val loss: 2.755\n",
      "840\n",
      "step: 840, train loss: 2.716, val loss: 2.704\n",
      "841\n",
      "step: 841, train loss: 2.700, val loss: 2.701\n",
      "842\n",
      "step: 842, train loss: 2.705, val loss: 2.694\n",
      "843\n",
      "step: 843, train loss: 2.690, val loss: 2.684\n",
      "844\n",
      "step: 844, train loss: 2.692, val loss: 2.684\n",
      "845\n",
      "step: 845, train loss: 2.695, val loss: 2.695\n",
      "846\n",
      "step: 846, train loss: 2.670, val loss: 2.663\n",
      "847\n",
      "step: 847, train loss: 2.655, val loss: 2.707\n",
      "848\n",
      "step: 848, train loss: 2.695, val loss: 2.644\n",
      "849\n",
      "step: 849, train loss: 2.696, val loss: 2.714\n",
      "850\n",
      "step: 850, train loss: 2.689, val loss: 2.664\n",
      "851\n",
      "step: 851, train loss: 2.725, val loss: 2.685\n",
      "852\n",
      "step: 852, train loss: 2.714, val loss: 2.692\n",
      "853\n",
      "step: 853, train loss: 2.663, val loss: 2.679\n",
      "854\n",
      "step: 854, train loss: 2.663, val loss: 2.687\n",
      "855\n",
      "step: 855, train loss: 2.666, val loss: 2.665\n",
      "856\n",
      "step: 856, train loss: 2.649, val loss: 2.724\n",
      "857\n",
      "step: 857, train loss: 2.694, val loss: 2.674\n",
      "858\n",
      "step: 858, train loss: 2.668, val loss: 2.761\n",
      "859\n",
      "step: 859, train loss: 2.697, val loss: 2.670\n",
      "860\n",
      "step: 860, train loss: 2.650, val loss: 2.692\n",
      "861\n",
      "step: 861, train loss: 2.669, val loss: 2.681\n",
      "862\n",
      "step: 862, train loss: 2.638, val loss: 2.671\n",
      "863\n",
      "step: 863, train loss: 2.674, val loss: 2.704\n",
      "864\n",
      "step: 864, train loss: 2.652, val loss: 2.700\n",
      "865\n",
      "step: 865, train loss: 2.711, val loss: 2.694\n",
      "866\n",
      "step: 866, train loss: 2.676, val loss: 2.643\n",
      "867\n",
      "step: 867, train loss: 2.708, val loss: 2.674\n",
      "868\n",
      "step: 868, train loss: 2.642, val loss: 2.678\n",
      "869\n",
      "step: 869, train loss: 2.686, val loss: 2.671\n",
      "870\n",
      "step: 870, train loss: 2.684, val loss: 2.704\n",
      "871\n",
      "step: 871, train loss: 2.664, val loss: 2.671\n",
      "872\n",
      "step: 872, train loss: 2.728, val loss: 2.675\n",
      "873\n",
      "step: 873, train loss: 2.657, val loss: 2.663\n",
      "874\n",
      "step: 874, train loss: 2.687, val loss: 2.675\n",
      "875\n",
      "step: 875, train loss: 2.659, val loss: 2.648\n",
      "876\n",
      "step: 876, train loss: 2.656, val loss: 2.654\n",
      "877\n",
      "step: 877, train loss: 2.705, val loss: 2.663\n",
      "878\n",
      "step: 878, train loss: 2.646, val loss: 2.669\n",
      "879\n",
      "step: 879, train loss: 2.695, val loss: 2.679\n",
      "880\n",
      "step: 880, train loss: 2.752, val loss: 2.663\n",
      "881\n",
      "step: 881, train loss: 2.676, val loss: 2.666\n",
      "882\n",
      "step: 882, train loss: 2.684, val loss: 2.706\n",
      "883\n",
      "step: 883, train loss: 2.720, val loss: 2.675\n",
      "884\n",
      "step: 884, train loss: 2.693, val loss: 2.681\n",
      "885\n",
      "step: 885, train loss: 2.702, val loss: 2.675\n",
      "886\n",
      "step: 886, train loss: 2.701, val loss: 2.665\n",
      "887\n",
      "step: 887, train loss: 2.645, val loss: 2.657\n",
      "888\n",
      "step: 888, train loss: 2.664, val loss: 2.652\n",
      "889\n",
      "step: 889, train loss: 2.661, val loss: 2.647\n",
      "890\n",
      "step: 890, train loss: 2.647, val loss: 2.661\n",
      "891\n",
      "step: 891, train loss: 2.719, val loss: 2.663\n",
      "892\n",
      "step: 892, train loss: 2.651, val loss: 2.691\n",
      "893\n",
      "step: 893, train loss: 2.697, val loss: 2.661\n",
      "894\n",
      "step: 894, train loss: 2.672, val loss: 2.702\n",
      "895\n",
      "step: 895, train loss: 2.675, val loss: 2.640\n",
      "896\n",
      "step: 896, train loss: 2.661, val loss: 2.671\n",
      "897\n",
      "step: 897, train loss: 2.631, val loss: 2.658\n",
      "898\n",
      "step: 898, train loss: 2.671, val loss: 2.621\n",
      "899\n",
      "step: 899, train loss: 2.639, val loss: 2.655\n",
      "900\n",
      "step: 900, train loss: 2.678, val loss: 2.694\n",
      "901\n",
      "step: 901, train loss: 2.649, val loss: 2.622\n",
      "902\n",
      "step: 902, train loss: 2.662, val loss: 2.688\n",
      "903\n",
      "step: 903, train loss: 2.660, val loss: 2.679\n",
      "904\n",
      "step: 904, train loss: 2.649, val loss: 2.663\n",
      "905\n",
      "step: 905, train loss: 2.625, val loss: 2.648\n",
      "906\n",
      "step: 906, train loss: 2.641, val loss: 2.633\n",
      "907\n",
      "step: 907, train loss: 2.670, val loss: 2.656\n",
      "908\n",
      "step: 908, train loss: 2.673, val loss: 2.685\n",
      "909\n",
      "step: 909, train loss: 2.653, val loss: 2.649\n",
      "910\n",
      "step: 910, train loss: 2.639, val loss: 2.651\n",
      "911\n",
      "step: 911, train loss: 2.635, val loss: 2.667\n",
      "912\n",
      "step: 912, train loss: 2.670, val loss: 2.670\n",
      "913\n",
      "step: 913, train loss: 2.725, val loss: 2.689\n",
      "914\n",
      "step: 914, train loss: 2.662, val loss: 2.659\n",
      "915\n",
      "step: 915, train loss: 2.675, val loss: 2.695\n",
      "916\n",
      "step: 916, train loss: 2.695, val loss: 2.667\n",
      "917\n",
      "step: 917, train loss: 2.624, val loss: 2.680\n",
      "918\n",
      "step: 918, train loss: 2.654, val loss: 2.670\n",
      "919\n",
      "step: 919, train loss: 2.712, val loss: 2.669\n",
      "920\n",
      "step: 920, train loss: 2.624, val loss: 2.657\n",
      "921\n",
      "step: 921, train loss: 2.669, val loss: 2.652\n",
      "922\n",
      "step: 922, train loss: 2.667, val loss: 2.695\n",
      "923\n",
      "step: 923, train loss: 2.696, val loss: 2.656\n",
      "924\n",
      "step: 924, train loss: 2.673, val loss: 2.671\n",
      "925\n",
      "step: 925, train loss: 2.642, val loss: 2.681\n",
      "926\n",
      "step: 926, train loss: 2.667, val loss: 2.644\n",
      "927\n",
      "step: 927, train loss: 2.632, val loss: 2.670\n",
      "928\n",
      "step: 928, train loss: 2.614, val loss: 2.654\n",
      "929\n",
      "step: 929, train loss: 2.646, val loss: 2.671\n",
      "930\n",
      "step: 930, train loss: 2.637, val loss: 2.646\n",
      "931\n",
      "step: 931, train loss: 2.614, val loss: 2.638\n",
      "932\n",
      "step: 932, train loss: 2.616, val loss: 2.617\n",
      "933\n",
      "step: 933, train loss: 2.612, val loss: 2.656\n",
      "934\n",
      "step: 934, train loss: 2.706, val loss: 2.647\n",
      "935\n",
      "step: 935, train loss: 2.637, val loss: 2.642\n",
      "936\n",
      "step: 936, train loss: 2.649, val loss: 2.660\n",
      "937\n",
      "step: 937, train loss: 2.657, val loss: 2.622\n",
      "938\n",
      "step: 938, train loss: 2.632, val loss: 2.636\n",
      "939\n",
      "step: 939, train loss: 2.619, val loss: 2.633\n",
      "940\n",
      "step: 940, train loss: 2.639, val loss: 2.666\n",
      "941\n",
      "step: 941, train loss: 2.633, val loss: 2.683\n",
      "942\n",
      "step: 942, train loss: 2.644, val loss: 2.668\n",
      "943\n",
      "step: 943, train loss: 2.640, val loss: 2.645\n",
      "944\n",
      "step: 944, train loss: 2.639, val loss: 2.621\n",
      "945\n",
      "step: 945, train loss: 2.623, val loss: 2.666\n",
      "946\n",
      "step: 946, train loss: 2.632, val loss: 2.702\n",
      "947\n",
      "step: 947, train loss: 2.657, val loss: 2.662\n",
      "948\n",
      "step: 948, train loss: 2.626, val loss: 2.671\n",
      "949\n",
      "step: 949, train loss: 2.608, val loss: 2.633\n",
      "950\n",
      "step: 950, train loss: 2.650, val loss: 2.660\n",
      "951\n",
      "step: 951, train loss: 2.648, val loss: 2.724\n",
      "952\n",
      "step: 952, train loss: 2.634, val loss: 2.622\n",
      "953\n",
      "step: 953, train loss: 2.632, val loss: 2.616\n",
      "954\n",
      "step: 954, train loss: 2.632, val loss: 2.653\n",
      "955\n",
      "step: 955, train loss: 2.656, val loss: 2.601\n",
      "956\n",
      "step: 956, train loss: 2.677, val loss: 2.638\n",
      "957\n",
      "step: 957, train loss: 2.601, val loss: 2.639\n",
      "958\n",
      "step: 958, train loss: 2.599, val loss: 2.608\n",
      "959\n",
      "step: 959, train loss: 2.626, val loss: 2.628\n",
      "960\n",
      "step: 960, train loss: 2.629, val loss: 2.608\n",
      "961\n",
      "step: 961, train loss: 2.650, val loss: 2.631\n",
      "962\n",
      "step: 962, train loss: 2.650, val loss: 2.638\n",
      "963\n",
      "step: 963, train loss: 2.660, val loss: 2.603\n",
      "964\n",
      "step: 964, train loss: 2.615, val loss: 2.694\n",
      "965\n",
      "step: 965, train loss: 2.613, val loss: 2.598\n",
      "966\n",
      "step: 966, train loss: 2.631, val loss: 2.625\n",
      "967\n",
      "step: 967, train loss: 2.654, val loss: 2.656\n",
      "968\n",
      "step: 968, train loss: 2.642, val loss: 2.628\n",
      "969\n",
      "step: 969, train loss: 2.659, val loss: 2.655\n",
      "970\n",
      "step: 970, train loss: 2.615, val loss: 2.669\n",
      "971\n",
      "step: 971, train loss: 2.649, val loss: 2.641\n",
      "972\n",
      "step: 972, train loss: 2.624, val loss: 2.621\n",
      "973\n",
      "step: 973, train loss: 2.637, val loss: 2.622\n",
      "974\n",
      "step: 974, train loss: 2.646, val loss: 2.642\n",
      "975\n",
      "step: 975, train loss: 2.615, val loss: 2.638\n",
      "976\n",
      "step: 976, train loss: 2.655, val loss: 2.614\n",
      "977\n",
      "step: 977, train loss: 2.650, val loss: 2.603\n",
      "978\n",
      "step: 978, train loss: 2.625, val loss: 2.608\n",
      "979\n",
      "step: 979, train loss: 2.627, val loss: 2.622\n",
      "980\n",
      "step: 980, train loss: 2.617, val loss: 2.668\n",
      "981\n",
      "step: 981, train loss: 2.643, val loss: 2.646\n",
      "982\n",
      "step: 982, train loss: 2.602, val loss: 2.633\n",
      "983\n",
      "step: 983, train loss: 2.609, val loss: 2.607\n",
      "984\n",
      "step: 984, train loss: 2.660, val loss: 2.640\n",
      "985\n",
      "step: 985, train loss: 2.671, val loss: 2.629\n",
      "986\n",
      "step: 986, train loss: 2.608, val loss: 2.663\n",
      "987\n",
      "step: 987, train loss: 2.600, val loss: 2.621\n",
      "988\n",
      "step: 988, train loss: 2.680, val loss: 2.678\n",
      "989\n",
      "step: 989, train loss: 2.673, val loss: 2.628\n",
      "990\n",
      "step: 990, train loss: 2.595, val loss: 2.603\n",
      "991\n",
      "step: 991, train loss: 2.637, val loss: 2.637\n",
      "992\n",
      "step: 992, train loss: 2.648, val loss: 2.635\n",
      "993\n",
      "step: 993, train loss: 2.638, val loss: 2.639\n",
      "994\n",
      "step: 994, train loss: 2.698, val loss: 2.634\n",
      "995\n",
      "step: 995, train loss: 2.634, val loss: 2.665\n",
      "996\n",
      "step: 996, train loss: 2.606, val loss: 2.647\n",
      "997\n",
      "step: 997, train loss: 2.626, val loss: 2.601\n",
      "998\n",
      "step: 998, train loss: 2.570, val loss: 2.658\n",
      "999\n",
      "step: 999, train loss: 2.624, val loss: 2.696\n",
      "3.030545949935913\n",
      "model saved\n",
      "Code block executed in 499.2037 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    print(iter)\n",
    "    losses = estimate_loss()\n",
    "    print(f\"step: {iter}, train loss: {losses['train']:.3f}, val loss: {losses['val']:.3f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model.forward(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(loss.item())\n",
    "\n",
    "with open('model-01a.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "print('model saved')\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Code block executed in {elapsed_time:.4f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0d4668c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! Can you see me?sire\n",
      " twhei lethe nsgoketBAcly\n",
      "\"imt!ayedosine the be yibad\n",
      "fau t toreathe,amdlere unnd theo\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Hello! Can you see me?'\n",
    "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
    "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=100)[0].tolist())\n",
    "print(generated_chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
